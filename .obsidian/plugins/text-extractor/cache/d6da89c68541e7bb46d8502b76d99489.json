{"path":"Year 1/RA/Lecture/1RA_Lecture_Notes_2024-2025.pdf","text":"University of Birmingham School of Mathematics Real Analysis – Integration – Spring 2025 Lecture Notes by Andrew Morris Contents Week 1: The Riemann–Darboux Integral (Preliminaries) 2 1.0. Introduction and motivation 2 1.1. Supremum/Infimum 3 1.2. Upper/Lower sums 5 Week 2: The Riemann–Darboux Integral 7 2.1. The Partition Lemma 7 2.2. Integrable functions 8 Week 3: Riemann’s Integrability Criterion 11 3.1. An equivalent definition for integrability 11 3.2. Integrability of monotonic functions 13 Week 4: Properties of Integrable Functions 15 4.1. Integrability of continuous functions 15 4.2. Restriction/extension, linearity, algebra and estimates 16 Week 5: The Fundamental Theorem of Calculus 21 5.1. The First Fundamental Theorem of Calculus 21 5.2. The Second Fundamental Theorem of Calculus 22 Week 7: Integral Calculus 24 7.1. Product and composite functions 24 7.2. Rational functions 27 Week 8: Integral Calculus (Continued) 30 8.1. Trigonometric functions 30 8.2. Reduction formulas and bijective substitution 33 Week 9: Improper Integrals 36 9.1. Unbounded intervals 36 9.2. Unbounded functions 38 Week 10: First-Order ODEs and Applications 41 10.1. Separable equations 41 10.2. Linear equations 44 Week 11: Second-Order ODEs and Applications 47 11.1. Homogeneous equations 47 11.2. Inhomogeneous equations 49 Updated 10:21 on 20 January 2025 1 2 Week 1: The Riemann–Darboux Integral (Preliminaries) The first two weeks we develop the Riemann–Darboux theory of integration to define precisely what is meant by an integrable function and its integral. The aim is to develop a theory of integration that is not limited to continuous functions. This will lead us to fundamental questions about the construction of the real numbers. 1.0. Introduction and motivation. The definitions of continuity and differen- tiability for functions are motivated by geometric properties of their graphs. These are, respectively, the connectivity of the graph and the existence of tangent lines on the graph. The definition of integrability is likewise motivated by the geometric notion of the area bound by the graph. The following example serves as a prototype for our construction of the Riemann–Darboux integral. Example 1.0.1 (Prototype). Suppose that f : [0, 1] → R is defined by f (x) := x. We can use rectangles to approximate the area bound by its graph and the x-axis. To begin, consider dividing the domain [0, 1] into three non-overlapping subintervals of equal width [0, 1] = [0, x1] ∪ [x1, x2] ∪ [x2, 1]. It seems convenient to label x0 = 0 and x3 = 1. The quantities (x1 − x0), (x2 − x1) and (x3 − x2) will provide the widths of our rectangles but we have many sensible choices for the heights of our rectangles. For example, on the subinterval [x0, x1], we could choose f (x0), f (x1) or any f (y) with y ∈ [x0, x1]. This was Riemann’s original approach but we shall consider a streamlined version developed later by Darboux. The innovation in this approach is to avoid using function values at specific locations by using the extreme values of the function on each subinterval. In our example, these are expressed as follows: m1 := min{f (x) : x ∈ [x0, x1]} M1 := max{f (x) : x ∈ [x0, x1]} m2 := min{f (x) : x ∈ [x1, x2]} M2 := max{f (x) : x ∈ [x1, x2]} m3 := min{f (x) : x ∈ [x2, x3]} M3 := max{f (x) : x ∈ [x2, x3]} We then consider just two types of approximations for the area bound by the graph. The first uses the minimum values for the heights of the rectangles, which thus underestimates the area as s3 := m1(x1 − x0) + m2(x2 − x1) + m3(x3 − x2) ≤ Area. The second uses the maximum values for the heights of the rectangles, which thus overestimates the area as S3 := M1(x1 − x0) + M2(x2 − x1) + M3(x3 − x2) ≥ Area. These sums are labelled as s3 and S3 to denote the fact that they are based on dividing [0, 1] into three subintervals of equal width. For each n ∈ N, we could follow the same procedure using n subintervals to obtain an estimate sn ≤ Area ≤ Sn in the hope that sn and Sn tend to a common value as n tends to infinity. It is this value that we would like to define as the area bound by the graph of the function. This example raises many issues that we will resolve this week, principally the precise meanings of the limits of sn and Sn, and the possibility that sn and Sn might tend to different values. Moreover, by considering a specific function (f (x) = x) and dividing its domain into pieces of equal width, we have unwittingly limited the scope of its applicability to more general functions. In particular, if f is not a continuous function, then the formulas for sn and Sn may not be well-defined, as the following example shows. 3 Example 1.0.2. Suppose that f : [0, 1] → [0, 1] is defined by f (x) := {1, x = 0 x, x ̸= 0 . Using the notation in Example 1.0.1, we find m1 := min{f (x) : x ∈ [0, x1]} = min({1} ∪ {x : x ∈ (0, x1]}) = min(0, x1] but the minimum of the interval (0, x1] does not exist because 0 /∈ (0, x1]. This is because the minimum of a set must be an element of the set which is less than all other elements in the set (see Definition 1.1.2 below). We can develop a theory of integration that is not limited to continuous functions, however, if we begin by replacing the minimum and maximum in Example 1.0.1 with the infimum and supremum. 1.1. Supremum/Infimum. We begin our construction of the Riemann–Darboux integral with an analysis of the supremum and infimum for subsets of the real line. To this end, let us introduce the following definition. Definition 1.1.1. A set X ⊆ R is called bounded above if there exists M ∈ R such that x ≤ M for all x ∈ X. If X ⊆ R is bounded above and not empty, then the following terms apply: (1) An upper bound for X is any M ∈ R such that x ≤ M for all x ∈ X. (2) The maximum of X, denoted max X, is an upper bound for X that is in X. (3) The supremum of X, denoted sup X, is the least upper bound for X, that is: (a) sup X is an upper bound for X; (b) sup X ≤ M for all upper bounds M for X. If X ⊆ R is not bounded above, then sup X := ∞, whilst sup ∅ := −∞ for the empty set ∅. It is equivalent in (3) to write sup X := min{M : M is an upper bound for X}. There is also the following analogue of these definitions based on lower bounds. Definition 1.1.2. A set X ⊆ R is bounded below if there exists m ∈ R such that x ≥ m for all x ∈ X. If X ⊆ R is bounded below and not empty, then a lower bound for X is any such number m; the minimum of X, denoted min X, is a lower bound that is in X; and the infimum of X, denoted inf X, is the greatest lower bound for X. If X ⊆ R is not bounded below, then inf X := −∞, whilst inf ∅ := ∞. Let us also highlight the definition of a bounded subset of R. Definition 1.1.3. A set X ⊆ R is bounded if it is bounded above and bounded below, so there exist real numbers m and M such that m ≤ x ≤ M for all x ∈ X. The next lemma provides a useful ‘ϵ-type’ characterisation for the supremum. It is a good exercise to try to prove an analogous result for the infimum. Lemma 1.1.4. Suppose that N ∈ R is an upper bound for a nonempty set X ⊆ R. The following properties are equivalent: (1) N = sup X; (2) For each ϵ > 0, there exists x in X such that N − ϵ < x. Proof. Suppose that N ∈ R is an upper bound for a nonempty set X ⊆ R. The equivalence requires us to prove that (1) holds if and only if (2) holds: First, we prove that (1) implies (2). Assume that (1) holds, so N = sup X. Suppose that ϵ > 0 and observe that N − ϵ is not an upper bound for X, because N − ϵ < N and N is the least upper bound for X. Now, since N − ϵ is not an upper bound for X, there exists x in X such that N − ϵ < x, as required. The proof that (2) implies (1) is posed as a question on the problem sheet. □ 4 Here are some examples that use an ‘ϵ-type’ approach to find the supremum and infimum of sets of real numbers. Example 1.1.5. Find, with proof, the supremum and infimum of these sets: (1) A = {x : x ≤ 0 and x2 + x − 1 < 0} (2) B = { n+1 n : n ∈ N} Solution. (1) If x2 + x − 1 < 0, then completing the square gives (x + 1 2 ) 2 − 5 4 < 0 and |x + 1 2 | < √5 2 (note √x2 = |x|). We thus have A = (− 1 2 − √5 2 , 0] and so we claim that inf A = − 1 2 − √5 2 =: a and sup A = 0. The interval expression A = (a, 0] shows that a is a lower bound for A, and 0 is an upper bound for A. It remains to prove that a is the greatest such lower bound, and 0 is the least such upper bound. To do this let ϵ > 0, and observe that a + ϵ is never a lower bound for A (since either a + ϵ 2 ∈ A or a + ϵ > 0 ∈ A), and that −ϵ is never an upper bound for A (since −ϵ < 0 ∈ A). (2) The elements of the set B = {1 + 1 n : n ∈ N} = {2, 1 1 2 , 1 1 3 , . . .} form a decreasing sequence with limn→∞(1 + 1 n ) = 1. This shows that 1 is a lower bound for B, and 2 is an upper bound for B. Now let ϵ > 0, and observe that 1 + ϵ is never a lower bound for B, since we can always find n ∈ N such that 1 + 1 n < 1 + ϵ (choose n ∈ N with n > 1 ϵ ), hence inf B = 1. Finally, observe that 2 − ϵ is never an upper bound for B, since 2 ∈ B, hence sup B = 2. Our discussion so far has purposefully neglected the question of whether the supremum and infimum of a set of real numbers exists. This is, in actual fact, the quintessential property of the real numbers. The real numbers can be constructed rigorously as a mathematical object known as a complete ordered field. It can be proved that there exists a unique complete ordered field, so the real numbers can be understood without ambiguity, but we don’t have time to present this lengthy and technical construction here. As a complete ordered field, however, we can understand the real numbers as a set on which certain properties hold. These properties are so fundamental that they are called axioms (it is a good exercise to look up these axioms) and these fall into three categories. The first are algebraic axioms, which say that R is a set on which appropriate operations of addition (+) and multiplication (·) are defined. This means that R is a field. The second are ordering axioms, which say that there is a certain order (<) between real numbers. This means that R is an ordered field. The rational numbers also form an ordered field, however, and it is only the final Completeness Axiom below that distinguishes Q from R. This (perhaps) obvious property is all that is required to provide a mathematically rigorous foundation for our intuition that the real numbers ‘complete’ the rational numbers by ‘filling in all the gaps’ between them. For example, using this axiom it can be proved that there exists a real number whose square is equal to 2 (so √2 ∈ R), and that given any two rational numbers q1 and q2, there exists a real number r such that q1 < r < q2 (this is known as the Archimedean Property of R). Axiom 1.1.6 (Completeness Axiom of R). If X ⊆ R is bounded above and not empty, then the supremum of X exists in R, that is, sup X ∈ R. It is simply a matter of historical choice that this axiom is stated for sets that are bounded above, rather than for sets that are bounded below. The Completeness Axiom implies the following analogue for sets that are bounded below. It is a good exercise to try to prove this result. Corollary 1.1.7. If X ⊆ R is not empty and bounded below, then the infimum of X exists in R. 5 1.2. Upper/Lower sums. We will initially be concerned with bounded functions. It is important to distinguish between the definition of a bounded set and the defini- tion of a bounded function (compare the following definition with Definition 1.1.3). Definition 1.2.1. A function f : X → R, with domain X ⊆ R, is bounded if there exist real numbers m and M such that m ≤ f (x) ≤ M for all x ∈ X. It is also important to recognise that whether or not a function is bounded depends on both the rule defining it and its domain. In fact, all continuous functions f : [a, b] → R are automatically bounded. This result is known as the Extreme Value Theorem but we do know pursue this here, since we want to develop a theory of integration for functions that may not be continuous. Example 1.2.2. The following functions are defined by the same rule on different domains. Determine which are bounded functions: (1) f : (0, 1] → R, f (x) := 1 x (2) g : [ 1 2 , 1] → R, f (x) := 1 x (3) h : [0, 1] → R, f (x) := { 1 x , x ∈ (0, 1] 1, x = 0 Solution. The functions f and h are not bounded, since for each M ≥ 1, it holds that f ( 1 2M ) = h( 1 2M ) = 2M > M . (In other words, for each M ≥ 1, there exists x in the domain of f and h such that f (x) = h(x) > M , so f and h are not bounded.) The function g is bounded, since 1 ≤ 1 x ≤ 2 for all x ∈ [ 1 2 , 1]. □ We now begin our construction of the integral in earnest by restricting attention to [0, ∞)-valued bounded functions f : [a, b] → [0, ∞), where −∞ < a < b < ∞. The following convention will reduce the associated burden of notation. Notation 1.2.3. We henceforth use [a, b] to denote a non-empty, closed and bounded subinterval of R, or more specifically, where −∞ < a < b < ∞. Note that our restriction to non-negative-valued functions is not necessary here but we do so for the convenience of avoiding negative-values in computations of area. It will be a simple matter to extend our theory to R-valued bounded functions in Lecture 8.4. We will also define ‘improper integrals’ on unbounded intervals and for unbounded functions in Lectures 10.3 and 10.4. Our prototype for the construction of the Riemann–Darboux integral in Exam- ple 1.0.1 was based on dividing the interval [a, b] into n subintervals of equal width. There is no reason, however, to limit our attention to such ‘equal-width’ divisions. The concept of a partition below allows for much more general divisions. Definition 1.2.4. A partition P of an interval [a, b] is a finite set such that {a, b} ⊆ P ⊆ [a, b]. A partition can always be expressed in the form P = {x0, x1, . . . , xn}, where a = x0 < x1 < . . . < xn = b, for some n ∈ N. Our original ‘equal-width’ divisions are then a special type of partition. Example 1.2.5. For each n ∈ N, we can express the interval [0, 1] as a union of n closed subintervals of equal width 1 n by writing [0, 1] = [0, 1 n ] ∪ [ 1 n , 2 n ] ∪ [ 2 n , 3 n ] ∪ . . . ∪ [ (n−1) n , 1]. The endpoints of these subintervals define the partition Pn := {0, 1 n , 2 n , . . . , (n−1) n , 1} = { i n : i = 0, 1, . . . , n}. 6 This can be done for any (closed bounded) interval [a, b] and the resulting partition is called the partition of [a, b] into n subintervals of equal width. We are now in a position to define sums, analogous to the prototypes sn and Sn in Example 1.0.1, to approximate the area bounded by the graph of a bounded function f : [a, b] → [0, ∞) and the x-axis. Definition 1.2.6. Suppose that f : [a, b] → [0, ∞) is a bounded function. Let P = {x0, x1, . . . , xn} denote a partition of [a, b]. The lower Riemann–Darboux sum L(f, P ) is defined by L(f, P ) := n∑ i=1 mi(xi − xi−1), where mi := inf{f (x) : x ∈ [xi−1, xi]}. The upper Riemann–Darboux sum U (f, P ) is defined by U (f, P ) := n∑ i=1 Mi(xi − xi−1), where Mi := sup{f (x) : x ∈ [xi−1, xi]}. These are commonly referred to, respectively, as the lower sum and upper sum. The following example shows how to compute these sums using the ‘equal-width’ partitions from Example 1.2.5. Example 1.2.7. Let f : [0, b] → [0, ∞) be defined by f (x) := x. (1) For each n ∈ N, calculate L(f, Pn) and U (f, Pn), where Pn is the partition of [0, b] into n subintervals of equal width. (2) Calculate sup{L(f, Pn) : n ∈ N} and inf{U (f, Pn) : n ∈ N}. Solution. (1) We begin by using the ideas in Example 1.2.5 to write Pn = {0, b n , 2b n , . . . , (n−1)b n , b} = { ib n : i = 0, 1, . . . , n}. We substitute f (x) = x and xi = ib n into Definition 1.2.6, beginning with mi = inf{x : x ∈ [ (i−1)b n , ib n ]} = inf[ (i−1)b n , ib n ] = (i−1)b n Mi = sup{x : x ∈ [ (i−1)b n , ib n ]} = sup[ (i−1)b n , ib n ] = ib n to obtain L(f, Pn) = n∑ i=1 mi(xi − xi−1) = n∑ i=1 (i−1)b n b n = n∑ i=1(i − 1) b 2 n2 = (n−1)n 2 b2 n2 = n−1 n b2 2 and U (f, Pn) = n∑ i=1 Mi(xi − xi−1) = n∑ i=1 ib n b n = n∑ i=1 i b2 n2 = n(n+1) 2 b2 n2 = n+1 n b2 2 , where we used the formula for an arithmetic sum in the penultimate equalities. (2) Now, recalling Example 1.1.5, we calculate sup{L(f, Pn) : n ∈ N} = sup{(1 − 1 n ) b 2 2 : n ∈ N} = b 2 2 and inf{U (f, Pn) : n ∈ N} = inf{(1 + 1 n ) b 2 2 : n ∈ N} = b 2 2 as required. The ‘equal-width’ partitions from Example 1.2.5 are less useful, however, for functions that have points of discontinuity. The following example introduces an alternative type of partition, often abbreviated Pδ, which can be used to zoom in on points of discontinuity by choosing δ > 0 sufficiently small. 7 Example 1.2.8. Let f : [0, 2] → [0, 1] be defined by f (x) := {1, x ̸= 1 0, x = 1 . (1) Calculate L(f, Pδ) and U (f, Pδ) for Pδ = {0, 1 − δ, 1, 1 + δ, 2} and δ ∈ (0, 1). (2) Calculate sup{L(f, Pδ) : δ ∈ (0, 1)} and inf{U (f, Pδ) : δ ∈ (0, 1)}. Solution. We adopt the notation from Definition 1.2.6 by writing {x0, x1, x2, x3, x4} = {0, 1 − δ, 1, 1 + δ, 2}. We have m1 = 1, m2 = 0, m3 = 0, m4 = 1 whilst M1 = 1, M2 = 1, M3 = 1, M4 = 1, so L(f, Pδ) = 4∑ i=1 mi(xi − xi−1) = 1(1 − δ) + 0(δ) + 0(δ) + 1(1 − δ) = 2 − 2δ and U (f, Pδ) = 4∑ i=1 Mi(xi − xi−1) = 1(1 − δ) + 1(δ) + 1(δ) + 1(1 − δ) = 2. 2) Now we calculate sup{L(f, Pδ) : δ ∈ (0, 1)} = sup{2 − 2δ : δ ∈ (0, 1)} = sup(1, 2) = 2 and inf{U (f, Pδ) : δ ∈ (0, 1)} = inf{2 : δ ∈ (0, 1)} = inf{2} = 2 as required. Week 2: The Riemann–Darboux Integral This week we will complete the construction of the Riemann–Darboux Integral. A technical result, here called the Partition Lemma, shows that there is a certain ordering amongst the lower sums and upper sums for any bounded function. This allows us to define a lower integral and an upper integral for any bounded function. A function will be called integrable precisely when these two formative integrals are equal. 2.1. The Partition Lemma. We need to establish some properties about lower sums and upper sums before we can finally define the Riemann–Darboux integral. These are contained in the following useful result. Lemma 2.1.1 (Partition Lemma). Suppose that f : [a, b] → [0, ∞) is a bounded function. The following properties hold for partitions P , Q, R and S of [a, b]: (1) L(f, P ) ≤ U (f, P ) (2) P ⊆ Q =⇒ { L(f, P ) ≤ L(f, Q) U (f, P ) ≥ U (f, Q) (3) L(f, R) ≤ U (f, S) Proof. Let P = {x0, x1, . . . , xn} with mi and Mi as in Definitions 1.2.4 and 1.2.6. (1) Observe that mi ≤ Mi, hence L(f, P ) = n∑ i=1 mi(xi − xi−1) ≤ n∑ i=1 Mi(xi − xi−1) = U (f, P ). (2) We will only prove the stated inequality for lower sums, since an analogous argument implies the reverse inequality for upper sums. 8 We first consider the special case when Q = P ∪ {y} for some y ∈ (x0, x1). In that case Q = {x0, y, x1, . . . , xn}, so we introduce the notation m1 := inf{f (x) : x ∈ [x0, x1]}, m ′ 1 := inf{f (x) : x ∈ [x0, y]}, m ′′ 1 := inf{f (x) : x ∈ [y, x1]}. Observe that m1 ≤ m′ 1 and m1 ≤ m ′′ 1 , hence m1(x1 − x0) = m1(x1 − y + y − x0) = m1(y − x0) + m1(x1 − y) ≤ m′ 1(y − x0) + m ′′ 1 (x1 − y). This implies the required estimate, since L(f, P ) = m1(x1 − x0) + n∑ i=2 mi(xi − xi−1) ≤ m ′ 1(y − x0) + m ′′ 1 (x1 − y) + n∑ i=2 mi(xi − xi−1) = L(f, Q). To obtain the result in the general case when P ⊆ Q, we make two observations. First, the proof above can be adapted to the case where y ∈ [xi, xi−1] for any i ∈ {1, . . . , n}. Second, since partitions are finite sets and P ⊆ Q, it must hold that Q = P ∪ {y1} ∪ {y2} ∪ . . . ∪ {yN } for some {y1, . . . , yN } ⊆ [a, b] and N ∈ N. Therefore, iterating the former result a finite number of times (at most N ) gives the result. (It is a good exercise to write out these details.) (3) In this case R and S are arbitrary partitions of [a, b], so (2) does not apply. Instead, since R ⊆ R ∪ S and S ⊆ R ∪ S, where R ∪ S is a partition of [a, b] (it is a good exercise to prove this), we may apply (2) to obtain L(f, R) ≤ L(f, R ∪ S) and U (f, S) ≥ U (f, R ∪ S). Moreover, we know from (1) that L(f, R ∪ S) ≤ U (f, R ∪ S), so altogether L(f, R) ≤ L(f, R ∪ S) ≤ U (f, R ∪ S) ≤ U (f, S), as required. □ 2.2. Integrable functions. The lower Riemann–Darboux sums and the upper Riemann–Darboux sums provide the following two candidates for the integral of a bounded function f : [a, b] → [0, ∞). Definition 2.2.1. Suppose that f : [a, b] → [0, ∞) is a bounded function. The lower integral of f is defined by ∫ b a f := sup{L(f, P ) : P is a partition of [a, b]}. Thee upper integral of f is defined by ∫ b a f := inf{U (f, P ) : P is a partition of [a, b]}. These are also often abbreviated respectively as sup P L(f, P ) and inf P U (f, P ). The Completeness Axiom of R guarantees that the lower integral and the upper integral of a bounded function f : [a, b] → [0, ∞) exist as real numbers. It is a good exercise to prove this. Let us now investigate whether these have to be equal. 9 Proposition 2.2.2. If f : [a, b] → [0, ∞) is a bounded function, then ∫ b a f ≤ ∫ b a f . Proof. We know from the Partition Lemma (Lemma 2.1.1) that L(f, R) ≤ U (f, S) for all partitions R and S of [a, b]. This means that U (f, S) is an upper bound for the set {L(f, R) : R is a partition of [a, b]}, so by the definition of the supremum sup R L(f, R) ≤ U (f, S), for all partitions S of [a, b]. This, in turn, means that the real number supR L(f, R) is a lower bound for the set {U (f, S) : S is a partition of [a, b]}, so by the definition of the infimum sup R L(f, R) ≤ inf S U (f, S). This inequality is exactly ∫ b a f ≤ ∫ b a f by the definition of the lower integral and the upper integral. □ The above proposition shows that the lower integral is always less than or equal to the upper integral for a bounded function f : [a, b] → [0, ∞) but the reverse inequality is not always true (see Example 2.2.4 below). The resolution to this (perhaps) unfortunate reality is to say that a function is integrable precisely when the lower integral and the upper integral are equal, and then the integral of f is defined to be this common value. Definition 2.2.3. A bounded function f : [a, b] → [0, ∞) is called integrable if ∫ b a f = ∫ b a f . If such f is integrable, then the integral of f is ∫ b a f := ∫ b a f = ∫ b a f . Here is an example of a function that is not integrable according to this definition. Example 2.2.4 (Non-integrable function). The function f : [a, b] → [0, 1] with f (x) := { 1, x ∈ Q 0, x /∈ Q satisfies ∫ b a f < ∫ b a f (hence f is not integrable). Proof. To prove this, let P = {x0, x1, . . . , xn} denote a partition of [a, b] with mi and Mi as in Definitions 1.2.4 and 1.2.6. Observe that mi := inf{f (x) : x ∈ [xi−1, xi]} = 0 and Mi := sup{f (x) : x ∈ [xi−1, xi]} = 1 since the non-empty interval [xi−1, xi] contains at least one (in fact infinitely many) rational numbers and irrational numbers. Therefore, we have L(f, P ) = n∑ i=1 0(xi − xi−1) = 0 and U (f, P ) = n∑ i=1 1(xi − xi−1) = b − a, hence ∫ b a f := sup P L(f, P ) = sup P {0} = 0 whilst ∫ b a f := inf P U (f, P ) = inf{b − a} = b − a. The result follows because 0 < b − a (recall Notation 1.2.3). □ We can now prove that the functions in Examples 1.2.7 and 1.2.8 are integrable. Example 2.2.5. Let f : [0, b] → [0, ∞) be defined by f (x) := x. Prove that f is integrable and compute ∫ b 0 f . 10 Solution. We continue on from Example 1.2.7, where we proved that sup{L(f, Pn) : n ∈ N} = b2 2 and inf{U (f, Pn) : n ∈ N} = b 2 2 . Now, as {L(f, Pn) : n ∈ N} ⊆ {L(f, P ) : P is a partition of [0, b]}, we have ∫ b 0 f := sup{L(f, P ) : P is a partition of [0, b]} ≥ sup{L(f, Pn) : n ∈ N} = b2 2 . Moreover, as {U (f, Pn) : n ∈ N} ⊆ {U (f, P ) : P is a partition of [0, b]}, we have ∫ b 0 f := inf{U (f, P ) : P is a partition of [0, b]} ≤ inf{U (f, Pn) : n ∈ N} = b 2 2 . We also know from Proposition 2.2.2 that ∫ b 0 f ≤ ∫ b 0 f , so altogether b 2 2 ≤ ∫ b 0 f ≤ ∫ b 0 f ≤ b 2 2 . Notice that this chain of inequalities can only hold if b 2 2 = ∫ b 0 f = ∫ b 0 f = b 2 2 , hence f is integrable and ∫ b 0 f = b 2 2 . □ The following example is completely analogous to that above but its is included for completeness. Example 2.2.6. Let f : [0, 2] → [0, 1] be defined by f (x) := { 1, x ̸= 1 0, x = 1 . Prove that f is integrable and compute ∫ 1 0 f . Solution. We continue on from Example 1.2.8, where we proved that sup{L(f, Pδ) : δ ∈ (0, 1)} = 2 and inf{U (f, Pδ) : δ ∈ (0, 1)} = 2. Now, as {L(f, Pδ) : δ ∈ (0, 1)} ⊆ {L(f, P ) : P is a partition of [0, 2]}, we have ∫ 2 0 f := sup{L(f, P ) : P is a partition of [0, 2]} ≥ sup{L(f, Pδ) : δ ∈ (0, 1)} = 2. Moreover, as {U (f, Pδ) : δ ∈ (0, 1)} ⊆ {U (f, P ) : P is a partition of [0, 2]}, we have ∫ 2 0 f := inf{U (f, P ) : P is a partition of [0, 2]} ≤ inf{U (f, Pδ) : δ ∈ (0, 1)} = 2. We also know from Proposition 2.2.2 that ∫ 2 0 f ≤ ∫ 2 0 f , so altogether 2 ≤ ∫ 2 0 f ≤ ∫ 2 0 f ≤ 2. Notice that this chain of inequalities can only hold if 2 = ∫ 2 0 f = ∫ 2 0 f = 2, hence f is integrable and ∫ 2 0 f = 2. □ 11 Week 3: Riemann’s Integrability Criterion This week we prove Riemann’s equivalent characterisation for integrability. This is an ‘ϵ-type’ characterisation which is often much easier to verify than the definition of integrability we encountered last week. In particular, we will use it to prove that monotonic functions and continuous functions on closed bounded intervals are always integrable. 3.1. An equivalent definition for integrability. The ‘ϵ-type’ characterisation for integrability proved below is known as Riemann’s Criterion for Integrability. Theorem 3.1.1 (Riemann’s Criterion). The following properties are equivalent for any bounded function f : [a, b] → R: (1) The function f is integrable. (2) For each ϵ > 0, there exists a partition P of [a, b] such that U (f, P ) − L(f, P ) < ϵ. Proof. The equivalence requires us to prove that (1) holds if and only if (2) holds: First, we prove that (1) implies (2). Assume that (1) holds, so f is integrable with ∫ b a f := ∫ b a f = ∫ b a f . Now suppose that ϵ > 0. The lower integral is the supremum ∫ b a f := supP L(f, P ), so the ‘ϵ-type’ characterisation of the supremum in Lemma 1.1.4 implies that there exists a partition P ′ of [a, b] such that ∫ b a f − ϵ 2 ≤ L(f, P ′). Moreover, the upper integral is the infimum ∫ b a f := inf P U (f, P ), so there exists a partition P ′′ of [a, b] such that ∫ b a f + ϵ 2 ≥ U (f, P ′′). We can combine the two estimates above by considering the partition P ′ ∪ P ′′ and using the Partition Lemma (Lemma 2.1.1) to note that L(f, P ′′) ≤ L(f, P ′ ∪ P ′′) and U (f, P ′) ≥ U (f, P ′ ∪ P ′′), since P ′ ⊆ P ′ ∪ P ′′ and P ′′ ⊆ P ′ ∪ P ′′. This gives ∫ b a f − ϵ 2 ≤ L(f, P ′ ∪ P ′′) and ∫ b a f + ϵ 2 ≥ U (f, P ′ ∪ P ′′). Now, subtracting the first inequality from the second inequality gives U (f, P ′ ∪ P ′′) − L(f, P ′ ∪ P ′′) ≤ (∫ b a f + ϵ 2 ) − (∫ b a f − ϵ 2 ) = ϵ, where we used the integrability assumption ∫ b a f = ∫ b a f to obtain the final equality. This proves (2). Second, we prove that (2) implies (1). Assume that (2) holds, and suppose that ϵ > 0, so there exists a partition Pϵ such that U (f, Pϵ) − L(f, Pϵ) < ϵ. We use the subscript ϵ here to show the explicit dependence of the partition on ϵ. This is in contrast to the arbitrary partition P considered in the estimate ∫ b a f := supP L(f, P ) ≥ L(f, Pϵ), which holds because the supremum is also an upper bound, and the estimate ∫ b a f := inf P U (f, P ) ≤ U (f, Pϵ) which holds because the infimum is also a lower bound. We subtract the two estimates above to obtain 0 ≤ ∫ b a f − ∫ b a f ≤ U (f, Pϵ) − L(f, Pϵ) < ϵ, 12 where the first inequality uses Proposition 2.2.2, and the last inequality is our assumption above. In summary, we have shown that 0 ≤ ∫ b a f − ∫ b a f < ϵ for all ϵ > 0, which is only possible when ∫ b a f −∫ b a f = 0, hence f is integrable by Definition 2.2.3. This proves (1). □ Let us verify Riemann’s Criterion for the two key examples (Examples 1.2.7 and 1.2.8) from Lecture 7.2. Example 3.1.2. Use Riemann’s Criterion for Integrability to prove that each of the functions below are integrable (1) f : [0, b] → [0, ∞), f (x) := x. (2) f : [0, 2] → [0, 1], f (x) := { 1, x = 1 0, x ̸= 1 Solution. (1) Let ϵ > 0. For each n ∈ N, using the partition Pn of [0, b] into n subin- tervals of equal width and the values for L(f, Pn) and U (f, Pn) in Example 1.2.7, we have U (f, Pn) − L(f, Pn) = n+1 n b 2 2 − n−1 n b 2 2 = b 2 n . To prove Riemann’s Criterion, we need to have b 2 n < ϵ or equivalently n > b 2 ϵ . Therefore, we now choose N ∈ N with N > b2 ϵ , so the above calculations give U (f, PN ) − L(f, PN ) = b2 N < ϵ. Hence f is integrable by Riemann’s Criterion. (2) Let ϵ > 0. For each δ ∈ (0, 1), using the partition Pδ of [0, 2] and the values for L(f, Pδ) and U (f, Pδ) in Example 1.2.8, we have U (f, Pδ) − L(f, Pδ) = 2 − (2 − 2δ) = 2δ. To prove Riemann’s Criterion, we need to have 2δ < ϵ or equivalently δ < ϵ 2 . Therefore, we now choose δ0 ∈ (0, 1) with δ0 < ϵ 2 , so the above calculations give U (f, Pδ0) − L(f, Pδ0) = 2δ0 < ϵ. Hence f is integrable by Riemann’s Criterion. □ In the example above, it is important to observe how ϵ > 0 remains arbitrary throughout the proof, as required by Riemann’s Criterion. In contrast, the variables n ∈ N and δ ∈ (0, 1) are initially allowed to be arbitrary in the calculations but then they are chosen, albeit dependent on ϵ, in the sense that we set n = N and δ = δ0 to produce the partitions PN and Pδ0 at the end of each part. We conclude this lecture with a few remarks and a result concerning Riemann sums and Riemann’s original definition of the integral, since you may encounter these in more advanced theory. This material is for information only and will not be assessed. The Riemann integral, as opposed to the Riemann–Darboux integral, relies on tagged partitions to define Riemann sums. A tagged partition (P, T ) of [a, b] consists of a partition P = {x0, x1, . . . , xn} of [a, b] and a subset T = {t1, . . . , tn} of [a, b] whose elements are called tags and must satisfy t1 ∈ [x0, x1], . . ., tn ∈ [xn−1, xn]. The maximum width of the subintervals defined by a such a partition is denoted ∆P := max{(x1 − x0), . . . , (xn − xn−1)}. 13 If f : [a, b] → R is bounded, then the Riemann sum R(f, P, T ) is defined by R(f, P, T ) := n∑ i=1 f (ti)(xi − xi−1). We are now in a position to state the following result, wherein property (2) is Riemann’s original definition of integrability. This result will not be assessed. Theorem 3.1.3 (Riemann’s Integral). The following properties are equivalent for any bounded function f : [a, b] → R: (1) The function f is integrable. (2) There exists s ∈ R such that for each ϵ > 0, there exists δ > 0 such that |R(f, P, T ) − s| < ϵ for all tagged partitions (P, T ) of [a, b] with ∆P < δ. Moreover, if (2) holds, then s = ∫ b a f . In Riemann’s formulation, the real number s above is defined to be the integral. This can actually be understood as a limit, written s = lim∆P →0 R(f, P, T ), but this limit must be understood using nets, which are a generalisation of sequences beyond the scope of this module. The above theorem shows, however, that the Riemann–Darboux integral and the original Riemann integral are in fact the same. An advantage of the Riemann integral, however, is that if f : [a, b] → R is known to be integrable, then it provides limiting formulas for the integral, such as ∫ b a f = limn→∞ 1 n ∑n i=1 f (ti), where {t1, . . . , tn} can be any set of tags associated with a partition of [a, b] into n subintervals of equal width (e.g. left endpoints, right endpoints, midpoints, etc.). These explicit formulas are useful in numerics and advanced analysis but we shall not pursue these here. 3.2. Integrability of monotonic functions. The following example will serve as a prototype for a proof that monotonic functions on closed bounded intervals are always integrable. Example 3.2.1 (Prototype for monotonic functions). Use Riemann’s Criterion to prove that f : [0, π 2 ] → R given by f (x) := sin(x) for all x ∈ [0, π 2 ] is integrable. Solution. Let ϵ > 0. For each n ∈ N, consider the partition Pn of [0, π 2 ] into n subintervals of equal width: Pn := {0, π 2n , 2π 2n , . . . , nπ 2n } = { iπ 2n : i = 0, 1, . . . , n}. The sine function is increasing on [0, π 2 ], so for each i ∈ {1, . . . , n}, we have mi := inf { sin(x) : x ∈ [ (i−1)π 2n , iπ 2n ] } = sin((i − 1) π 2n ), Mi := sup { sin(x) : x ∈ [ (i−1)π 2n , iπ 2n ] } = sin(i π 2n ). 14 Together, we have U (f, Pn) − L(f, Pn) = n∑ i=1 Mi(xi − xi−1) − n∑ i=1 mi(xi − xi−1) = n∑ i=1 sin(i π 2n ) π 2n − n∑ i=1 sin((i − 1) π 2n ) π 2n =   n∑ i=1 sin(i π 2n ) − n−1∑ j=0 sin(j π 2n )   π 2n = [ sin(n π 2n ) − sin(0 π 2n )] π 2n = [ sin( π 2 ) − sin(0) ] π 2n = π 2n , where in the third line we made the substitution j := i − 1. To prove Riemann’s Criterion, we need to have π 2n < ϵ or equivalently n > π 2ϵ . Therefore, we now choose N ∈ N with N > π 2ϵ , so the above calculations give U (f, PN ) − L(f, PN ) = π 2N < ϵ. Hence f is integrable by Riemann’s Criterion. □ An inspection of the above proof reveals that the method extends almost imme- diately to any bounded monotonic function f : [a, b] → R. Definition 3.2.2. A function f : [a, b] → R is monotonic if either of the following conditions hold: (1) f (x) ≤ f (y) for all x, y ∈ [a, b] such that x ≤ y (2) f (x) ≥ f (y) for all x, y ∈ [a, b] such that x ≤ y More specifically, the function is called monotonic increasing when (1) holds, and monotonic decreasing when (2) holds. Theorem 3.2.3. If f : [a, b] → R is bounded and monotonic, then f is integrable. Proof. We will only prove the result in the case of monotonic increasing functions f : [a, b] → R, since the result for monotonic decreasing functions is analogous. We adapt the approach presented in Example 3.2.1. Let ϵ > 0. For each n ∈ N, consider the partition Pn := {x0, x1, . . . , xn} of [a, b] into n subintervals of equal width. For each i ∈ {1, . . . , n}, since f is monotonic increasing, we have mi := inf{f (x) : x ∈ [xi−1, xi]} = f (xi−1), Mi := sup{f (x) : x ∈ [xi−1, xi]} = f (xi). Together, we have U (f, Pn) − L(f, Pn) = n∑ i=1(Mi − mi)(xi − xi−1) = n∑ i=1(f (xi) − f (xi−1)) b−a n = [(f (x1) − f (x0)) + (f (x2) − f (x1)) + . . . + (f (xn) − f (xn−1))] b−a n = [−f (x0) + (f (x1) − f (x1)) + . . . + (f (xn−1) − f (xn−1)) + f (xn)] b−a n = (f (xn) − f (x0)) b−a n = (f (b) − f (a))(b − a) 1 n , 15 where in the fourth line we have simply reordered the terms in the sum to show how they can be grouped into pairs that cancel (this was expressed slightly differently using summation notation in Example 3.2.1). To prove Riemann’s Criterion, we need to have (f (b) − f (a))(b − a) 1 n < ϵ or equivalently n > (f (b) − f (a))(b − a) 1 ϵ . Therefore, we now choose N ∈ N with N > (f (b) − f (a))(b − a) 1 ϵ , so the above calculations give U (f, PN ) − L(f, PN ) = (f (b) − f (a))(b − a) 1 N < ϵ. Hence f is integrable by Riemann’s Criterion. □ Week 4: Properties of Integrable Functions This week we will first use Riemann’s Criterion for Integrability to prove that continuous functions on closed bounded intervals are always integrable. It will then be applied to establish a range of useful properties for integrable functions. 4.1. Integrability of continuous functions. In this lecture, we use Riemann’s Criterion to prove that all continuous functions f : [a, b] → R are integrable. In particular, we are only concerned here with continuous functions on closed bounded intervals, so let’s recall the definition in that case. Definition 4.1.1. A function f : [a, b] → R is continuous if for each x ∈ [a, b] and each ϵ > 0, there exists δ > 0 such that the following property holds: y ∈ [a, b] and |x − y| < δ =⇒ |f (x) − f (y)| < ϵ This is the same as saying that f is continuous at each x in [a, b]. It is a subtle but extremely important observation that the value of δ is allowed to depend on both x and ϵ in the above definition. It might help to think of δ as a function δ(x, ϵ). The possible dependence on x, however, becomes problematic if we want to use Riemann’s Criterion to prove integrability. The following stronger property, known as uniform continuity, is more suited to our purposes. Definition 4.1.2. A function f : [a, b] → R is uniformly continuous if for each ϵ > 0, there exists δ > 0 such that the following property holds: x ∈ [a, b], y ∈ [a, b] and |x − y| < δ =⇒ |f (x) − f (y)| < ϵ The value of δ is only allowed to depend on ϵ in the above definition of uniform continuity, so we can think of it as a function δ(ϵ). If we replace [a, b] with an arbitrary set Ω ⊆ R in Definitions 4.1.1 and 4.1.2 , then we obtain the definitions of continuity and uniform continuity for functions f : Ω → R. In such generality, uniform continuity is a stronger property than continuity (see Example 4.1.4 below). In the case Ω = [a, b], however, there is no difference! This is the content of the following theorem (the proof will not be assessed). Theorem 4.1.3. If f : [a, b] → R is continuous, then f is uniformly continuous. Proof. This can be proved directly by using the Completeness Axiom of R (see the Appendix to Chapter 8 in Spivak). Alternatively, the more common proof relying on the Bolzano–Weierstrass theorem can be found in many real analysis texts. □ The following example shows how to work with uniform continuity. The second part is quite challenging and such questions will not be assessed. Example 4.1.4. Prove the following facts: (1) f : R → R, f (x) := cx, where c ∈ R \\ {0}, is uniformly continuous. (2) g : (0, 1] → R, g(x) := 1 x , is not uniformly continuous. 16 Proof. (1) Let ϵ > 0 and choose δ := ϵ |c| . If x ∈ R, y ∈ R and |x − y| < δ, then |f (x) − f (y)| = |cx − cy| = |c||x − y| < |c|δ = |c| ϵ |c| = ϵ, so f is uniformly continuous. (It is important to observe here that the choice of δ did not depend on x or y.) (2) Let ϵ = 1 and assume for a contradiction that g is uniformly continuous, so there exists δ > 0 such that the following property holds: x ∈ (0, 1], y ∈ (0, 1] and |x − y| < δ =⇒ | 1 x − 1 y | < 1 (4.1.1) We expect that g is not uniformly continuous because of its growth near the origin. In particular, if y = x 2 , then |x − y| = x 2 whilst | 1 x − 1 y | = 1 x (notice that 1 x → ∞ as x 2 → 0). This will contradict (4.1.1) when x 2 < δ and 1 x ≥ 1, or equivalently, when x < 2δ and x ≤ 1. Therefore, we choose x = 1 2 min{2δ, 1}, to obtain x ∈ (0, 1], y = x 2 ∈ (0, 1], |x − y| = x 2 = min{2δ,1} 4 < δ, whilst | 1 x − 1 y | = 1 x = 2 min{2δ,1} ≥ 2, which contradicts (4.1.1). This proves that g is not uniformly continuous. □ We can now rely on uniform continuity to prove the following result. Theorem 4.1.5. If f : [a, b] → R is continuous, then f is integrable. Proof. We let ϵ > 0 with the aim of verifying Riemann’s Criterion to prove that f is integrable. We know from Theorem 4.1.3 that f is uniformly continuous, so we can choose δ > 0, depending only on ϵ, such that the following property holds: x ∈ [a, b], y ∈ [a, b] and |x − y| < δ =⇒ |f (x) − f (y)| < ϵ b − a (4.1.2) Now let Pδ = {x0, x1, . . . , xn} denote a partition of [a, b] into n subintervals of equal width less than δ. This means that |x1 − x0| = |x2 − x1| = . . . = |xn − xn−1| < δ. For each i ∈ {1, . . . , n}, we then have s ∈ [xi−1, xi], t ∈ [xi−1, xi] =⇒ |s − t| ≤ |xi − xi−1| < δ =⇒ |f (s) − f (t)| < ϵ b − a , where we used (4.1.2) to obtain the final implication. It follows (using the ‘ϵ-type’ interpretation of the infimum/supremum in Lemma 1.1.4) that sup{f (s) : s ∈ [xi−1, xi]} − inf{f (t) : t ∈ [xi−1, xi]} < ϵ b − a . Altogether, we obtain U (f, Pδ) − L(f, Pδ) = n∑ i=1(Mi − mi)(xi − xi−1) < ϵ b − a n∑ i=1(xi − xi−1) = ϵ. This proves that f is integrable by Riemann’s Criterion. □ 4.2. Restriction/extension, linearity, algebra and estimates. The following result shows that integrability is preserved when the domain of a function is suitably restricted or extended. It is important to know this result but the proof is somewhat tedious and technical, so only a sketch is provided and you are not expected to be familiar with the details. Theorem 4.2.1 (Restriction and extension of the integral). Suppose that c ∈ [a, b]. The following properties hold for any bounded function f : [a, b] → R: (1) If f is integrable on both [a, c] and [c, b], then f is integrable on [a, b]. 17 (2) If f is integrable on [a, b], then f is integrable on both [a, c] and [c, b]. In both cases, the identity ∫ b a f = ∫ c a f + ∫ b c f holds. Sketch of the proof. This is a technical proof that seems more complicated than it actually is when written down. We only sketch the main details here. It is a good exercise to transform this into a complete proof yourself. We consider three parts: To prove (1), we use Riemann’s Criterion. If f is integrable on [a, c] and [c, d], then there exists a partition P1 of [a, c] and a partition P2 of [c, b] such that U (f |[a,c], P1) − L(f |[a,c], P1) < ϵ U (f |[c,b], P2) − L(f |[c,b], P2) < ϵ. (4.2.1) The union P := P1 ∪ P2 is a partition of [a, b] and you can prove that U (f, P ) = U (f |[a,c], P1) + U (f |[c,b], P2) L(f, P ) = L(f |[a,c], P1) + L(f |[c,b], P2) to deduce that U (f, P )−L(f, P ) < 2ϵ. The required integrability would then follow from Riemann’s Criterion if the 2ϵ here was replaced by ϵ, but this can be achieved by reverse engineering the original choice of ϵ in (4.2.1). To prove (2), we again use Riemann’s Criterion. If f is integrable on [a, b], then there exists a partition P of [a, b] such that U (f, P ) − L(f, P ) < ϵ. We need to use P to extract partitions of [a, c] and [c, b]. We can indeed split P into those points contained in [a, c] and those contained in [c, d], but these won’t necessarily be partitions, because they may not contain the endpoint c. This is remedied by starting with the partition P ∪ {c} and noting (by the Partition Lemma) that U (f, P ∪ {c}) − L(f, P ∪ {c}) ≤ U (f, P ) − L(f, P ) < ϵ. It is then possible to write P ∪ {c} = P1 ∪ P2, where P1 is a partition of [a, c] and P2 is a partition of [c, b]. The required integrability results will follow from Riemann’s Criterion if you can prove that U (f, P1) − L(f, P1) < ϵ and U (f, P2) − L(f, P2) < ϵ. To prove the identity, we recall the definition of the Riemann–Darboux integral as the supremum of lower sums and the infimum of upper sums. In particular, for any partition P of [a, b], writing P ∪ {c} = P1 ∪ P2 as above, you can prove that L(f |[a,c], P1) ≤ ∫ c a f ≤ U (f |[a,c], P1) (4.2.2) L(f |[c,b], P2) ≤ ∫ b c f ≤ U (f |[c,b], P2) (4.2.3) L(f, P ) ≤ L(f, P ∪ {c}) ≤ ∫ c a f + ∫ b c f ≤ U (f, P ∪ {c}) ≤ U (f, P ). (4.2.4) The definition of the integral then implies that ∫ b a f = sup P L(f, P ) ≤ ∫ c a f + ∫ b c f ≤ inf P U (f, P ) = ∫ b a f and the required identity follows. □ The following result shows that the integral is as a linear operator. Theorem 4.2.2 (Linearity of the integral). If α ∈ R and f, g : [a, b] → R are both bounded and integrable, then f + g and αf are both bounded and integrable with (1) ∫ b a (f + g) = ∫ b a f + ∫ b a g (2) ∫ b a (αf ) = α ∫ b a f Proof. We only prove the results for f + g here. The proof of the results for αf , which follows a similar approach, is posed as a question on the problem sheet. 18 Suppose that f, g : [a, b] → R are both bounded and integrable. It is left as an exercise to prove that f + g is bounded. Next, let ϵ > 0, so Riemann’s Criterion guarantees that there exist partitions P ′ and P ′′ of [a, b] such that U (f, P ′) − L(f, P ′) < ϵ 2 and U (g, P ′′) − L(g, P ′′) < ϵ 2 . We can combine the two estimates above by defining the partition P := P ′ ∪ P ′′ and using the Partition Lemma (Lemma 2.1.1) to note that L(f, P ′) ≤ L(f, P ) and U (f, P ′) ≥ U (f, P ), hence U (f, P ) − L(f, P ) ≤ U (f, P ′) − L(f, P ′), since P ′ ⊆ P and P ′′ ⊆ P , and similarly U (g, P ) − L(g, P ) ≤ U (f, P ′′) − L(f, P ′′). This gives U (f, P ) − L(f, P ) < ϵ 2 and U (g, P ) − L(g, P ) < ϵ 2 . Furthermore, adding these two inequalities, we arrive at [U (f, P ) + U (g, P )] − [L(f, P ) + L(g, P )] < ϵ 2 + ϵ 2 = ϵ. (4.2.5) It is a good exercise to prove that U (f + g, P ) ≤ U (f, P ) + U (g, P ) L(f + g, P ) ≥ L(f, P ) + L(g, P ). (4.2.6) It follows that U (f + g, P ) − L(f + g, P ) < ϵ, hence f + g is integrable by Riemann’s Criterion. In order to prove the equality of the associated integrals, however, we must recall the definition of the Riemann–Darboux integral as the supremum of lower sums and the infimum of upper sums. In particular, we have L(f, P ) ≤ ∫ b a f ≤ U (f, P ) (4.2.7) L(g, P ) ≤ ∫ b a g ≤ U (g, P ) (4.2.8) L(f + g, P ) ≤ ∫ b a (f + g) ≤ U (f + g, P ). (4.2.9) These inequalities hold for arbitrary partitions, but here P continues to denote the partition P ′ ∪ P ′′ from above. Using (4.2.6), inequality (4.2.9) becomes L(f, P ) + L(g, P ) ≤ ∫ b a (f + g) ≤ U (f, P ) + U (g, P ) (4.2.10) Now, subtracting (4.2.7) and (4.2.8) from (4.2.10), we obtain [L(f, P ) + L(g, P )] − [U (f, P ) + U (g, P )] ≤ ∫ b a (f + g) − (∫ b a f + ∫ b a g) ≤ [U (f, P ) + U (g, P )] − [L(f, P ) + L(g, P )]. Using (4.2.5), it follows that −ϵ < ∫ b a (f + g) − (∫ b a f + ∫ b a g) < ϵ. This holds for all ϵ > 0, which implies that ∫ b a (f + g) − (∫ b a f + ∫ b a g) = 0, hence ∫ b a (f + g) = ∫ b a f + ∫ b a g, as required. □ Here is an example of how the extension and linearity properties of the integral from the previous two theorems can be applied. Example 4.2.3. Prove that f : [0, 2] → R given by f (x) := { sin(1) + 3, x = 1 sin(x), x ̸= 1 for all x ∈ [0, π] is integrable. 19 Proof. In order to use the linearity of the integral, we write f = g + 3h, where g(x) := sin(x) and h(x) := { 1, x = 1 0, x ̸= 1 are both bounded functions on [0, 2]. The function g is monotonic increasing [0, π 2 ] and monotonic decreasing on [ π 2 , 2], so g is integrable on both [0, π 2 ] and [ π 2 , 2] by the integrability of monotonic functions in Theorem 3.2.3. The extension of integrability in Theorem 4.2.1 then implies that g is integrable on [0, 2]. Meanwhile, the function h is integrable on [0, 2] by Example 2.2.6. The linearity of the integral in Theorem 4.2.2 thus implies that f = g + 3h is integrable on [0, 2]. □ In order to extend our definition of integrability to the class of bounded R-valued functions f : [a, b] → R, we write f = f + − f −, where f + and f − are referred to respectively as the positive part of f and the negative part of f , defined as f +(x) := max{f (x), 0} = {f (x), if f (x) ≥ 0 0, otherwise and f −(x) := − min{f (x), 0} = {−f (x), if f (x) ≤ 0 0, otherwise . The linearity of the integral in Theorem 4.2.2 suggests the following extension of the definition of integrability from Definition 2.2.3. Definition 4.2.4. A bounded function f : [a, b] → R is called integrable if both f + and f − are integrable (according to Definition 2.2.3). If such f is integrable, then the integral of f is ∫ b a f := ∫ b a f + − ∫ b a f −. The following example shows how to work with this definition. The result is often useful when dealing with R-valued integrable functions. Example 4.2.5. Prove that if f : [a, b] → R is bounded and integrable, then |f | : [a, b] → R, given by |f |(x) := |f (x)| for all x ∈ [a, b], is bounded and integrable. Proof. The fact that |f | is bounded follows from the fact that f is bounded. It is a good exercise to write out the details. The integrability of f , meanwhile, means that both f + and f − are integrable. After we observe that |f | = f + + f −, the linearity of the integral in Theorem 4.2.2 implies that |f | is integrable. □ The attentive reader will note that all of the results presented so far this week have actually been stated for R-valued functions, whereas until now, our definition of integrability only applied to [0, ∞)-valued functions. To correct this, one could state Definition 4.2.4 at the start of the week, and then check that all of our results can be adapted to the case of R-valued functions. We chose to avoid this approach, however, in order to use the linearity of the integral in Theorem 4.2.2 to motivate Definition 4.2.4. Our next aim is to prove that a product of two integrable functions is itself integrable. The following lemma contains the heart of the matter. The proof is left as a challenging exercise. Lemma 4.2.6. If f : [a, b] → R is bounded and integrable, then f 2 is bounded and integrable. We can now prove that integrability is preserved by products of functions. Theorem 4.2.7 (Algebra of integrability). If f, g : [a, b] → R are both bounded and integrable, then f g is bounded and integrable. 20 Proof. In order to apply Lemma 4.2.6, we observe the identity f g = 1 4 (f + g)2 − 1 4 (f − g) 2, which can be proved by expanding the right-hand side. In particular, if f and g are both integrable, then the linearity of the integral in Theorem 4.2.2 implies that (f + g) and (f − g) are both bounded and integrable. The algebra of integrability in Lemma 4.2.6 then implies that (f + g) 2 and (f − g) 2 are both bounded and integrable. Another application of the linearity of the integral thus finally implies that 1 4 (f + g)2 − 1 4 (f − g) 2 = f g is bounded and integrable, as required. □ We conclude by proving some useful bounds for integrals. These will be essential for proving the Fundamental Theorem of Calculus next week. Theorem 4.2.8 (Estimates for the integral). If f : [a, b] → R is integrable and m ≤ f (x) ≤ M for all x ∈ [a, b] and some m, M ∈ R, then m(b − a) ≤ ∫ b a f ≤ M (b − a). Proof. Suppose that f satisfies the hypotheses in the theorem and consider the partition P0 := {a, b} of [a, b]. Observe that m1 := inf{f (x) : x ∈ [a, b]} ≥ m hence L(f, P0) := 1∑ i=1(xi − xi−1) = m1(b − a) ≥ m(b − a). We combine this with the definition of the integral, since f is integrable, to obtain ∫ b a f := ∫ b a f := supP L(f, P ) ≥ L(f, P0) ≥ m(b − a), as required. A similar argument implies the required upper bound for the integral. It is a good exercise to write out the details. □ 21 Week 5: The Fundamental Theorem of Calculus The theory of integration culminates this week with the proof of two theorems known collectively as the Fundamental Theorem of Calculus. These results reveal how integration and differentiation are inverse operations. The first allows us to differentiate certain integral expressions whilst the second allows us to compute integrals using so-called antiderivatives. The latter result also remedies the main shortcoming of the theory developed so far by providing a method for computing integrals as routine calculations. 5.1. The First Fundamental Theorem of Calculus. This first part of the theorem is concerned with the differentiation of certain integral functions. Theorem 5.1.1 (First Fundamental Theorem of Calculus). Let f : [a, b] → R denote a bounded integrable function and suppose that F : [a, b] → R is given by F (x) := ∫ x a f for all x ∈ [a, b]. If f is continuous at c, where c ∈ [a, b], then F is differentiable at c with derivative F ′(c) = f (c). Proof. Let f and F be defined as in the theorem. We only consider the case when c ∈ (a, b), in which case we need to prove that lim h→0+ F (c + h) − F (c) h = f (c) = lim h→0− F (c + h) − F (c) h . It is a good exercise to adjust this proof to the remaining cases when c ∈ {a, b}. In what follows, we only consider when h > 0, since the arguments when h < 0 are analogous. Using the restriction and extension properties of the integral in Theorem 4.2.1, we obtain F (c + h) − F (c) = ∫ c+h a f − ∫ c a f = (∫ c a f + ∫ c+h c f ) − ∫ c a f = ∫ c+h c f. Next, we use the estimates for integrals in Theorem 4.2.8 to obtain mhh = mh((c + h) − c) ≤ ∫ c+h c f ≤ Mh((c + h) − c) = Mhh, where mh := inf{f (x) : x ∈ [c, c + h]} and Mh := sup{f (x) : x ∈ [c, c + h]}. Altogether, we have mh ≤ F (c + h) − F (c) h ≤ Mh. We now use that f is continuous at c to obtain limh→0+ mh = f (c) = limh→0+ Mh. A good diagram can help to convince you of these limits but it is a better exercise to try and write out a proof using ϵ-type arguments. The required result then follows from the Sandwich Theorem. □ The First Fundamental Theorem of Calculus shows how differentiation is an inverse of integration. We can see this more explicitly by using Leibniz notation to express the conclusion of the theorem as d dx (∫ x a f )∣ ∣ ∣ ∣x=c = f (c). Moreover, if f is actually continuous (meaning continuous on [a, b] rather than just continuous at a single point c), then this becomes d dx (∫ x a f ) = f (x) for all x ∈ [a, b]. It is helpful to record this particular case of the theorem as a separate corollary. 22 Corollary 5.1.2. If f : [a, b] → R is continuous and F : [a, b] → R is given by F (x) := ∫ x a f for all x ∈ [a, b], then F is differentiable with derivative F ′(x) = f (x) for all x ∈ [a, b]. Proof. If f : [a, b] → R is continuous, then it is bounded (by the Extreme Value Theorem) and integrable (by Theorem 4.1.5). The hypotheses of Theorem 5.1.1 thus hold for all c ∈ [a, b] from which the result follows. □ The First Fundamental Theorem of Calculus is particularly sensitive to the form of the function F (x) := ∫ x a f which appears in the statement of the theorem. For example, if the variable x appears as the lower endpoint of the integral, or if it is replaced by some other function of x, then the theorem does not give a formula for the derivative. The following example, however, shows how the theorem can be combined effectively with other results in such contexts. Example 5.1.3. Prove that the following functions are differentiable and find a formula for their derivatives: (1) G : [0, 1] → R, G(x) := ∫ x3 0 √1 − t4 dt (2) H : [0, 1] → R, H(x) := ∫ 1 x √1 − t4 dt Solution. Let F : [0, 1] → R be given by F (x) := ∫ x 0 √1 − t4 dt for all x ∈ [0, 1]. The function f : [0, 1] → R given by f (t) := √1 − t4 for all t ∈ [0, 1] is continuous, as a composition of a nonnegative-valued polynomial and the square root function. The First Fundamental Theorem of Calculus for continuous functions in Corollary 5.1.2 thus implies that F is differentiable with F ′(x) = f (x) = √1 − x4 for all x ∈ [0, 1] (1) Observe that G(x) = F (x3), so G = F ◦ g, where g(x) := x3 is differentiable. We also know that F is differentiable by the preceding discussion, so the Chain Rule implies that G is differentiable with derivative G ′(x) = F ′(g(x))g′(x) = √1 − (x3)4 3x2 = √1 − x12 3x 2 for all x ∈ [0, 1]. (2) Here we can use the restriction and extension properties of the integral in Theorem 4.2.1 to write H(x) = ∫ 1 x √1 − t4 dt = ∫ 1 0 √1 − t4 dt − ∫ x 0 √1 − t4 dt = C − F (x) for all x ∈ [0, 1], where C := ∫ 1 0 √1 − t4 dt ∈ R. A constant function has deriva- tive identically equal to 0, and we know that F is differentiable by the preceding discussion, so H is differentiable as a linear combination of differentiable functions with derivative H ′(x) = 0 − F ′(x) = −√1 − x4 for all x ∈ [0, 1]. 5.2. The Second Fundamental Theorem of Calculus. This second part of the theorem is concerned with the integration of certain derivative functions and leads ultimately to the consideration of antiderivatives. Theorem 5.2.1 (Second Fundamental Theorem of Calculus). If f : [a, b] → R is a bounded integrable function, and there exists a differentiable function g : [a, b] → R such that f = g′, then ∫ b a f = g(b) − g(a). Proof. We only present the proof when f = g′ is assumed to be continuous. In that case, it follows from the First Fundamental Theorem of Calculus for continuous functions in Corollary 5.1.2 that if F : [a, b] → R is given by F (x) := ∫ x a f , then F ′(x) = f (x), for all x ∈ [a, b]. We then have F ′ = f = g′, so by Example 5.2.3, there exists C ∈ R such that F (x) = g(x) + C for all x ∈ [a, b]. In particular, since F (a) = ∫ a a f = 0, we find 0 = g(a) + C so C = −g(a), whence F (x) = g(x) − g(a) for all x ∈ [a, b]. The result follows since ∫ b a f = F (b) = g(b) − g(a). 23 In the case when f = g′ is only assumed to be bounded and integrable, this result is not a corollary of the First Fundamental Theorem of Calculus. It relies instead on using the Mean Value Theorem to prove that L(f, P ) ≤ g(b) − g(a) ≤ U (f, P ) for all partitions P of [a, b]. The result then follows by observing that ∫ b a f = supP L(f, P ) ≤ g(b) − g(a) ≤ inf P U (f, P ) = ∫ b a f. We do not detail this here, however, since such applications of the Mean Value Theorem will be considered in a second-year analysis module. □ The Second Fundamental Theorem of Calculus shows how integration is the inverse of differentiation, since the conclusion of the theorem implies that ∫ b a g′ = g(b) − g(a). The theorem actually asserts more than this fact, however, since it shows that any differentiable function g satisfying f = g′ can be used to compute the integral of f using the formula ∫ b a f = g(b) − g(a). We call such functions antiderivatives of f in accordance with the following definition. Definition 5.2.2. If Ω ⊆ R and f : Ω → R, then an antiderivative of f is a differentiable function g : Ω → R with the property that f (x) = g′(x) for all x ∈ Ω. We use the notation ∫ f to denote an antiderivative of f , which simply means that∫ f is a differentiable function such that (∫ f ) ′ = f . It is important to note here that a single function f may have many different antiderivatives, so the notation ∫ f does not represent a unique function associated with f . This is precisely why the definition above refers to an antiderivative (and not the antiderivative) of f . In the case Ω = [a, b], however, the example below characterises all antiderivatives for a bounded integrable function f : [a, b] → R. In particular, if g is an antiderivative of such a function f , then any antiderivative of f must satisfy ∫ f = g + C for some C ∈ R. Example 5.2.3. Suppose that f : [a, b] → R is bounded and integrable: (1) Prove that if g and h are antiderivatives of f , then there exists C ∈ R such that g(x) = h(x) + C for all x ∈ [a, b]. (2) Prove that an antiderivative of f exists when f is continuous. Proof. (1) If g and h are antiderivatives of f , then f (x) = g′(x) = h ′(x) whence (g − h) ′(x) = 0 for all x ∈ [a, b]. This means that g − h is monotonic increasing (because (g − h) ′(x) ≥ 0) and monotonic decreasing (because (g − h) ′(x) ≤ 0). This is only possible, however, if (g − h)(x) = (g − h)(a) (or (g − h)(b)) for all x ∈ [a, b]. The result follows with C := g(a) − h(a). (2) The First Fundamental Theorem of Calculus for continuous functions in Corollary 5.1.2 shows that the function F : [a, b] → R given by F (x) := ∫ x a f satisfies F ′(x) = f (x) for all x ∈ [a, b], hence F is an antiderivative of f . □ The example above shows that any continuous function f : [a, b] → R has an antiderivative given by ∫ x a f for all x ∈ [a, b]. This does not mean, however, that the antiderivative of any continuous function can be expressed in terms of elementary functions. On the contrary, differential Galois Theory shows that the antiderivatives for most continuous functions (even those given by simple expressions such as e−x2 and √1 − x4) cannot be expressed in terms of elementary functions. In any assessment for this module, however, when you are asked to find an antiderivative of a given function f , it is always expected that your answer should be in terms of elementary functions (so writing ∫ x a f will not do!). 24 Example 5.2.4. Find an antiderivative of the function f in each case below: (1) f : R → R, f (x) := xα, α ∈ R \\ {−1} (2) f : R \\ {0} → R, f (x) := x−1 Solution. (1) The function g : R → R given by g(x) := xα+1/(α + 1) for all x ∈ [a, b] is differentiable with g′(x) = xα = f (x) for all x ∈ [a, b]. This proves that g is an antiderivative of f . (This proof breaks down when α = −1, since then g(x) = 0 with g′(x) = 0 so g′ ̸= f .) (2) The function h : R \\ {0} → R given by h(x) := log |x| for all x ∈ R \\ {0} is well-defined (the logarithm function has domain (0, ∞)) and differentiable with h ′(x) = { d dx (log(x)), if x > 0 d dx (log(−x)), if x < 0 = { 1 x , if x > 0 −( 1 −x ), if x < 0 = f (x) for all x ∈ R \\ {0}, where we used the Chain Rule to obtain the second equality. This proves that h is an antiderivative of f . Week 7: Integral Calculus The Fundamental Theorem of Calculus provides a bridge connecting our newly acquired understanding of the Riemann–Darboux integral from analysis with our previous experience of integral calculus. We will use it this week to establish the familiar calculus techniques for integrating products and compositions of functions as theorems known as Integration by Parts and the Substitution Formula. We will also see how the antiderivative of any rational function can, in principle, be found using the method of Partial Fractions. 7.1. Product and composite functions. Here we combine the Fundamental Theorem of Calculus with the Product Rule and Chain Rule for derivatives to prove the familiar formulas for integrals of products and compositions of functions. Theorem 7.1.1 (Integration by Parts). If u and v : [a, b] → R are differentiable with bounded integrable derivatives u ′ and v′, then ∫ b a uv′ = [uv]b a − ∫ b a u ′v and ∫ uv′ = uv − ∫ u ′v where [ uv]b a := u(b)v(b) − u(a)v(a). Proof. The functions u and v are assumed to be differentiable so the Product Rule implies that uv is differentiable with derivative (uv) ′ = u ′v + uv′. Moreover, the functions u and v are continuous and hence (bounded) integrable by Theorem 4.1.5. The derivatives u ′ and v′ are also assumed to be bounded integrable, so by the algebra of integrability in Theorem 4.2.7, the products u ′v and uv′ are bounded integrable, and in turn, by the linearity of the integral in Theorem 4.2.2, the sum u ′v + uv′ is bounded integrable with ∫ b a (u ′v + uv′) = ∫ b a u′v + ∫ b a uv′. Meanwhile, the Second Fundamental Theorem of Calculus from Theorem 5.2.1 applied with f = u′v + uv′ and g = uv implies that ∫ b a (u′v + uv′) = (uv)(b) − (uv)(a). Altogether, we have ∫ b a uv′ = u(b)v(b) − u(a)v(a) − ∫ b a u ′v, as required. 25 The equality for antiderivatives ∫ uv′ = uv − ∫ u′v follows because (∫ uv′) ′ = uv′ whilst the Product Rule implies that (uv − ∫ u ′v)′ = (uv)′ − (∫ u ′v) ′ = uv′ + u ′v − u ′v = uv′. To understand this more, recall the meaning of the symbol ∫ in Definition 5.2.2. □ The next example shows some useful strategies involving Integration by Parts. Example 7.1.2. Find the following integrals and antiderivatives: (1) ∫ π 2 0 x cos(x) dx (2) An antiderivative of f : (0, ∞) → R, f (x) := log(x), x ∈ (0, ∞). (3) ∫ x2e x dx (4) ∫ ex cos(x) dx Solution. (1) We apply Integration by Parts with { u(x) = x v(x) = sin(x) and {u ′(x) = 1 v′(x) = cos(x) to obtain ∫ π 2 0 x cos(x) dx = [x sin(x)] π 2 0 − ∫ π 2 0 sin(x) dx = π 2 − [− cos(x)] π 2 0 = π 2 − 1. (2) We apply Integration by Parts with { u(x) = log(x) v(x) = x and { u ′(x) = 1 x v′(x) = 1 to obtain ∫ log(x) dx = x log(x) − ∫ ( 1 x ) x dx = x log(x) − x. (3) We apply Integration by Parts with { u(x) = x2 v(x) = ex and { u ′(x) = 2x v′(x) = ex to obtain ∫ x2ex dx = x2ex − ∫ 2xex dx. Integration by Parts (again) with { u(x) = x v(x) = ex and { u ′(x) = 1 v′(x) = ex gives ∫ xex dx = xex − ∫ ex dx = (x − 1)ex. Altogether, we have ∫ x2ex dx = x2ex − 2(x − 1)ex = (x2 − 2x + 2)ex. (4) We apply Integration by Parts with { u(x) = ex v(x) = sin(x) and {u ′(x) = ex v′(x) = cos(x) to obtain ∫ ex cos(x) dx = ex sin(x) − ∫ ex sin(x) dx. Integration by Parts with { u(x) = ex v(x) = − cos(x) and { u ′(x) = e x v′(x) = sin(x) 26 gives ∫ ex sin(x) dx = −ex cos(x) + ∫ ex cos(x) dx. Altogether, we have ∫ ex cos(x) dx = ex sin(x) − (−ex cos(x) + ∫ ex cos(x) dx) and after rearranging this becomes ∫ ex cos(x) dx = 1 2 (ex sin(x) + ex cos(x)). The following proof is somewhat technical and the details will not be assessed since they rely on the Extreme Value Theorem and Intermediate Value Theorem, which will be considered in a second-year analysis module. The main point, how- ever, is to recognise that the theorem provides a rigorous justification for the formal algebraic manipulation “ du dx dx = du” made when changing variables in integral cal- culus. In particular, the term u ′(x) dx appearing in the formula below becomes du dx dx in Leibniz notation. Theorem 7.1.3 (Substitution Formula). If f : R → R is a continuous function 1 and u : [a, b] → R is differentiable with bounded integrable derivative u ′, then ∫ b a f (u(x))u ′(x) dx = ∫ u(b) u(a) f (x) dx. Proof. Let F denote an antiderivative of f on the range of u. 2 The function f is assumed to be continuous so the existence of such an antiderivative is guaranteed by Example 5.2.3. This means that F is differentiable with derivative F ′ = f on the range of u. Meanwhile, the function u is assumed to be differentiable, so the Chain Rule implies that the composition F ◦ u is differentiable with derivative (F ◦ u) ′(x) = F ′(u(x))u ′(x) = f (u(x))u ′(x). We write this as (f ◦ u)u ′ = (F ◦ u) ′ and apply the Second Fundamental Theorem of Calculus from Theorem 5.2.1 to obtain ∫ b a (f ◦ u)u ′ = ∫ b a (F ◦ u) ′ = (F ◦ u)(b) − (F ◦ u)(a) = F (u(b)) − F (u(a)) = ∫ u(b) u(a) F ′ = ∫ u(b) u(a) f, where the Fundamental Theorem of Calculus was used in the second and fourth equalities, and the final equality uses the antiderivative property F ′ = f . □ The following example shows how to apply this result. Example 7.1.4. Find the value of ∫ 2 1 e−3x 4+1x3 dx. 1The proof shows that f actually only needs to be defined and continuous on the range of u. 2The Extreme Value Theorem and the Intermediate Value Theorem imply that the range of u is the bounded interval [m, M ] given by m := min{u(x) : x ∈ [a, b]} and M := max{u(x) : x ∈ [a, b]}. 27 Solution. Let u(x) := −3x4 + 1 for all x ∈ [1, 2], so u ′(x) = −12x3, u(1) = −2 and u(2) = −47. Applying the Substitution Formula from Theorem 7.1.3 we obtain ∫ 2 1 e−3x 4+1x3 dx = − 1 12 ∫ 2 1 eu(x)u ′(x) dx = − 1 12 ∫ −47 −2 e x dx = − 1 12 (e−47 − e−2). Notice how the order of the endpoints in the transformed integral conforms with Theorem 7.1.3, which in this case meant integrating “backwards” from -2 to -47. 7.2. Rational functions. The method of partial fractions allows us to find an antiderivative ∫ p(x) q(x) dx of a rational function p q , where p and q are polynomials, as follows: (1) If the degree of p is greater than or equal to that of q, then apply polynomial long division to reduce to an expression in which this is not the case. (2) Factorise q(x) into linear factors (ax + b) and irreducible quadratic factors (ax2 + bx + c) which have no real roots. (3) Use Table 7.2 to choose the correct partial fraction expansion for each of the factors in q(x) and then determine the appropriate coefficients. (4) Use the partial fraction expansion to find an antiderivative of p q . Factor Expression Partial Fraction Expansion linear ax + b A ax + b repeated linear (ax + b)N A1 ax + b + A2 (ax + b)2 + . . . + AN (ax + b)N quadratic ax2 + bx + c Ax + B ax2 + bx + c repeated quadratic (ax2 + bx + c) N A1x + B1 ax2 + bx + c + . . . + AN x + BN (ax2 + bx + c)N Table 7.1. Partial Fraction Expansions The following example demonstrates the method. Example 7.2.1. Evaluate ∫ x4 − 2x2 + 4x + 1 x3 − x2 − x + 1 dx. Solution. We use polynomial long division to obtain x4 − 2x2 + 4x + 1 x3 − x2 − x + 1 = (x + 1) + 4x x3 − x2 − x + 1 , so we have ∫ x4 − 2x2 + 4x + 1 x3 − x2 − x + 1 dx = x2 2 + x + ∫ 4x x3 − x2 − x + 1 dx. 28 The factorisation x 3 − x2 − x + 1 = (x − 1)2(x + 1) and Table 7.2 show that the correct partial fraction expansion for the last antiderivative above is 4x x3 − x2 − x + 1 = A x + 1 + B x − 1 + C (x − 1)2 . To determine the coefficients A, B and C, we add these fractions to obtain 4x = A(x − 1)2 + B(x + 1)(x − 1) + C(x + 1) and equating the coefficients of x2, x and x 0 gives the simultaneous equations    A + B = 0 −2A + C = 4 A − B + C = 0 which have the solution A = −1, B = 1 and C = 2. Altogether, we have ∫ x4 − 2x2 + 4x + 1 x3 − x2 − x + 1 dx = x2 2 + x + ∫ ( −1 x + 1 + 1 x − 1 + 2 (x − 1)2 ) dx = x2 2 + x − log |x + 1| + log |x − 1| − 2 x − 2 . The following two antiderivatives often arise in the method of partial fractions. If a > 0, then using d dx (tan −1 x) = (x2 + 1)−1 and the Chain Rule, we obtain ∫ 1 x2 + a2 dx = 1 a tan −1 ( x a ) (7.2.1) for all x ∈ R. If Ω ⊆ R and f : Ω → R is a differentiable function with continuous derivative f ′, then using Example 5.2.4 and the Chain Rule, we obtain ∫ f ′(x) f (x) dx = log |f (x)| (7.2.2) for all x ∈ Ω such that f (x) ̸= 0. The following example shows how these arise. Example 7.2.2. Evaluate ∫ x x3 − 1 dx. Solution. The factorisation x 3 −1 = (x−1)(x2 +x+1) can be obtained by observing that 1 is a root of the cubic and then dividing by the factor (x − 1). The quadratic factor x2 + x + 1 is irreducible, because it does not have any real roots, so Table 7.2 shows that the correct partial fraction expansion for this antiderivative is x x3 − 1 = A x − 1 + Bx + C x2 + x + 1 . To determine the coefficients A, B and C, we add these fractions to obtain x = A(x2 + x + 1) + (Bx + C)(x − 1) and equating the coefficients of x 2, x and x0 shows that A = 1 3 , B = − 1 3 and C = 1 3 . We now have ∫ x x3 − 1 dx = ∫ ( 1 3 x − 1 + −x 3 + 1 3 x2 + x + 1 ) dx = 1 3 log |x − 1| − 1 3 ∫ x − 1 x2 + x + 1 dx. To find an expression for the remaining antiderivative, we aim to express it as a linear combination of the antiderivatives in (7.2.1) and (7.2.2) with f : R → R given by f (x) := x2 + x + 1 = (x + 1 2 )2 + 3 4 ̸= 0 for all x ∈ R. In particular, since f ′(x) = 2x + 1, our aim is to find two real numbers a and b such that x − 1 x2 + x + 1 = a f ′(x) f (x) + b 1 f (x) = a(2x + 1) + b x2 + x + 1 . 29 This holds provided that x − 1 = a(2x + 1) + b, and equating the coefficients of x and x0 reveals that 2a = 1 whilst a + b = −1, thus a = 1 2 and b = − 3 2 . We combine the resulting expression with (7.2.1) and (7.2.2) to obtain ∫ x − 1 x2 + x + 1 dx = 1 2 ∫ 2x + 1 x2 + x + 1 dx − 3 2 ∫ 1 (x + 1 2 )2 + 3 4 dx = 1 2 log |x2 + x + 1| − 3 2 2√3 tan−1( 2√3 (x + 1 2 )) so finally ∫ x x3 − 1 dx = 1 3 log |x − 1| − 1 6 log |x2 + x + 1| + 1√3 tan−1( 1√3 (2x + 1)). 30 Week 8: Integral Calculus (Continued) This week we complete our survey of integral calculus techniques by developing strategies for integrating trigonometric functions, considering recursive formulas for antiderivatives also known as reduction formulas, and introducing trigonometric substitutions for integrating irrational functions. 8.1. Trigonometric functions. We developing a general strategy for finding the antiderivative ∫ sin m(x) cos n(x) dx, where m, n ∈ N, by considering the following three cases: (1) If n is an odd number, then n = 2k + 1 for some k ∈ N ∪ {0}, so we substitute u(x) = sin(x), noting that u ′(x) = cos(x), to obtain ∫ sin m(x) cos 2k+1(x) dx = ∫ sin m(x) cos 2k(x) cos(x) dx = ∫ sin m(x)(1 − sin 2(x)) k cos(x) dx = ∫ u m(1 − u 2)k du, where we used cos2(x) = 1−sin 2(x) in the second line. This reduces matters to finding the antiderivative of a polynomial. (2) If m is an odd number, then m = 2k + 1 for some k ∈ N ∪ {0}, so we substitute u(x) = cos(x), noting that u ′(x) = − sin(x), to obtain ∫ sin 2k+1(x) cos n(x) dx = ∫ sin 2k(x) cos n(x) sin(x) dx = − ∫ (1 − cos2(x))k cosn(x)(− sin(x)) dx = − ∫ (1 − u 2)ku n du, where we used cos2(x) = 1−sin 2(x) in the second line. This reduces matters to finding the antiderivative of a polynomial. (3) If n and m are both even numbers, then the double-angle identities below can be used either directly or to reduce to the above cases: sin 2(x) = 1 2 (1 − cos(2x)) cos2(x) = 1 2 (1 + cos(2x)) sin(x) cos(x) = 1 2 sin(2x) (8.1.1) The key idea in this approach is to isolate odd powers of the trigonometric functions and then use appropriate substitutions to reduce to polynomial antiderivatives. The method can also be applied sometimes when n and m are integers, rather than just natural numbers, as the following example shows. Example 8.1.1. Find the following antiderivatives: (1) ∫ cos3(x) dx (2) ∫ sin 7(x) cos4(x) dx (3) ∫ cos4(x) dx. 31 Solution. (1) We substitute u(x) = sin(x), noting that u′(x) = cos(x), to obtain ∫ cos3(x) dx = ∫ (1 − sin 2(x)) cos(x) dx = ∫ (1 − u 2) du = sin(x) − 1 3 sin 3(x). (2) We substitute u(x) = cos(x), noting that u ′(x) = − sin(x), to obtain ∫ sin 7(x) cos4(x) dx = ∫ sin 6(x) cos −4(x) sin(x) dx = − ∫ (1 − cos2(x))3 cos−4(x)(− sin(x)) dx = − ∫ (1 − u 2)3u −4 du = − ∫ (1 − 3u 2 + 3u 4 − u 6)u −4 du = ∫ (−u −4 + 3u −2 − 3 + u 2) du = 1 3 cos3(x) − 3 cos(x) − 3 cos(x) + 1 3 cos3(x). (3) We use a double-angle identity in (8.1.1) to obtain ∫ cos4(x) dx = ∫ ( 1 2 [1 + cos(2x)]) 2 dx = 1 4 ∫ (1 + 2 cos(2x) + cos2(2x)) dx = 1 4 ∫ (1 + 2 cos(2x) + 1 2 [1 + cos(4x)]) dx = 1 4 ∫ ( 3 2 + 2 cos(2x) + 1 2 cos(4x)) dx = 1 4 ( 3 2 x + 2 1 2 sin(2x) + 1 2 1 4 sin(4x)) = 3 8 x + 1 4 sin(2x) + 1 32 sin(4x). The following identities somewhat mirror the relation between sine and cosine functions that made the above method work and thus suggest a similar strategy for finding antiderivatives of products of secant and tangent functions: d dx (tan(x)) = sec 2(x) d dx (sec(x)) = sec(x) tan(x) tan2(x) + 1 = sec2(x) We developing a general strategy for finding the antiderivative ∫ tanm(x) sec n(x) dx, where m, n ∈ N, by considering the following two cases: 32 (1) If n is a even number, then n = 2k for some k ∈ N, so we substitute u(x) = tan(x), noting that u ′(x) = sec 2(x), to obtain ∫ tanm(x) sec 2k(x) dx = ∫ tan m(x) sec 2k−2(x) sec 2(x) dx = ∫ tanm(x)(tan 2(x) + 1)k−1 sec2(x) dx = ∫ u m(u 2 + 1)k−1 du, where we used sec2(x) = tan2(x) + 1 in the second line. This reduces matters to finding the antiderivative of a polynomial. (2) If m is an odd number, then m = 2k + 1 for some k ∈ N ∪ {0}, so we substitute u(x) = sec(x), noting that u ′(x) = sec(x) tan(x), to obtain ∫ tan 2k+1(x) sec n(x) dx = ∫ tan 2k(x) sec n−1(x) sec(x) tan(x) dx = ∫ (sec2(x) − 1)k secn−1(x) sec(x) tan(x) dx = ∫ (u 2 − 1) ku n−1 du, where we used tan 2(x) = sec2(x) − 1 in the second line. This reduces matters to finding the antiderivative of a polynomial. In the remaining case where n is an odd number and m is an even number, there does not seem to be a general strategy that always works. We often need to think creatively, as the following example shows. Example 8.1.2. Find the following antiderivatives: (1) ∫ tan(x) dx (2) ∫ sec(x) dx (3) ∫ sec3(x) dx. Solution. (1) We substitute u(x) = cos(x), noting that u′(x) = − sin(x), to obtain ∫ tan(x) dx = ∫ sin(x) cos(x) dx = ∫ −1 u du = − log | cos(x)| = log | sec(x)|. (2) We use (7.2.2) with f (x) := sec(x)+tan(x), and f ′(x) = sec(x) tan(x)+sec 2(x), to obtain ∫ sec(x) dx = ∫ sec(x) sec(x) + tan(x) sec(x) + tan(x) dx = ∫ sec2(x) + sec(x) tan(x) sec(x) + tan(x) dx = ∫ f ′(x) f (x) dx = log | sec(x) + tan(x)|. (3) We apply Integration by Parts with { u(x) = sec(x) v(x) = tan(x) and {u ′(x) = sec(x) tan(x) v′(x) = sec 2(x) 33 to obtain ∫ sec3(x) dx = sec(x) tan(x) − ∫ sec(x) tan 2(x) dx = sec(x) tan(x) − ∫ sec(x)(sec2(x) − 1) dx = sec(x) tan(x) − ∫ sec3(x) dx + ∫ sec(x) dx. We now use the antiderivate from part (2) and combine terms to obtain ∫ sec3(x) dx = 1 2 sec(x) tan(x) + 1 2 log | sec(x) + tan(x)|. This is an example of a reduction formula which we will consider in the next lecture. We conclude this lecture with an application to household electronics. Example 8.1.3. Mains electricity in the UK is supplied as alternating current rated as 230 V at 50 Hz. The means that the voltage alternates through 50 cycles per second with a root-mean-square voltage of 230 V. We can model the voltage using the function V (t) := Vpeak sin( 2π T t), where V (t) is the voltage (measured in volts) at time t (measured in seconds), whilst T is the length of one period in seconds and Vpeak is the peak voltage of the supply. The peak voltage is not the same as the root-mean-square voltage (Vrms = 230 V) but we can calculate it since the latter is defined by the following formula: Vrms := ( 1 T ∫ T 0 V (t)2 dt )1/2 . In particular, we a double-angle identity in (8.1.1), to obtain (Vrms)2 = 50 ∫ 1 50 0 V 2 peak sin 2(100πt) dt = 50V 2 peak ∫ 1 50 0 1 2 [1 − cos(200πt)] dt = 25V 2 peak [ t − 1 200π sin(200πt) ] 1 50 1 = 1 2 V 2 peak, hence Vpeak = √2 Vrms = √2 (230) V ≈ 325 V. 8.2. Reduction formulas and bijective substitution. An inspection of the method used to find the antiderivative in Example 8.1.2(3) shows that it can be extended to obtain a recursive formula for finding an antiderivative of any higher integer power of the secant function. Integration by Parts is often used to derive such reduction formulas, as the following example shows. Example 8.2.1. For each n ∈ N ∪ {0}, define Sn := ∫ sin n(x) dx. Prove that nSn(x) = − sin n−1(x) cos(x) + (n − 1)Sn−2(x) for all x ∈ R and all integers n ≥ 2, and use this formula to find ∫ sin 3(x) dx. Solution. If n ≥ 2 is an integer, then we apply Integration by Parts with {u(x) = sin n−1(x) v(x) = − cos(x) and {u ′(x) = (n − 1) sin n−2(x) cos(x) v′(x) = sin(x) 34 to obtain Sn(x) = − sin n−1(x) cos(x) − (n − 1) ∫ sin n−2(x) cos(x)(− cos(x)) dx = − sin n−1(x) cos(x) + (n − 1) ∫ sin n−2(x)(1 − sin 2(x)) dx = − sin n−1(x) cos(x) + (n − 1)(Sn−2(x) − Sn(x)) We now combine terms to obtain nSn(x) = (1 + n − 1)Sn(x) = − sin n−1(x) cos(x) + (n − 1)Sn−2(x). This allows us to calculate ∫ sin 3(x) dx = S3(x) = − 1 3 sin 2(x) cos(x) + 2 3 S1(x) = − 1 3 sin 2(x) cos(x) − 2 3 cos(x), as required. In the next part of this lecture, we introduce some trigonometric substitutions which can be used to transform certain irrational functions into rational functions for the purposes of integration. The following theorem, which is a reprise of the Substitution Formula from Theorem 7.1.3 in the case of a bijective substitution, will allow us to justify such substitutions. It is not important to be familiar with this proof for the assessment in this module. Theorem 8.2.2 (Bijective Substitution Formula). If f : [a, b] → R is a continuous function and g : [c, d] → [a, b] is bijective and differentiable with bounded integrable derivative g′, then ∫ b a f (x) dx = ∫ g−1(b) g−1(a) f (g(θ))g′(θ) dθ. Proof. The function g is a bijection, so we may apply the Substitution Formula from Theorem 7.1.3 with (a, b, u) replaced by (g−1(a), g−1(b), g) to obtain ∫ b a f (x) dx = ∫ g(g−1(b)) g(g−1(a)) f (x) dx = ∫ g−1(b) g−1(a) f (g(θ))g′(θ) dθ, as required. □ Table 8.2 lists three trigonometric substitutions which can be use to transform an integral with respect to x into an integral with respect to θ by writing x = g(θ). These are useful because they transform the irrational expressions in the left column into rational expressions by noting the identities in the right column. It is important to recognise that the Bijective Substitution Formula in Theorem 8.2.2 justifies the formal algebraic manipulation “dx = dx dθ dθ” associated with these changes of variable provided that g, or specifically the mapping θ ↦→ x = g(θ), is a bijection. Example 8.2.3. Find the area of the ellipse enclosed by the equation x2 a2 + y2 b2 = 1. Solution. The equation corresponds to an ellipse centre at the origin in the x-y plane with vertices at (±a, 0) and (0, ±b). Using symmetry, the total area enclosed by the ellipse is equal to 4 ∫ a 0 b a √a2 − x2 dx 35 Irrational Term Trigonometric Substitution x = g(θ) Rationalising Identity √a2 − x2 x = a sin(θ) 1 − sin 2(θ) = cos 2(θ) √x2 + a2 x = a tan(θ) tan2(θ) + 1 = sec2(θ) √x2 − a2 x = a sec(θ) sec2(θ) − 1 = tan 2(θ) Table 8.2. Trigonometric Substitutions for Rationalising Integrands We follow Table 8.2, making the substitution x = a sin(θ), and apply the Bijective Substitution Formula in Theorem 8.2.2 with the bijection g : [0, π 2 ] → [0, a] given by g(θ) := a sin(θ), noting that g′(θ) = a cos(θ), g(0) = 0 and g( π 2 ) = a, to obtain 4 ∫ a 0 b a √a2 − x2 dx = 4b a ∫ π 2 0 √a2(1 − sin 2(θ)) a cos(θ) dθ = 4ab ∫ π 2 0 cos2(θ) dθ = πab, where we used a double-angle identity from (8.1.1) to obtain the final equality. The Bijective Substitution Formula can also be used to find antiderivatives. It is good practice, and required in the assessment for this module, to remove any inverse trigonometric expressions from such antiderivatives. This is because there is no widespread agreement on the domains for inverse trigonometric functions. The following example shows how to achieve this by using some trigonometry. Example 8.2.4. Find the following antiderivatives (1) ∫ 1 x2√x2 + 4 dx (2) ∫ x √x2 + x + 1 dx. Solution. (1) We follow Table 8.2, making the substitution x = 2 tan(θ) and noting that g′(θ) = 2 sec2(θ), to obtain ∫ 1 x2√x2 + 4 dx = ∫ 2 sec 2(θ) 4 tan 2(θ) √4(tan 2(θ) + 1) dθ = ∫ sec(θ) 4 tan 2(θ) dθ = ∫ cos(θ) 4 sin 2(θ) dθ = ∫ 1 4u2 du = −1 4 sin(θ) , where we use tan(θ) = sin(θ)/ cos(θ) in the third line, and made the substitution u(θ) = sin(θ) with u ′(θ) = cos(θ) in the fourth line. To conclude, we avoid writing sin(θ) = sin(tan −1( x 2 )) by using trigonometry to reveal the relationship between sin(θ) and tan(θ). In particular, we have tan(θ) = x 2 = opp adj , where we regard θ as the acute angle in a right-angled triangle with opposite side-length x and adjacent side-length 2, hence sin(θ) = opp hyp = x√x2+4 , where we used Pythagoras’ Theorem to 36 calculate the hypotenuse side-length √x2 + 4. We thus have ∫ 1 x2√x2 + 4 dx = −√x2 + 4 4x . (2) We complete the square and make the substitution y = x + 1 2 , to obtain ∫ x √x2 + x + 1 dx = ∫ x √(x + 1 2 )2 + 3 4 dx = ∫ y − 1 2√y2 + 3 4 dy = ∫ y √y2 + 3 4 dy + ∫ − 1 2√y2 + 3 4 dy =: I + J The term I can be found using the substitution u = y2 + 3 4 as I = ∫ 1 2 u − 1 2 du = u 1 2 = (y2 + 3 4 ) 1 2 = √x2 + x + 1 The term II can be found using the substitution y = √3 2 tan(θ) as J = − 1 2 ∫ √3 2 sec2(θ) √3 2 sec(θ) dθ = − 1 2 ∫ sec(θ) dθ = − 1 2 log | sec(θ) + tan(θ)|, where we used (8.1.2) to obtain the final equality. Using trigonometry, we have tan(θ) = y√3/2 = opp adj , hence sec(θ) = hyp adj = √y2+3/4 √3/2 . We thus have J = − 1 2 log | 2√3 (√ x2 + x + 1 + x + 1 2 )| = − 1 2 log | √x2 + x + 1 + x + 1 2 | − 1 2 log( 2√3 ). We can choose to ignore the final additive constant above, since we only need to find an antiderivative, in which case ∫ x √x2 + x + 1 dx = I + J = √x2 + x + 1 − 1 2 log |√ x2 + x + 1 + x + 1 2 |. Week 9: Improper Integrals This week we return to the theory of integration to define improper integrals for bounded functions on unbounded intervals and for unbounded functions on bounded intervals. 9.1. Unbounded intervals. The Riemann–Darboux theory of integration is con- cerned with bounded functions on bounded intervals. It is possible, however, for the area beneath a bounded function on an unbounded domain to be finite. For example, if A(t) denotes the area beneath the bounded function f (x) := 1 x2 on the interval [1, t], where t > 1, then A(t) = ∫ t 1 1 x2 dx = 1 − 1 t , and we notice that limt→∞ A(t) = 1. In this way, we interpret the area beneath f on the unbounded interval [1, ∞) as a limit of Riemann–Darboux integrals on bounded intervals [1, t]. The limit is finite in this particular example because 1 x2 decays sufficiently fast as x approaches infinity. This motivates the following definition of improper integrals 37 for bounded functions on semi-infinite domains of the form [a, ∞) and (−∞, b], as well as on R. Definition 9.1.1. The following improper integrals are called convergent when they exist in R (otherwise they are called divergent): (1) If f : [a, ∞) → R is bounded and integrable on all bounded intervals [a, t], where t > a, then the improper integral ∫ ∞ a f is defined by ∫ ∞ a f := lim t→∞ ( ∫ t a f ) whenever the limit exists in R := R ∪ {−∞, +∞}. (2) If f : (−∞, b] → R is bounded and integrable on all bounded intervals [t, b], where t < b, then the improper integral ∫ b −∞ f is defined by ∫ b −∞ f := lim t→−∞ ( ∫ b t f ) whenever the limit exists in R := R ∪ {−∞, +∞}. (3) If f : R → R is bounded and there exists c ∈ R such that both improper integrals ∫ c −∞ f and ∫ ∞ c f are convergent, then the improper integral ∫ ∞ −∞ f is the real number defined by ∫ ∞ −∞ f := ∫ c −∞ f + ∫ ∞ c f . The following example shows how to calculate these types of improper integral. The main point is to work in two stages, first computing the integral on a bounded domain, and then taking an appropriate limit. Example 9.1.2. Calculate the following improper integrals or prove that they are divergent: (1) ∫ ∞ 1 1 x dx (2) ∫ 0 −∞ xex dx (3) ∫ ∞ −∞ e−|x| dx Solution. (1) We calculate ∫ ∞ 1 1 x dx = lim t→∞ ( ∫ t 1 1 x dx) = lim t→∞ [ log |x|]t 1 = lim t→∞ log(t) = ∞, hence this improper integral is divergent. (2) We apply Integration by Parts with { u(x) = x v(x) = e(x) and { u ′(x) = 1 v′(x) = e(x) 38 to calculate lim t→−∞ ( ∫ 0 t xex) = lim t→−∞ ( [xe x]0 t − ∫ 0 t e x) = lim t→−∞ ( 0 − tet − [ ex]0 t ) = lim t→−∞ ( −t e−t − 1 + et) = ( lim t→−∞ −1 −e−t ) − 1 = ( lim t→−∞ et) − 1 = −1, where we used the Algebra of Limits and L’Hˆopital’s Rule on the indeterminate form −t/e−t to obtain the fourth equality. It is important to note here how the Integration by Parts was performed on the interval [t, 0] before the limit was taken. (3) We calculate ∫ ∞ 0 e−|x| dx = lim t→∞ ( ∫ t 0 e−x dx) = lim t→∞(−e−t + 1) = 1, which is convergent, and ∫ 0 ∞ e −|x| dx = lim t→−∞ ( ∫ 0 t ex dx) = lim t→−∞(1 − et) = 1, which is also convergent. This proves that ∫ ∞ −∞ e−|x| dx is convergent with ∫ ∞ −∞ e−|x| dx = ∫ ∞ 0 e−|x| dx + ∫ 0 ∞ e−|x| dx = 2. It is important to note here how the improper integral on R needed to be split into two improper integrals on semi-infinite domains. In particular, an improper integral on R should not be calculated as single combined limit, such as limt→∞ ∫ t −t f , as this is not in accordance with (3) in Definition 9.1.1 9.2. Unbounded functions. If f : [a, b] → R is an unbounded function, then the sums L(f, P ) = ∑n i=1 mi(xi − xi−1) and U (f, P ) = ∑n i=1 Mi(xi − xi−1) may not be well-defined, because mi = −∞ on any subinterval [xi−1, xi] ⊆ [a, b] where the function is not bounded below, and Mi = ∞ on any subinterval where the function is not bounded above. If the function is bounded except for some asymp- totic behaviour at certain points, however, then the Riemann–Darboux integral can be defined on subintervals that exclude those singular points. For example, the function f (x) := 1√x is not bounded on (0, 1], but it is bounded on (δ, 1] with ∫ 1 δ 1√x dx = 2 − 2 √δ for all δ ∈ (0, 1], and limδ→0 ∫ 1 δ 1√x dx = 2. The improper integrals below are defined by taking such limits of Riemann–Darboux integrals. Definition 9.2.1. The following improper integrals are called convergent when they exist in R (otherwise they are called divergent): (1) If f : (a, b] → R is bounded and integrable on all intervals [a + δ, b] ⊂ (a, b], where δ > 0, then the improper integral ∫ b a f is defined by ∫ b a f := lim δ→0+ ( ∫ b a+δ f ) whenever the limits exists in R := R ∪ {−∞, +∞}. 39 (2) If f : [a, b) → R is bounded and integrable on all intervals [a, b − δ] ⊂ [a, b), where δ > 0, then the improper integral ∫ b a f is defined by ∫ b a f := lim δ→0+ ( ∫ b−δ a f ) whenever the limit exists in R := R ∪ {−∞, +∞}. (3) If f : [a, c)∪(c, b] → R is bounded and integrable on all closed subintervals of [a, c)∪(c, b], and both improper integrals ∫ c a f and ∫ b c f are convergent, then the improper integral ∫ b a f is the real number defined by ∫ b a f := ∫ c a f +∫ b c f . The following example shows how to calculate these types of improper integral. Example 9.2.2. Calculate the following improper integrals or prove that they are divergent: (1) ∫ 5 2 1 √x − 2 dx (2) ∫ π 2 0 sec(x) dx (3) ∫ 3 0 1 x − 1 dx Solution. (1) The integrand has a singularity at x = 2, so we calculate ∫ 5 2 1 √x − 2 dx = lim δ→0+ ( ∫ 5 2+δ(x − 2)−1/2 dx) = lim δ→0+ [ 2(x − 2) 1/2]5 2+δ = lim δ→0+(2√3 − 2 √δ) = 2 √3. (2) The integrand has a singularity at x = π 2 , so we calculate ∫ π 2 0 sec(x) dx = lim δ→0+ ( ∫ π 2 −δ 0 sec(x) dx) = lim δ→0+ [ log | sec(x) + tan(x)|] π 2 −δ 0 = lim δ→0+ log | sec( π 2 − δ) + tan( π 2 − δ)| = ∞, hence this improper integral is divergent. (3) The integrand has a singularity at x = 1, so we calculate ∫ 1 0 1 x − 1 dx = lim δ→0+ ( ∫ 1−δ 0 1 x − 1 dx) = lim δ→0+ [ log |x − 1| ]1−δ 0 = lim δ→0+ log(δ) = −∞, hence this improper integral is divergent. It is important to note here how treating this as an (ordinary) integral, that is by writing ∫ 3 0 1 x−1 dx = [log |x − 1|]3 0 = log(2), gives the wrong answer. The following result provides a convergence test for improper integrals. Theorem 9.2.3 (Integral Comparison Test). If f : [a, ∞) → R and g : [a, ∞) → R are bounded functions such that 0 ≤ f (x) ≤ g(x) for all x ∈ [a, ∞), then the following implications hold: 40 (1) If ∫ ∞ a g is convergent, then ∫ ∞ a f is convergent. (2) If ∫ ∞ a f is divergent, then ∫ ∞ a g is divergent. Proof. The improper integrals ∫ ∞ a f and ∫ ∞ a g will both exist in [0, +∞]. This is because f (x) ≥ 0 and g(x) ≥ 0 for all x ∈ [a, ∞), which implies that the values of both ∫ t a f and ∫ t a g increase as t increases, hence limt→∞( ∫ t a f ) and limt→−∞( ∫ b t f ) will be either finite and non-negative or equal to +∞. The assumption that f (x) ≤ g(x) for all x ∈ [a, ∞) implies that ∫ t a f ≤ ∫ t a g for all t ∈ (a, ∞). It is a good exercise to write out a proof of this fact by considering ∫ t a (g − f ) and combining the estimates for the integral in Theorem 4.2.8 with the linearity properties of the integral in Theorem 4.2.2. The above facts justify the following implications: (1) If ∫ ∞ a g is convergent, then 0 ≤ ∫ ∞ a f = lim t→∞ ( ∫ t a f ) ≤ lim t→∞ ( ∫ t a g) = ∫ ∞ a g < ∞, hence ∫ ∞ a f is convergent. (2) If ∫ ∞ a f is divergent, then ∞ = ∫ ∞ a f = lim t→∞ ( ∫ t a f ) ≤ lim t→∞ ( ∫ t a g) = ∫ ∞ a g, hence ∫ ∞ a g is divergent. □ There are similar versions of the Integral Comparison Test for improper integrals on intervals of the form (−∞, b], and also for improper integrals of unbounded functions on domains (a, b] and [a, b) as in Deifinition 9.2.1. We leave the details to the interested reader. We conclude with an example of how to apply this result. Example 9.2.4. Determine whether ∫ ∞ 1 1 + e−x x dx is convergent or divergent. Solution. This improper integral is divergent by the Integral Comparison Test, since 1 + e−x x ≥ 1 x ≥ 0 for all x ∈ [1, ∞), and ∫ ∞ 1 1 x dx is divergent by Example 9.1.2. 41 Week 10: First-Order ODEs and Applications This week we develop strategies to solve first-order ordinary differential equa- tions. Direct integration solves the equation y′ = f (x), the method of separation of variables applies when y′ = f (x)g(y), and an integration factor is used to solve linear equations y′ + g(x)y = f (x). We will also see how such equations can be used to model mixing problems in engineering applications. 10.1. Separable equations. We will typically be solving differential equations to find a function y in terms of an independent variable x (so x ↦→ y(x)). We use y′ = dy dx and y′′ = d 2y dx2 to denote the first and second derivatives of y. We begin by making precise the meaning of a solution to a first-order differential equation and associated initial value problems. Definition 10.1.1. Suppose that f : R×R → R and that I ⊆ R is an open interval. A solution of the (first-order) differential equation y′ = f (x, y) on I is a differentiable function y : I → R such that y′(x) = f (x, y(x)) for all x ∈ I. We can extend this definition to the case when f : Ω1 × Ω2 → R, where Ω1 ⊆ R and Ω2 ⊆ R, by requiring that I ⊆ Ω1 and that y : I → Ω2. We typically need some solution data to specify a unique solution of a first-order equation, which leads us to consider initial value problems. Definition 10.1.2. Suppose that f : R × R → R. Let x0, y0 ∈ R and x1 ∈ (x0, ∞]. A solution of the initial value problem y′ = f (x, y), y(x0) = y0 is a continuous function y : [x0, x1) → R that is differentiable on (x0, x1) such that y′(x) = f (x, y(x)) for all x ∈ (x0, x1), and such that y(x0) = y0. In other words, a solution of the initial value problem is a solution of the ordinary differential equation on (x0, x1) that is also continuous on [x0, x1) with y(x0) = y0. We can also extend this definition to the case when f : Ω1 × Ω2 → R, where Ω1 ⊆ R and Ω2 ⊆ R, by requiring that (x0, x1) ⊆ Ω1 and that y : (x0, x1) → Ω2. The simplest first-order equation y′ = f (x), where f is function of x only, can be solved by direct integration when f is continuous. Moreover, we have the explicit expression below for the solution of initial value problems on [x0, ∞). Theorem 10.1.3. If x0, y0 ∈ R and f : [x0, ∞) → R is continuous, then the initial value problem y′ = f (x), y(x0) = y0 has the solution y : [x0, ∞) → R given by y(x) := ∫ x x0 f + y0 for all x ∈ [x0, ∞). Proof. The function f is continuous, so the first Fundamental Theorem of Calculus implies that y(x) := ∫ x x0 f + y0 defines a differentiable function with derivative y′(x) = f (x) for all x ∈ [x0, ∞), hence y : [x0, ∞) → R is a continuous function, and initial value y(x0) = ∫ x0 x0 f + y0 = y0, as required. □ Here is an example of how to apply this theorem. In general, however, there is no need to quote this theorem, and you may integrate directly to obtain a solution. Example 10.1.4. Find a solution y : [0, ∞) → R of the initial value problem y′ = x √x2 + x + 1 , y(0) = 3. 42 Solution. We combine Theorem 10.1.3 with the antiderivative from Example 8.2.4(2) and the second Fundamental Theorem of Calculus to obtain the solution y(x) : = ∫ x 0 t √t2 + t + 1 dt + 3 = [√t2 + t + 1 − 1 2 log ∣ ∣ ∣√ t2 + t + 1 + t + 1 2 ∣ ∣ ∣]x 0 + 3 = √ x2 + x + 1 − 1 2 log (√x2 + x + 1 + x + 1 2 ) + 2 + 1 2 log 3 2 . for all x ∈ [0, ∞). It is not usually the case, however, that solutions to a general class of differential equations can be expressed as explicitly as in Theorem 10.1.3. We are instead led to develop formal strategies for generating solutions. These methods are formal3 in the sense that we temporarily abandon rigour by assuming that all relevant mathematical objects (numbers, functions etc.) have properties that are sufficient to justify our manipulations. In this way, the methods only generate candidates for solutions, and we must then analyse these rigorously to verify whether they actually satisfy all of the properties required by the relevant definition of a solution. We will encounter four such formal methods in the remainder of this module. The first, which we shall develop now, is the method of separation of variables. This applies to separable first-order ordinary differential equations y′ = f (x)g(y) where f : R → R and g : R → R (or more generally f : Ω1 → R and g : Ω2 → R). These are called separable because f and g separate the x and y dependency. The method is as follows: First, for each y0 ∈ R such that g(y0) = 0, we obtain the constant solution y(x) := y0 for all x0 ∈ R. Next, assuming that y is a solution such that g(y(x)) ̸= 0, a formal application of the Substitution Formula gives 1 g(y) dy dx = f (x) =⇒ ∫ 1 g(y) dy = ∫ f (x) dx. A candidate solution can then be obtained by finding antiderivatives for both sides of this equation and trying to solve to obtain an explicit expression for y(x). These antiderivatives can involve additive constants and it is important to account for these here (as well as in all of the formal methods we develop this week). The following convention is commonly used to reduce the burden of notation associated with constants in calculations and estimates. Notation 10.1.5. We use the convention whereby C denotes an arbitrary real number, the value of which can change from one line to the next in calculations and estimates. The following example shows how to apply the method of separation of variables. Example 10.1.6. Find two solutions of the differential equation y′ = 15 − 3y on R. Solution. Observe that 15 − 3y = 0 when y = 5, hence y1 : R → R given by y1(x) := 5 for all x ∈ R is a solution on R, since it is differentiable on R and y′ 1(x) = 0 = 15 − 3y1(x) for all x ∈ R. 3In mathematical analysis, a formal computation refers to a manipulation of mathematical symbols based on experience and intuition but without rigorous justification. For example, we use the formal algebraic manipulation “du = du dx dx” when applying the Substitution Formula. 43 Next, assume that y is a solution on R such that 15 − 3y(x) ̸= 0 for all x ∈ R, so a formal application of the Substitution Formula gives 1 15 − 3y dy dx = 1 =⇒ ∫ 1 15 − 3y dy = ∫ 1 dx =⇒ − 1 3 log |15 − 3y| = x + C =⇒ |15 − 3y| = Ce −3x for some C ∈ R, where we have used the convention regarding the value of C from Notation 10.1.5. The assumption that y is a solution on R implies that 15 − 3y is differentiable and hence continuous. The continuity of 15 − 3y combined with the assumption that 15 − 3y(x) ̸= 0 for all x ∈ R implies that either 15 − 3y(x) > 0 for all x ∈ R or 15 − 3y(x) < 0 for all x ∈ R. In either case, since the value of C can be different on each line, we have 1 15 − 3y dy dx = 1 =⇒ |15 − 3y| = Ce −3x =⇒ 15 − 3y = Ce −3x =⇒ y = 5 − Ce −3x for some C ∈ R. The above manipulations suggest that for any C ∈ R, the function y2 : R → R given by y2(x) := 5 − Ce −3x is a solution on R. This is indeed the case, since y2 is differentiable on R (as a linear combination of a constant function and an exponential function) and y′ 2(x) = 15 − 3y2(x) for all x ∈ R by inspection. This gives an infinite number of solutions and we recover y2 = y1 when C = 0. The following example shows how separable first-order differential equations can be used to model mixing problems in engineering applications Example 10.1.7 (Mixing Problem I). A 5,000 L industrial mixing tank is filled with a salt solution containing 20 kg of dissolved salt. The tank is drained at a rate of 25 L min−1 whilst a mixture containing 0.03 kg L −1 of salt is pumped into the tank at the same rate. Assume that the mixture is instantly and uniformly mixed: (1) Let y(t) denote the kilograms of salt in the tank after t minutes of mixing. Formulate an initial value problem to model the flow using the flow equation y′(t) = rate in − rate out. (2) Find a solution of the initial value problem to determine the mass of salt in the tank after 30 minutes of mixing. (3) Determine the long-term behaviour of the salt concentration in the tank. Solution. (1) The flow equation y′(t) = rate in − rate out (in kg min−1) becomes y′(t) = (0.03 kg L ) ( 25 L min ) − ( y(t) 5000 kg L ) ( 25 L min ) . The flow is thus modelled by solutions y : [0, ∞) → R to the initial value problem y′(t) = 150 − y 200 , y(0) = 20. (2) This is a separable first-order differential equation. Observe that 150−y 200 = 0 when y = 150, but we ignore the constant solution y(t) = 150 because it does not satisfy the initial condition. Next, assume that y is a solution on (0, ∞) such that 44 150−y(t) 200 ̸= 0 for all t ∈ (0, ∞), so proceeding formally we obtain 1 150 − y(t) dy dt = 1 200 =⇒ ∫ 1 150 − y dy = ∫ 1 200 dt =⇒ − log |150 − y| = t 200 + C =⇒ |150 − y| = Ce − t 200 =⇒ 150 − y = Ce − t 200 =⇒ y = 150 + Ce − t 200 for some C ∈ R, since either 150 − y(t) > 0 for all t ∈ (0, ∞), or 150 − y(t) < 0 for all t ∈ (0, ∞), because y is continuous and 150 − y(t) ̸= 0 for all t ∈ (0, ∞). The initial condition y(0) = 20 requires that 150 + C = 20, so C = −130 The function y : [0, ∞) → R given by y(t) := 150 − 130e− t 200 is differentiable on (0, ∞) and continuous on [0, ∞), since it is the composition of an exponential function and linear functions. It also satisfies the differential equation and ini- tial condition by construction, hence it is a solution of the initial value problem. Therefore, the mass of salt in the tank after 30 minutes of mixing is modelled by y(30) = 150 − 130e − 3 20 ≈ 38.1 kg. (3) The long behaviour of the salt concentration in the tank is modelled by lim t→∞ y(t) 5000 = 1 5000 lim t→∞ (150 − 130e− t 200 ) = 150 5000 = 0.03 kg L −1. In other words, the salt concentration in the tank approaches the concentration of the input flow in the long-term. 10.2. Linear equations. We now introduce an integration factor to generate solu- tions of linear first-order differential equations y′ + g(x)y = f (x). These equations are called linear because the dependent variable y and its derivative y′ appear as linear factors when x is fixed. The following example motivates the definition of the integration factor. Example 10.2.1. Suppose that g : [a, b] → R is continuous and define I : [a, b] → R by I(x) := e∫ x a g for all x ∈ [a, b]. Prove that I is differentiable and that I ′ = Ig. Proof. The function g is continuous so the First Fundamental Theorem of Calculus implies that F : [a, b] → R given by F (x) := ∫ x a g is differentiable with derivative F ′(x) = g(x) for all x ∈ [a, b]. Now observe that I(x) = eF (x) = (exp ◦F )(x), where exp(x) := ex is a differentiable function on R, so the Chain Rule implies that I is differentiable with derivative I ′(x) = (exp ◦F )′(x) = exp ′(F (x))F ′(x) = eF (x)g(x) = I(x)g(x) for all x ∈ [a, b], hence I ′ = Ig. □ More generally, if Ω ⊆ R and g : Ω → R has an antiderivative h : Ω → R, then the above proof also shows that I(x) := eh(x) is differentiable with I ′ = Ig. The first step to generate solutions of the equation y′ + g(x)y = f (x) is to introduce an integration factor I := e∫ g, where ∫ g denotes (as usual) an antiderivative of g. It is not necessary to introduce an additive constant C here, as we noted above that any antiderivative of g will ensure that I is differentiable with I ′ = Ig. Next, proceeding formally, we multiply 45 the differential equation by I and combine the identity I ′ = Ig with the Product Rule (Iy)′ = Iy′ + I ′y, to obtain Iy′ + (Ig)y = If =⇒ Iy′ + I ′y = If =⇒ (Iy) ′ = If =⇒ Iy = ∫ If =⇒ y = 1 I ∫ If, where the final division by I is justified because the exponential function is never equal to 0. In this way, we obtain the candidate solutions y(x) := 1 I(x) (∫ I(x)f (x) dx + C) , where C ∈ R is included here to emphasise that it is particularly important to account for such additive constants when substituting an antiderivative of If . The following examples show how to use an integration factor to obtain a solution to an initial value problem. Example 10.2.2. Find a solution y : [0, ∞) → R of the initial value problem y′ − 2xy = 2x, y(0) = 1. Solution. We introduce the integration factor I(x) := e∫ (−2x) dx = e−x2. Next, we multiply the differential equation by I and proceed formally to obtain I(x)[y′(x) − 2xy(x)] = I(x)2x =⇒ (Iy) ′(x) = e−x22x =⇒ (Iy)(x) = ∫ e−x22x dx =⇒ e−x2y(x) = −e−x2 + C =⇒ y(x) = −1 + Ce x2, where C ∈ R. The initial condition y(0) = 1 requires that −1 + C = 1, so C = 2. The function y : [0, ∞) → R given by y(x) := −1 + 2e x2 for all x ∈ [0, ∞) is differentiable on (0, ∞) and continuous on [0, ∞), since it is the composition of an exponential function and a polynomial. It also satisfies the differential equation and initial condition by construction, hence it is a solution of the initial value problem. Example 10.2.3. Find a solution y : [0, ∞) → R of the initial value problem y′ + 1 x + 1 y = x, y(0) = y0, where y0 ∈ R. Solution. We introduce the integration factor I(x) := e∫ 1 x+1 dx = elog |x+1| = |x + 1| = (x + 1), where the final equality is justified because we are only concerned with x ∈ [0, ∞). Next, we multiply the differential equation by I and proceed formally to obtain (x + 1)y′(x) + y(x) = (x + 1)x =⇒ (Iy) ′(x) = x2 + x =⇒ (Iy)(x) = ∫ (x2 + x) dx =⇒ (x + 1)y(x) = 1 3 x3 + 1 2 x2 + C =⇒ y(x) = 1 x+1 ( 1 3 x3 + 1 2 x2 + C) , 46 where C ∈ R. The initial condition y(0) = y0 requires that C = y0. The function y : [0, ∞) → R given by y(x) := 1 x+1 ( 1 3 x3 + 1 2 x2 + y0) for all x ∈ [0, ∞) is differentiable on (0, ∞) and continuous on [0, ∞), since it is a rational function with nonzero denominator. It also satisfies the differential equation and initial condition by construction, hence it is a solution of the initial value problem. The following example shows how linear first-order differential equations can be used to model mixing problems in engineering applications. Example 10.2.4 (Mixing Problem II). Consider again the mixing problem from Example 10.1.7. Suppose that the input pump is malfunctioning and only allows the mixture to flow into the tank at a reduced rate of 20 L min−1. Solve an initial value problem to determine the mass of salt in the tank after t minutes of mixing. Solution. The flow equation y′(t) = rate in − rate out (in kg min−1) now becomes y′(t) = ( 0.03 kg L ) ( 20 L min ) − ( y(t) 5000 − 5t kg L ) ( 25 L min ) . In particular, since the output flow is now 5 L min −1 greater than the input flow, the volume in the tank decreases from the tank’s 5, 000 L capacity to 5, 000 − 5t at time t ∈ (0, 1000), until the tank is completely drained at t = 1000. The flow is thus modelled by solutions y : [0, 1000) → R to the initial value problem y′(t) = 3 5 − 5 1000 − t y, y(0) = 20. This is a linear first-order differential equation, which in standard form becomes y′(t) + 5 1000 − t y = 3 5 , y(0) = 20. We introduce the integration factor I(t) := e∫ 5 1000−t dx = e−5 log |1000−t| = |1000 − t|−5 = (1000 − t)−5, where the final equality is justified because we are only concerned with t ∈ [0, 1000). Next, we multiply the differential equation by I and proceed formally to obtain I(t) [y′(t) + 5 1000−t y(t) ] = 3 5 (1000 − t)−5 =⇒ (Iy) ′(t) = 3 5 (1000 − t)−5 =⇒ (Iy)(t) = 3 5 ∫ (1000 − t)−5 dt =⇒ (1000 − t)−5y(t) = −3 −20 (1000 − t)−4 + C =⇒ y(t) = 3 20 (1000 − t) + C(1000 − t)5, where C ∈ R. The initial condition y(0) = 20 requires that C = −1.3 × 10−13. The function y : [0, ∞) → R given by y(t) := 3 20 (1000 − t) − 1.3 × 10−13(1000 − t) 5 for all t ∈ [0, 1000) is differentiable on (0, 1000) and continuous on [0, 1000), since it is a rational function with nonzero denominator. It also satisfies the differential equation and initial condition by construction, hence it is a solution of the initial value problem. More generally, since the tank will be empty when t ≥ 1000, the function ˜y : [0, ∞) → R given by ˜y(t) := { 3 20 (1000 − t) − 1.3 × 10 −13(1000 − t) 5, t ∈ [0, 1000) 0, t ≥ 1000 models the mass of salt in the tank after t minutes of mixing for all t ≥ 0. 47 Week 11: Second-Order ODEs and Applications This week we develop strategies to solve second-order ordinary differential equa- tions. For equations with constant coefficients ay′′ + by′ + cy = f (x), we will obtain general solutions in the homogeneous case (when f (x) = 0) using the method of characteristic equations, and in the inhomogeneous case (when f (x) ̸= 0) using the method of undetermined coefficients. We will also see how such equations model the motion of vibrating springs in physics. 11.1. Homogeneous equations. We now consider second-order equations with constant coefficients ay′′ + by′ + cy = f (x). These equations are also linear because the dependent variable y and its derivatives y′ and y′′ appear as linear factors when x is fixed. They are also called homogeneous when f (x) = 0 because each term is then a homogeneous function of the dependent variable y. This means that the equation has the multiplicative scaling property ay′′ + by′ + cy = 0 =⇒ (αy)′′ + b(αy)′ + c(αy) = α(ay′′ + by′ + cy) = 0 for all α ∈ R. We begin by making precise the meaning of a solution to these equations and associated initial value problems and boundary value problems. Definition 11.1.1. Suppose that f : R → R, that a, b, c ∈ R with a ̸= 0, and that I ⊆ R is an open interval. A solution of the (second-order) differential equation ay′′ + by′ + cy = f (x) on I is a twice differentiable function y : I → R such that ay′′(x) + by′(x) + cy(x) = f (x) for all x ∈ I. We can extend this definition to the case when f : Ω → R, where Ω ⊆ R, by requiring that I ⊆ Ω. We typically need two pieces of solution data to specify a unique solution of a second-order differential equation. This leads use to consider both initial value problems and boundary value problems. Definition 11.1.2. Suppose that f : R → R and that a, b, c ∈ R with a ̸= 0. Let x0, y0, y1 ∈ R and x1 ∈ (x0, ∞]. A solution of the initial value problem ay′′(x) + by′(x) + cy(x) = f (x), y(x0) = y0, y′(x0) = y1 is a differentiable function y : [x0, x1) → R that is twice differentiable on (x0, x1) such that ay′′(x)+by′(x)+cy(x) = f (x) for all x ∈ (x0, x1), and such that y(x0) = y0 and y′(x0) = y1. Let x0, y0, y1 ∈ R and x1 ∈ (x0, ∞). A solution of the boundary value problem ay′′(x) + by′(x) + cy(x) = f (x), y(x0) = y0, y(x1) = y1 is a continuous function y : [x0, x1] → R that is twice differentiable on (x0, x1) such that ay′′(x) + by′(x) + cy(x) = f (x) for all x ∈ (x0, x1), and such that y(x0) = y0 and y(x1) = y1. We will now see how to obtain general solutions of homogeneous equations by reducing matters to the algebra of certain characteristic equations. We begin by proving the linearity property in (1) below for solutions of homogeneous equations. This is the first step toward obtaining the full characterisation of solutions in (2), although this requires techniques beyond the scope of this module and is more suited to a second-year differential equations module. In particular, part (2) below requires the notion that two functions f and g are linearly independent when they are not constant multiples of each other (so f ̸= Cg) but you are not expected to be familiar with this property for the assessment in this module. Proposition 11.1.3. Suppose that a, b, c ∈ R with a ̸= 0. The following properties concern solutions of the homogeneous differential equation ay′′ + by′ + cy = 0 on R: 48 (1) If y1, y2 are solutions and C1, C2 ∈ R, then y := C1y1 + C2y2 is a solution. (2) If y is a solution and y1, y2 are linearly independent solutions, then there exist C1, C2 ∈ R such that y = C1y1 + C2y2. Proof. (1) We have ay′′ 1 (x) + by′ 1(x) + cy1(x) = 0 and ay′′ 2 (x) + by′ 2(x) + cy2(x) = 0 for all x ∈ R. We multiply these equations by C1 and C2 respectively to obtain C1(ay′′ 1 (x) + by′ 1(x) + cy1(x)) + C2(ay′′ 2 (x) + by′ 2(x) + cy2(x)) = 0 for all x ∈ R. The linearity of the derivatives implies that a(C1y1 + C2y2)′′(x) + b(C1y1 + C2y2) ′(x) + c(C1y1 + C2y2)(x) = 0 for all x ∈ R, so y := C1y1 + C2y2 is a solution on R. (2) There is no scope to provide this proof here, except to mention that it involves the fact that if the Wronksian W (y1, y2) := y1y′ 2 − y2y′ 1 does not vanish identically on R, then the solutions y1 and y2 must be linearly independent. □ The main point to recognise from (2) in Proposition 11.1.3 is that every solution of the equation ay′′+by′+cy = 0 can be characterised as a linear combination of two linearly independent solutions y1 and y2. Table 11.1 below shows how to obtain two such solutions from the roots λ ∈ C of the characteristic equation aλ2 + bλ + c = 0. The linear combination y := C1y1 + C2y2, where C1, C2 ∈ R, is then called the general solution of the differential equation. Roots of the Characteristic Equation General Solution y := C1y1 + C2y2 on R, aλ2 + bλ + c = 0, where λ ∈ {λ1, λ2} where C1, C2 ∈ R λ1 ̸= λ2 (λ1, λ2 ∈ R) y(x) := C1eλ1x + C2eλ2x λ1 = λ2 (λ1, λ2 ∈ R) y(x) := C1eλ1x + C2xeλ1x λ1 = α + iβ, λ2 = α − iβ (α, β ∈ R) y(x) := eαx(C1 cos(βx) + C2 sin(βx)) Table 11.3. Roots of the characteristic equation aλ 2 + bλ + c = 0 corresponding to general solutions of ay′′ + by′ + cy = 0. We now use Proposition 11.1.3 to prove these results. Theorem 11.1.4. Suppose that a, b, c ∈ R with a ̸= 0. The general solution of the equation ay′′ + by′ + cy = 0 on R is determined by the roots of the characteristic equation aλ2 + bλ + c = 0 as in Table 11.1. Proof. First, observe that if λ ∈ C satisfies (aλ2 + bλ + c) and y(x) := eλx for all x ∈ R, then y is twice differentiable with ay′′(x) + by′(x) + cy(x) = aλ 2eλx + bλeλx + ce λx = (aλ2 + bλ + c)eλx = 0 for all x ∈ R, so y is a solution of the differential equation on R. We use this result in the following three cases: Case 1: If the characteristic equation has two distinct real roots λ1 ̸= λ2, then y1(x) := e λ1x and y2(x) := eλ2x are two linearly independent solutions, so the result follows from Proposition 11.1.3. Case 2: If the characteristic equation has only one real root λ = − b 2a , then y1(x) := eλx is a solution. In this case, observe that y2(x) := xe λx is a solution, 49 since it is twice differentiable with ay′′ 2 (x) + by′ 2(x) + cy2(x) = a(2λeλx + λ2xeλx) + b(eλx + λxeλx) + cxe λx = (2aλ + b)eλx + (aλ2 + bλ + c)xeλx = (2a −b 2a + b)eλx = 0 for all x ∈ R. The solutions y1 and y2 are also linearly independent, so the result follows from Proposition 11.1.3. Case 3: If the characteristic equation has a pair of complex conjugate roots λ1 = α + iβ and λ2 = α − iβ, then y1(x) := 1 2 (e(α+iβ)x + e(α−iβ)x) = eαx cos(βx) and y2(x) := 1 2i (e (α+iβ)x − e(α−iβ)x) = eαx sin(βx) are two linearly independent solutions, so the result follows from Proposition 11.1.3. □ The next example shows how to use characteristic equations to obtain general solutions, as well as solve initial value problems and boundary value problems, for second-order homogeneous equations with constant coefficients. Example 11.1.5. (1) Find the general solution of the equation y′′ + 3y′ + 2y = 0 on R. (2) Find a solution y : [0, ∞) → R of the initial value problem y′′ + 36y = 0, y(0) = 0, y′(0) = 3. (3) Find a solution y : [0, 1] → R of the boundary value problem y′′ + 12y′ + 36y = 0, y(0) = 0, y(1) = 3. Solution. (1) The characteristic equation λ2 + 3λ + 2 = (λ + 1)(λ + 2) = 0 has the two real roots λ = −1 and λ = −2. Using Theorem 11.1.4 (and Table 11.1), the general solution is y(x) := C1e−x + C2e−2x for all x ∈ R, where C1, C2 ∈ R. (2) The characteristic equation λ 2 + 36 = 0 has the complex roots λ = ±6i, so by Theorem 11.1.4 the general solution is y(x) := C1 cos(6x) + C2 sin(6x) for all x ∈ R, where C1, C2 ∈ R. The requirement that y(0) = 0 implies that C1 = 0, whilst y′(0) = 3 requires that 6C2 = 3 so C2 = 1 2 . The differentiable function y : [0, ∞) → R given by y(x) := 1 2 sin(6x) for all x ∈ [0, ∞) is thus a solution of the initial value problem. (3) The characteristic equation λ 2+12λ+36 = (λ+6)2 = 0 has only the single real root λ = −6, so by Theorem 11.1.4 the general solution is y(x) := C1e−6x+C2xe −6x for all x ∈ R, where C1, C2 ∈ R. The requirement that y(0) = 0 implies that C1 = 0, whilst y(1) = 3 requires that C2e−6 = 3 so C2 = 3e6. The continuous function y : [0, 1] → R given by y(x) := 3e6xe−6x for all x ∈ [0, 1] is thus a solution of the boundary value problem. 11.2. Inhomogeneous equations. The method of undetermined coefficients com- bines the following theorem with a trial-and-error procedure to find solutions of second-order inhomogeneous equations ay′′ + by′ + cy = f (x). These equations are called inhomogeneous because they are not homogeneous when the function f is not identically zero. Theorem 11.2.1. Suppose that f : R → R and that a, b, c ∈ R with a ̸= 0. The general solution of the equation ay′′ + by′ + cy = f (x) on R is given by y(x) := yc(x) + yp(x) for all x ∈ R, where yc is the general solution of the homogenous equation ay′′ + by′ + cy = 0, and yp is any particular solution of ay′′ p + by′ p + cyp = f (x), on R. 50 Proof. If y denotes the general solution to the equation ay′′ + by′ + cy = f (x) on R and yp is any particular solution to the same equation, then a(y − yp) ′′(x) + b(y − yp)′(x) + c(y − yp)(x) = [ay′′(x) + by′(x) + cy(x)] − [ay′′ p (x) + by′ p(x) + cyp(x)] = f (x) − f (x) = 0 for all x ∈ R, hence y − yp is a solution of the homogenous equation. This proves that y − yp = yc, so we have y(x) = yp(x) + yc(x) for all x ∈ R, as required. □ The main point to recognise from Theorem 11.2.1 is that the general solution to such an inhomogeneous equation can be found by solving the homogenous version of the equation (for which we can use the characteristic equation methods from the preceding lecture) and by finding one solution to the inhomogeneous equation. The method of undetermined coefficients proceeds by guessing that any such particular solution might have a similar form to the inhomogeneous term f (x). For example, Table 11.2 suggests some trial solutions yp for common inhomogeneous terms f . The coefficients in the right-hand column are determined by substituting the trial solution into the equation and equating coefficients. If these coefficients can be determined, then a particular solution has been obtained. If the coefficients cannot be determined, however, then another trial solution needs to be considered. Note that if a trial solution is contained in the general homogeneous solution, then it cannot be a solution of the inhomogeneous equation. In that case, it is often useful to try multiplying the trial function by x to obtain a particular solution. Inhomogeneous Term f (x) Trial Solution yp(x) polynomial a0 + a1x + . . . + anxn A0 + A1x + . . . + Anxn exponential eαx Aeαx sine sin(αx) or cosine cos(αx) A cos(αx) + B sin(αx) Table 11.4. Trial particular solutions yp suggested for common inhomogeneous terms f in the method of undetermined coefficients. The following example shows how to use the method of undetermined coefficients to obtain general solutions of second-order inhomogeneous equations. Example 11.2.2. Find the general solution of the equation y′′ + 3y′ + 2y = x2 − 5x + 2 on R. Solution. We know from Theorem 11.2.1 that y = yc + yp. We also know from Example 11.1.5(1) that yc(x) := C1e−x + C2e−2x for all x ∈ R, where C1, C2 ∈ R. To find a particular solution we use Table 11.2 and consider yp(x) := Ax2 +Bx+C. We substitute y′ p(x) := 2Ax + B and y′′ p (x) = 2A into the differential equation to obtain 2A + 3(2Ax + B) + 2(Ax2 + Bx + C) = x2 − 5x + 2. To determine the coefficients A, B and C, we equate the coefficients of x2, x and x0 to obtain the simultaneous equations 2A = 1 6A + 2B = −5 2A + 3B + 2C = 2 51 which have the solution A = 1 2 , B = −4 and C = 13 2 . Altogether, we have y(x) = C1e−x + C2e−2x + 1 2 x2 − 4x + 13 2 for all x ∈ R, where C1, C2 ∈ R. The following example shows how to use the method of undetermined coefficients to solve initial value problems for second-order inhomogeneous equations. Example 11.2.3. Find a solution y : [0, ∞) → R of the initial value problem y′′ + 3y′ − 18y = 9e3x, y(0) = 0, y′(0) = 0. Solution. We know from Theorem 11.2.1 that y = yc +yp. To find the homogeneous solution yc, we solve the characteristic equation λ2 + 3λ − 18 = (λ + 6)(λ − 3) = 0 to obtain the two real roots λ = −6 and λ = 3. Theorem 11.1.4 then shows that the general solution is yc(x) := C1e−6x + C2e3x for all x ∈ R, where C1, C2 ∈ R. To find a particular solution yp, we use Table 11.2, and after noting that e3x appears in yc, we consider yp(x) := Axe3x. We substitute y′ p(x) = 3Axe3x + Ae3x and y′′ p (x) = 9Axe3x + 6Ae3x into the differential equation to obtain (9Axe 3x + 6Ae3x) + 3(3Axe3x + Ae 3x) − 18Axe3x = 9e3x. We then equate the coefficients of e3x and xe3x to obtain the simultaneous equations 18A − 18A = 0 and 9A = 9, which have the solution A = 1, so yp(x) := xe3x is a particular solution to the inhomogeneous equation. We now have y(x) = C1e−6x + C2e3x + xe 3x for all x ∈ R, where C1, C2 ∈ R. The requirement that y(0) = 0 implies that C1 + C2 = 0, whilst y′(0) = 0 requires that −6C1 + 3C2 + 1 = 0, so C1 = 1 9 and C2 = − 1 9 . Altogether, the differentiable function y : [0, ∞) → R given by y(x) := 1 9 e−6x − 1 9 e3x + xe 3x for all x ∈ [0, ∞) is thus a solution of the boundary value problem. We conclude with an example to show how second-order differential equations with constant coefficients model the motion of vibrating springs in physics. Example 11.2.4 (Vibrating Springs). Consider a small object of mass m > 0 attached to the end of a horizontal spring. Let x(t) denote the position of the object with respect to its equilibrium position at time t ≥ 0. According to Hooke’s Law, the spring exerts a restorative force on the object equal to −kx, where k > 0 is known as the spring constant, whilst Newton’s Law asserts that this force must be equal to mx′′. We thus obtain the second-order differential equation mx′′ + kx = 0. This is a homogeneous equation and the characteristic equation mλ2 + k = 0 has the complex roots λ = ±√ k m i, so by Theorem 11.1.4 we obtain the general solution x(t) := C1 cos(√ k m t) + C2 sin(√ k m t) for all t ∈ (0, ∞), where C1, C2 ∈ R. The oscillatory behaviour of the object’s motion described by this equation is known as simple harmonic motion. A more realistic model of the object’s motion is obtained by incorporating the frictional force −cx ′ exerted on the object, where c > 0 is known as the coefficient of friction, to obtain the equation mx′′ + cx ′ + kx = 0. In this case, the roots of the characteristic equation mλ2 + cλ + k = 0 are given by λ = 1 2m (−c ± √c2 − 4mk), so by Theorem 11.1.4, and in particular Table 11.1, the nature of the motion will be governed by the sign of the term c 2 − 4mk. More specifically, we identify the following three types of motion: 52 (1) If c 2 − 4mk < 0, then the under damped motion is modelled by x(t) := e−αt(C1 cos(ωt) + C2 sin(ωt)) for all t ∈ (0, ∞), where α := c 2m and ω := 1 2m √|c2 − 4mk|. (2) If c 2 − 4mk = 0, then the critically damped motion is modelled by x(t) := C1e−αt + C2te −αt for all t ∈ (0, ∞), where α := c 2m . (3) If c 2 − 4mk > 0, then the over damped motion is modelled by x(t) := C1e−(α−ω)t + C2e−(α+ω)t for all t ∈ (0, ∞), where α := c 2m and ω := 1 2m √|c2 − 4mk|.","libVersion":"0.3.2","langs":""}