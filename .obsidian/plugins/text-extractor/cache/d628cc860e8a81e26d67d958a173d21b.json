{"path":"VGLA/Lecture/1VGLA_AU.pdf","text":"1VGLA Vectors, Geometry & Linear Algebra 2024 – 2025 LECTURE NOTES Andrew Treglown and Galane Luo i Preamble 1. The lecture notes cover material presented in both Semester 1 and Semester 2 of the 20 credit module 1VGLA. 2. The material presented in these notes can be supplemented by reading the recom- mended chapters of the text book by Adams and Essex [2], online resources available via links on the American Institute of Mathematics website (such as [4] and [5]) or other recommended text books. Recommendations to specific resources are given at the beginning of each chapter. 3. In these notes, at the beginning of each chapter, the “Learning Outcomes\" for that chapter are included. The definition of “Learning Outcomes\", given below, is taken from the Quality Assurance Agency [6]. Definition 0.1. (Learning Outcomes)“Learning outcomes\" are statements of what a learner is expected to know, understand and/or be able to demonstrate after completion of a process of learning. • Each module will have a number of formally identified learning outcomes where each outcome is expected to relate to a fundamental aspect of the module. The learning outcome informs the curriculum content and the assessment strategy for the module. • The learning outcomes must be assessable and formal assessment criteria pro- vided which tell the learners how it will be determined that the outcomes have been met. • Normally the learner should satisfy all specific outcomes before credit is awarded. Quality or standard of performance may be reflected by the use of a grading system. ii Contents 1 Vectors 1 1.1 Notation and definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Parallel Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Position Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.4 Rectangular Cartesian Coordinate Systems . . . . . . . . . . . . . . . . . . 5 1.5 Uses of Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.6 Vector Equation of a Line . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.7 Scalar Product of Two Vectors . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.8 General Right Handed Systems . . . . . . . . . . . . . . . . . . . . . . . . 15 1.9 Vector Product of Two Vectors . . . . . . . . . . . . . . . . . . . . . . . . 15 1.10 Equations of a Plane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.10.1 Scalar Equation of a plane . . . . . . . . . . . . . . . . . . . . . . . 18 1.10.2 Vector Equation of a plane∗ . . . . . . . . . . . . . . . . . . . . . . 20 1.11 Intersection of Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 1.12 Lines in Three Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 1.13 Perpendicular Distance From Point to Plane . . . . . . . . . . . . . . . . . 23 1.14 Scalar Triple Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 1.15 An explanation of the distributive rule for the vector product . . . . . . . . 26 2 Groups and Fields 29 2.1 Binary operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.2 Groups and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 iii iv CONTENTS 3 Complex Numbers 35 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3.2 C - the field of complex numbers, . . . . . . . . . . . . . . . . . . . . . . . 36 3.2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.3 The Argand diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 3.4 Modulus-argument form (polar form) . . . . . . . . . . . . . . . . . . . . . 43 3.5 Product and Quotient using modulus-argument form . . . . . . . . . . . . 45 3.6 de Moivre’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.7 Euler’s formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.8 De Moivre’s Theorem and trigonometric formulae . . . . . . . . . . . . . . 49 3.9 Euler’s formula and trigonometric formulae . . . . . . . . . . . . . . . . . . 49 3.10 n-th roots of complex numbers . . . . . . . . . . . . . . . . . . . . . . . . . 51 3.11 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 3.12 Quadratic equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.13 The Fundamental Theorem of Algebra . . . . . . . . . . . . . . . . . . . . 54 3.14 Polynomials with real coefficients . . . . . . . . . . . . . . . . . . . . . . . 55 3.15 Basic inequalities in C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 4 Linear Equations and Matrices 61 4.1 Simultaneous linear equations . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.2 Introduction to Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.3 Matrix Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.4 Properties of Matrix Addition . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.5 Scalar Multiple of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.6 Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 4.7 Matrix Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.8 Matrices and Linear Equations . . . . . . . . . . . . . . . . . . . . . . . . . 73 4.9 Elementary Row Operations . . . . . . . . . . . . . . . . . . . . . . . . . . 73 4.10 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 4.11 Guidance for Reducing to Echelon Form . . . . . . . . . . . . . . . . . . . 78 4.12 Block Matrices∗ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 CONTENTS v 5 The Inverse of an Invertible Matrix 85 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 5.2 Gaussian Elimination Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 88 5.3 Introduction to Elementary Matrices . . . . . . . . . . . . . . . . . . . . . 91 5.4 Some Basic Properties of Elementary Matrices . . . . . . . . . . . . . . . . 92 5.5 Validity of the Gaussian Elimination Algorithm . . . . . . . . . . . . . . . 95 6 Conics 99 6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 6.2 Parabola . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 6.2.1 The standard equation of a parabola . . . . . . . . . . . . . . . . . 100 6.2.2 The graph with equation y = ax2 + bx + c . . . . . . . . . . . . . . 101 6.2.3 Alternative equations . . . . . . . . . . . . . . . . . . . . . . . . . . 102 6.3 Ellipse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 6.3.1 The standard equation of an ellipse . . . . . . . . . . . . . . . . . . 104 6.3.2 Eccentricity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 6.3.3 Additional equations and properties . . . . . . . . . . . . . . . . . . 107 6.4 Hyperbola . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 6.4.1 Standard equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 6.4.2 Asymptotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 6.4.3 Eccentricity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 6.4.4 Additional equations and properties . . . . . . . . . . . . . . . . . . 112 6.5 General polar equation of a conic . . . . . . . . . . . . . . . . . . . . . . . 113 7 Determinants 115 7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 7.2 An excursion in to group theory . . . . . . . . . . . . . . . . . . . . . . . . 117 7.3 Definition of the Determinant . . . . . . . . . . . . . . . . . . . . . . . . . 120 7.4 The transpose matrix and its determinant . . . . . . . . . . . . . . . . . . 122 7.5 Determinants and row/column operations . . . . . . . . . . . . . . . . . . 124 vi CONTENTS 7.6 Row/Column expansion of the determinant . . . . . . . . . . . . . . . . . . 128 7.7 Triangular matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 7.8 Calculating determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 7.9 Determinants of elementary matrices . . . . . . . . . . . . . . . . . . . . . 135 7.10 Two major theorems about determinants . . . . . . . . . . . . . . . . . . . 138 7.11 Cofactor and adjoint matrices . . . . . . . . . . . . . . . . . . . . . . . . . 142 7.12 Cramer’s rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 7.13 Eigenvalues of square matrices . . . . . . . . . . . . . . . . . . . . . . . . . 147 8 Vector Spaces 149 8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 8.2 Definition of a vector space . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 8.2.1 E2: Vectors in the plane . . . . . . . . . . . . . . . . . . . . . . . . 152 8.2.2 R 3: Points in 3-dimensional space . . . . . . . . . . . . . . . . . . . 153 8.3 Properties of vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 8.4 Subspaces of vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 8.5 Spanning set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 8.6 Linear (In)dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 8.7 Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 8.8 More properties of vector space bases . . . . . . . . . . . . . . . . . . . . . 170 8.9 Vector spaces arising from linear ODEs ∗ . . . . . . . . . . . . . . . . . . . 174 8.10 The dimension of a sum of subspaces . . . . . . . . . . . . . . . . . . . . . 175 8.11 Row space, column space, row rank and column rank of a matrix . . . . . . 177 8.12 Systems of linear equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 9 Linear Transformations 185 9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 9.2 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 9.3 Properties of Linear Transformations . . . . . . . . . . . . . . . . . . . . . 187 9.4 Kernel and image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 9.5 Matrix representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 9.6 Transformation of coordinates . . . . . . . . . . . . . . . . . . . . . . . . . 194 9.7 Transition matrices; change of basis matrices . . . . . . . . . . . . . . . . . 197 CONTENTS vii A Sets and Notation i A.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . i A.2 Interval Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii A.3 Inclusion Among Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv A.4 Intersection, Union and Difference . . . . . . . . . . . . . . . . . . . . . . . v A.5 The Empty Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi A.6 Operations With Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii A.7 The Universal Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix A.8 de Morgan’s Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix A.9 Cartesian Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi B Mathematical Induction xiii C Conics: Optional Extra Content xix C.1 General notions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix C.1.1 Intersection of a cone and a plane . . . . . . . . . . . . . . . . . . . xix C.1.2 Rotation of the coordinate system . . . . . . . . . . . . . . . . . . . xx C.1.3 Translation of the coordinate system . . . . . . . . . . . . . . . . . xxi C.2 Parabola . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxii C.2.1 Parabolas with vertex at the origin . . . . . . . . . . . . . . . . . . xxiii C.2.2 Shifting a parabola . . . . . . . . . . . . . . . . . . . . . . . . . . . xxvi C.2.3 The tangent to a parabola . . . . . . . . . . . . . . . . . . . . . . . xxviii C.2.4 Reflective property . . . . . . . . . . . . . . . . . . . . . . . . . . . xxx C.3 Ellipse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxxii C.3.1 Ellipse with a vertical major axis . . . . . . . . . . . . . . . . . . . xxxv C.3.2 Ellipse with centre at P (q, s) . . . . . . . . . . . . . . . . . . . . . . xxxvi C.3.3 Ellipse with a tilted major axis . . . . . . . . . . . . . . . . . . . . xxxviii C.3.4 The tangent to an ellipse . . . . . . . . . . . . . . . . . . . . . . . . xxxix C.3.5 Reflection property . . . . . . . . . . . . . . . . . . . . . . . . . . . xlii C.4 Hyperbola . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xliii viii CONTENTS C.4.1 Alternative formulae . . . . . . . . . . . . . . . . . . . . . . . . . . xlv C.4.2 The tangent to a hyperbola . . . . . . . . . . . . . . . . . . . . . . xlviii C.4.3 Reflection property . . . . . . . . . . . . . . . . . . . . . . . . . . . li C.5 Classification of quadratic equations in 2 variables . . . . . . . . . . . . . . lii Index liii Bibliography liii Chapter 1 Vectors ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • understand basic properties of vectors in 2 and 3 dimensions; • use vectors to define and work with lines and planes in 2 and 3 dimen- sions; and • understand scalar products and vector products and their applications. [2, p.564-594] contains an alternative presentation of material in this chapter that you may find helpful. Definition 1.1. A vector is a mathematical object (thing) that has both magnitude and direction. For example, displacement, velocity and acceleration are all vector quantities as they have both magnitude and direction. Temperature is an example of a scalar object which has only magnitude. In two and three dimensions, vectors are often represented as in Figure 1.1 by an initial point and a final point of a line segment. Figure 1.1: Here, the arrow’s head indicates the direction of the vector and the length of the line represents the magnitude of the vector. Suppose that P and Q are points (locations). By ⃗P Q, we represent a vector in the direction P to Q with magnitude given by the distance between P and Q. 1 2 CHAPTER 1. VECTORS The zero vector is defined to be the vector which has magnitude 0. Since it has no length, it can’t point in any particular direction (or it points in all directions at once). We choose to express this property as saying that it has no direction. We represent the zero vector by 0. We have 0 = ⃗P P for P any point. Definition 1.2. The set of vectors in three dimensions, denoted by E3, is the set containing all such vectors described above (with P, Q ∈ R 3) together with the zero vector. A similar definition applies for En, vectors in n-dimensions for n ∈ N \\ {1}. En = { ⃗P Q : P, Q ∈ R n}. Remember that a vector only has direction and magnitude and so ⃗P Q = ⃗RS does not necessarily mean P = R and Q = S. Rather, it only means that the direction from P to Q is the same as the direction from R to S and that P and Q are the same distance apart as R and S. 1.1 Notation and definitions We will use lowercase bold letters e.g. u, a and v to denote vectors (some other variations of this may appear). Sometimes, in typed text, underlined symbols is used. Notation 1.3. Suppose that u is a vector. Then |u| denotes the magnitude of the vector u. For all vectors u, we have |u| is a non-negative real number. Definition 1.4. Let u and v be vectors. Then u = v if and only if u and v have the same magnitude and direction. If v is a vector, −v is the unique vector which has magnitude the same as v and opposite direction. Hence if v = ⃗P Q, then −v = ⃗QP . Definition 1.5. (Vector Addition) Suppose that u and v are vectors. Choose points P , Q and R such that u = ⃗P Q and v = ⃗QR. Then u + v = ⃗P R. 1.1. NOTATION AND DEFINITIONS 3 With Definition 1.5, we obtain that vector addition is commutative and associative, that is for all vectors u, v, v ∈ En, (u + v) = (v + u), (1.1) (u + v) + w = u + (v + w). (1.2) Notation 1.6. (Vector Subtraction) Vector subtraction is defined as follows: u − v = u + (−v). Given a real number α ∈ R and a vector u we can change the length of u by “scaling\" it by α. For this reason, in this context, we call the elements of R scalars. Definition 1.7. (Scalar Multiplication) Given a vector v and a real number α (called a scalar), we define the scalar multiple αv of v by: (1) If α > 0, then αv has magnitude α|v| and same direction as v. (2) If α < 0, then αv has magnitude |α||v| and the same direction as −v. (3) If α = 0, then αv = 0. If u and v are vectors and α, β are scalars, then it follows from Definition 1.7 that: (α + β)v = αv + βv, (1.3) α(u + v) = αu + αv, (1.4) α(βv) = (αβ)v. (1.5) Notice that in (1.3), the addition sign on the left-hand side represents addition of real numbers, whereas the addition sign on the right-hand side represents addition of vectors. Definition 1.8. A non-zero vector v is a unit vector if and only if |v| = 1. 4 CHAPTER 1. VECTORS 1.2 Parallel Vectors Definition 1.9. Non-zero vectors u and v in En are parallel (sometimes denoted u ∥ v) if and only if either 1. they have the same direction; or 2. they have the opposite direction. Thus the non-zero vectors that are parallel to u are {αu : α ∈ R \\ {0}}. Three distinct points P , Q and R are co-linear if and only if ⃗P Q and ⃗P R are parallel, which is if and only if ∃ α ∈ R \\ {0} such that ⃗P Q = α ⃗P R. Notice that, as Q ̸= R, α ̸= 1. 1.3 Position Vectors Definition 1.10. Let A be a point in R n and denote the origin of R n by O. The position vector a of A (with respect to the origin O) is the vector ⃗OA. If points A and B have position vectors a and b relative to an origin O, then: ⃗AB = b − a. This follows since1 ⃗OA + ⃗AB = ⃗OB ⇐⇒ ⃗AO + ( ⃗OA + ⃗AB) = ⃗AO + ⃗OB (via Definition 1.5) ⇐⇒ ( ⃗AO + ⃗OA) + ⃗AB = ⃗AO + ⃗OB (via (1.2)) ⇐⇒ ⃗AB + ( ⃗AO + ⃗OA) = ⃗OB + ⃗AO (via (1.1)) ⇐⇒ ⃗AB + ( ⃗AO − ⃗AO) = ⃗OB − ⃗OA (via Definition 1.4) ⇐⇒ ⃗AB + 0 = b − a (via Definition 1.5 (2)) ⇐⇒ ⃗AB = b − a (via Definition 1.5 (3)). (1.6) 1Typically we do not require this amount of detail when giving justification of a mathematical state- ment. We do so here for absolute clarity. 1.4. RECTANGULAR CARTESIAN COORDINATE SYSTEMS 5 Example 1: Suppose A, B and C have distinct position vectors a, b and c relative to O and that C is on the line segment joining A to B. Assume that the distance from A to C and the distance from C to B are in the ratio α : β. Express c as a sum of scalar multiples of a and b. Answer: Since C divides AB internally, in the ratio α : β, we have | ⃗AC| | ⃗CB| = α β from which it follows that β| ⃗AC| = α| ⃗CB|. (1.7) As ⃗AC and ⃗CB have the same direction, it follows from (1.7) that β ⃗AC = α ⃗CB β(c − a) = α(b − c) αc + βc = αb + βa (α + β)c = αb + βa. Therefore, c = β α + β a + α α + β b. SPECIAL CASE: If C is the midpoint of AB then α = β = 1 and the position vector of the midpoint of AB is: 1 2 (a + b). 1.4 Rectangular Cartesian Coordinate Systems In a rectangular Cartesian2 coordinate system in three dimensions, the reference frame- work consists of a fixed point called the origin (usually denoted by O) and three mutually perpendicular straight lines through O known as the coordinate axes. Very often the coordinate axes are labelled the x-axis, the y-axis and the z-axis. We usually think of the x- and y-axes as lying in a flat plane with the z-axis being vertical. Each point P in 3-dimensional space can then be identified by the means of an ordered triple (x1, y1, z1) of real numbers (see Figure 1.2). The real number ‘x1’ is the signed perpendicular distance of P from the plane containing the y- and z-axes (known as the yz-plane) and the real numbers y1 and z1 are the signed perpendicular distances of P from the zx-plane and the xy-plane respectively. In Figure 1.2 the xy-plane can be considered as the paper, and the z coordinate of the point P (namely z1) is obtained by attaching a + or − to the 6 CHAPTER 1. VECTORS O x y z P (x1, y1, z1) x1 y1 z1 Figure 1.2: A representation of a 3-dimensional Cartesian system. perpendicular distance of P to the xy-plane according to whether or not the point P is above or below the paper, as indicated by the arrow on the z-axis. Similarly the arrows on the y- and z-axes indicate the signs that are to be attached to the perpendicular distances of P from the zx-plane and the xy-plane respectively to give the y and z coordinates of P . The rectangular Cartesian coordinate system Oxyz illustrated in Figure 1.2 is an example of a right-handed Cartesian coordinate system. A rectangular coordinate system in three dimensions which is not right-handed is said to be left-handed. We shall now show how vectors in 3-dimensional space as well as points in 3-dimensional space can be labelled by ordered triples of real numbers. We shall arrange things in such a way that if (x1, y1, z1) are the coordinates of a point P in a rectangular Cartesian coordinate system Oxyz, the position vector p (equal to ⃗OP ) of P relative to O is also labelled by the same ordered triple (x1, y1, z1). For ease of illustration, we shall consider a point P = (x1, y1, z1) whose coordinates are all positive. Let i, j and k be three vectors whose directions are parallel to the x-axis, the y-axis and the z-axis respectively and in the direction of increasing x, increasing y, and increasing z respectively. Another way of saying this, is to say that i, j and k are the position vectors relative to O of the points (1, 0, 0), (0, 1, 0) and (0, 0, 1) respectively. Note that the unit vectors i, j and k (in that order) form a right handed triad (of mutually perpendicular unit vectors). Consider the rectangular block OP QRST U V as shown in Figure 1.3. Since P = (x1, y1, z1) with x1, y1 and z1 all positive, we have ⃗OR = x1i, ⃗OS = y1j and ⃗OT = z1k. Thus, p = ⃗OP 2Named after René Descartes [10]. 1.5. USES OF COMPONENTS 7 O x y z P (x1, y1, z1) R U S T Figure 1.3: P in a 3-dimensional Cartesian system as a sum of three perpendicular vectors. = ⃗OR + ⃗RP = ⃗OR + ⃗RU + ⃗U P = x1i + y1j + z1k. (1.8) The numbers x1, y1 and z1 in (1.8) are known as the components of p relative to the base {i, j, k}. It is usual to omit reference to the base {i, j, k} and to simply write p = (x1, y1, z1). We are therefore using p = (x1, y1, z1) as notation for p = x1i + y1j + z1k i.e. we are representing the vector p by its components x1, y1 and z1 relative to the base {i, j, k}. The components of the direction vector of the point P relative to O are the same as the coordinates of p in the rectangular Cartesian coordinate system Oxyz. 1.5 Uses of Components Let p = (x1, y1, z1) and q = (x2, y2, z2) be vectors in 3-dimensional space relative to some base. Then: (i) p + q = (x1 + x2, y1 + y2, z1 + z2). (ii) p − q = (x1 − x2, y1 − y2, z1 − z2). (iii) αp = (αx1, αy1, αz1) for α ∈ R. (iv) |p| = √x2 1 + y2 1 + z2 1. This is referred to as the Euclidean length of p. Note that for x ∈ [0, ∞), √ x indicates the non-negative real number which when multi- plied by itself, is equal to x. 8 CHAPTER 1. VECTORS Also, note that for En with n ∈ N \\ {1}, identities analogous to (i)-(iv) hold with the Euclidean length of ⃗P Q for initial point P and final point Q (where p = (p1, p2, . . . , pn) and q = (q1, q2, . . . , qn)), given by | ⃗P Q| = |q − p| = √ √ √ √ n∑ i=1(qi − pi)2. So, for example, in E3 if P = (x1, y1, z1) and Q = (x2, y2, z2), the Euclidean length of ⃗P Q is given by: | ⃗P Q| = √(x2 − x1)2 + (y2 − y1)2 + (z2 − z1)2. Example 2: If P = (2, 1, 3), Q = (3, −2, 4) and R = (0, 7, 5), then ⃗P Q = ⃗OQ− ⃗OP = q−p = (3, −2, 4)−(2, 1, 3) = (1, −3, 1) and ⃗P R = ⃗OR− ⃗OP = (−2, 6, 2). Since ⃗P R is not a multiple of ⃗P Q it follows that P , R and Q are not co-linear. Fact 1.11. There are two unit-vectors parallel to any non-zero vector v in En. These two unit vectors are: 1 |v| v and − 1 |v| v. Example 3: Taking v = (1, −3, 4), then |v| = √12 + 32 + 42 = √ 26. Therefore Fact 1.11 implies there are exactly two unit vectors parallel to v: ( 1 √ 26, −3 √ 26, 4 √ 26 ) and ( −1 √ 26, 3 √ 26, −4 √ 26 ) . 1.6 Vector Equation of a Line Suppose that q is a non-zero vector and P is a point. We want to define a line L such that P is on the line L and if R is on the line L with R ̸= P , then q and ⃗P R are parallel. This means that we want ⃗P R = αq for some real number α ̸= 0. Now ⃗P R = r − p 1.7. SCALAR PRODUCT OF TWO VECTORS 9 hence r − p = αq. (1.9) In (1.9), r and p are the position vectors of R and P respectively. It follows that r = p + αq. (1.10) Equation (1.10) is the vector equation of the straight line that passes through the point P and which has the same direction as q. Note that q is sometimes referred to as the direction vector of L and we will say that the line L is a parallel to q. Formally, the line L through the point P parallel to the vector q is defined as the set of points { R ∈ R 3 : r = p + αq for α ∈ R } . Example 4: Suppose that P = (1, 2, 3) and q = (3, 2, 1). Then the line through P and parallel to q is L = {(1 + 3α, 2 + 2α, 3 + α) : α ∈ R}. 1.7 Scalar Product of Two Vectors The scalar product of two vectors is also referred to as the ‘dot product’. Definition 1.12. Given two vectors u and v in E3 (or E2), the scalar product is denoted by u · v and is defined as follows: (1) If u ̸= 0 and v ̸= 0, then u · v = |u||v| cos (θuv) where θuv is the non-reflex angle between u and v. (2) If u = 0 or v = 0, then u · v = 0. In relation to Definition 1.12 we have the following comments: 1. If necessary, translate u or v to arrange the angle as shown in Figure 1.4. 2. By non-reflex angle, we mean that 0 ≤ θu,v ≤ π. 10 CHAPTER 1. VECTORS u O v θuv u O v θuv Figure 1.4: Illustration of the terms in the scalar product of two pairs of vectors in E3. 3. If u and v point in the same direction, then θu,v = 0 and so, u · v = |u||v| cos (0) = |u||v|. Similarly, if u and v point in opposite directions, then θu,v = π and so, u · v = |u||v| cos (π) = −|u||v|. 4. If u and v are perpendicular (sometimes denoted u ⊥ v), then θu,v = π 2 , and so, u · v = |u||v| cos ( π 2 ) = 0. Therefore, if u · v = 0, then either u = 0, v = 0, or u and v are perpendicular. (1.11) 5. Since u · u = |u||u| cos (0) = (|u|) 2, we observe that |u| = √u · u. 6. If u and v are vectors in E2 (or E3) and α ∈ (0, ∞) then, u · (αv) = |u||αv| cos (θu,αv) = α|u||v| cos (θu,v) = α(u · v). More generally, for any α ∈ R we have: (αv) · u = v · (αu) = α(v · u). 7. If u, v and w are vectors in E2 (or E3), then the scalar product is distributive over addition of vectors, i.e. u · (v + w) = u · v + u · w and (v + w) · u = v · u + w · u. (1.12) To see that (1.12) holds in E2, see Figure 1.5. 1.7. SCALAR PRODUCT OF TWO VECTORS 11 SP Q θu,v R θu,w |v|cos (θu,v) |w|cos (θu,w) T1 Q SP R θu,v+w |v + w|cos (θu,v+w) T2 Figure 1.5: The distributive property in equations (1.12) in E2 can be seen to hold from the diagrams above. Here u = ⃗P S, v = ⃗P Q and w = ⃗QR. Observe that | ⃗P T1| is given by 1 |u|u · v + 1 |u| u · w, | ⃗P T2| is given by 1 |u| u · (v + w). As | ⃗P T1| = | ⃗P T2|, the distributive law follows. The result in E3 follows similarly. Example 5: For vectors i = (1, 0, 0), j = (0, 1, 0) and k = (0, 0, 1), we have i · i = |i||i| cos (0) = 1 j · j = |j||j| cos (0) = 1 k · k = |k||k| cos (0) = 1 i · j = |i||j| cos ( π 2 ) = 0. Similarly, i · k = j · k = 0. 12 CHAPTER 1. VECTORS Proposition 1.13. For any two vectors in E3 given in Cartesian form, (x1, y1, z1) · (x2, y2, z2) = x1x2 + y1y2 + z1z2. Similarly for two vectors in E2 given in Cartesian form, (x1, y1) · (x2, y2) = x1x2 + y1y2. Proof: We provide a proof in E3 with the proof in E2 following similarly. Since for i, j and k in Example 5, we have for u, v ∈ E3 that u = (x1, y1, z1) = x1i + y1j + z1k and v = (x2, y2, z2) = x2i + y2j + z2k, (1.13) for scalars xi, yi, zi ∈ R for i = 1, 2. Since the dot product distributes over addition (from 7. above) and is linear under scalar multiplication (from 6. above), it follows from (1.13) that u · v = (x1i + y1j + z1k) · (x2i + y2j + z2k) = x1x2i · i + x1y2i · j + x1z2i · k + y1x2j · i + y1y2j · j + y1z2j · k + z1x2k · i + z1y2k · j + z1z2k · k = x1x2 + y1y2 + z1z2, via Example 5, as required. Example 6: Find all λ ∈ R for which u = (7, 2, λ) is perpendicular to v = (1, −3, λ). Answer: Via (1.11) u and v are non-zero, and perpendicular if and only if u · v = 0. Observe that u · v = (7, 2, λ) · (1, −3, λ) = (7i + 2j + λk) · (1i − 3j + λk) = 7i · i − 6j · j + λ 2k · k where we note that all expressions involving i · j etc are equal to zero. Hence u ⊥ v if and only if 7 − 6 + λ 2 = 0 (λ ∈ R). This quadratic equation has no real number solutions. Therefore, for every λ ∈ R, u is not perpendicular to v. Using cos (θ) = u · v |u||v| 1.7. SCALAR PRODUCT OF TWO VECTORS 13 we see that the scalar product encapsulates the idea of “angle” and the important idea of orthogonality. Example 7: Find the (non-reflex) angle between u = (1, −2, 2) and v = (−2, 2, 1). Answer: Here u · v = (1, −2, 2) · (−2, 2, 1) = −2 − 4 + 2 = −4 and |u| = √ 12 + (−2)2 + 22 = 3, which is the same as |v|. Therefore, cos (θ) = −4 9 , and moreover, θ = arccos (− 4 9 ), as required. We can check for the five key properties of the scalar product as an operation between two vectors: 1. The scalar product is not an internal operation, since it takes two vectors as input and outputs a real number, not a vector. 2. The scalar product is not associative. To clarify, for every u, v, w ∈ E3 the state- ment (u · v) · w is meaningless, since u · v ̸∈ E3 and the scalar product requires both arguments to be (in this case) in E3. 3. The scalar product has no identity, since the output of the product a · b can never be a vector, so definitively not a. 4. Via 3., the scalar product cannot have an inverse. 5. The scalar product is commutative since a · b = b · a. Example 8: Express the vector w = (2, 3, −1) in the form w = u + v where u is parallel to (1, 1, 1) and v is perpendicular to u. Answer: Here we require w = α(1, 1, 1) + v where v is perpendicular to u i.e. w = (2, 3, −1) = α(1, 1, 1) + v. (1.14) Now v = (2, 3, −1) − α(1, 1, 1) = (2 − α, 3 − α, −1 − α) (1.15) and since u, v ̸= 0 and u · v = 0, we have (1, 1, 1) · (2 − α, 3 − α, −1 − α) = 0 2 − α + 3 − α − 1 − α = 0 14 CHAPTER 1. VECTORS 4 − 3α = 0 α = 4 3 . (1.16) Substituting (1.16) and (1.15) into (1.14) gives w = 4 3(1, 1, 1) + 1 3(2, 5, −7), as required. Definition 1.14. Given two vectors u and w, the projection of w onto u is the vector parallel to u given by proju (w) = ( w · u |u| ) u |u| . The projection onto a unit vector e simplifies to proje (w) = (w · e) e. u0 w θu,w proju(w) Figure 1.6: Geometric representation of the projection of w onto u. Example 9: Express the vector w = (2, 3, −1) as w = u + v where u is parallel to (1, 1, 1) and v is perpendicular to u. Answer: We know that u is the projection of w onto t = (1, 1, 1). So we calculate u = projt (w) = ( w · t |t| ) t |t| = ( (2, 3, −1) · (1, 1, 1) √3 ) 1 √3(1, 1, 1) = ( 4 3, 4 3 , 4 3 ) . Then v is given by v = w − u = (2, 3, −1) − ( 4 3 , 4 3 , 4 3 ) = ( 2 3, 5 3, −7 3 ) . By construction, v is perpendicular to t, and can be checked as follows: ( 2 3, 5 3, −7 3 ) · (1, 1, 1) = 2 + 5 − 7 3 = 0. Since u is parallel to t is follows that v is perpendicular to u, as required. 1.8. GENERAL RIGHT HANDED SYSTEMS 15 1.8 General Right Handed Systems Let u, v and w be three non co-planar vectors. Let θu,v be the non-reflex angle from u to v. Then u, v and w form a right handed triad or a right handed system. This system, as depicted in Figure 1.7 is sometimes referred to as “perturbed\". u v θuv w θvw Figure 1.7: A perturbed right handed system. 1.9 Vector Product of Two Vectors Definition 1.15. The vector product of two vectors u and v in E3 is denoted by u × v. The definition is split into three cases: (1) If u, v ̸= 0 and u is not parallel to v. Let ⃗OU and ⃗OV represent u and v respectively and let θ be the non reflex angle between u and v. Then u × v is a vector with: (a) magnitude given by, |u × v| = |u||v| sin θ. (b) direction perpendicular to both u and v such that u, v and u × v are a right handed triad. (2) If u and v are non-zero parallel vectors, then u × v = 0. (3) If u = 0 or v = 0, then u × v = 0 and in particular, 0 × 0 = 0. 16 CHAPTER 1. VECTORS u v θ u × v Figure 1.8: Depiction of Definition 1.15 (1). Note that the area of the parallelogram with sides u and v is |u||v| sin (θ). In relation to Definition 1.15 we have the following comments: • Note that u × v ̸= |u||v| sin θ. Geometrically, u × v, defines a vector that is perpendicular to u and v with direc- tion determined via the right hand rule, and which has length defined by the area |u||v| sin θ of the parallelogram defined by u × v, depicted in Figure 1.8. • Also, importantly, it can be shown that the vector product is distributive over addition of vectors (see the end of the section) i.e. for any u, v, w ∈ E3, it follows that (u + v) × w = (u × w) + (v × w). • The vector product (as a result of the right hand rule) is anti-symmetric, i.e. for u, v ∈ E3, it follows that u × v = −v × u. Example 10: Observe that since i is parallel to itself i × i = 0. Now observe that since i ⊥ j ⊥ k ⊥ i we have i × j = k, j × k = i, k × i = j. We can check for the five key properties of the vector product as an operation between two vectors: 1. The vector product is an internal operation, since it takes two vectors as input and outputs a vector. 1.9. VECTOR PRODUCT OF TWO VECTORS 17 2. The vector product is not associative. For example the vector triple product i × (i × j) = −j and (i × i) × j = 0. Hence to write i × j × j is meaningless (it is not clear which operation to perform first). 3. The vector product has no identity since the output of the product u × v is perpen- dicular to both u and v, so can never be u. 4. Since the vector product has no identity, the vector product cannot have an inverse. 5. The vector product is not commutative since u × v = −v × u. This follows since the non-reflex angle from u to v is the reverse of that of v to u i.e. u × v and v × u are in opposite directions with the same magnitude. Example 11: Observe that j × i = −i × j = −k, k × j = −i, i × k = −j = −k × i. Proposition 1.16. (Vector product in Cartesian form) Let u = u1i + u2j + u3k and v = v1i + v2j + v3k. (1.17) Then u × v = (u2v3 − u3v2)i + (u3v1 − u1v3)j + (u1v2 − u2v1)k. Proof: Since the vector product is distributive over vector addition, we have, u × v =(u1i + u2j + u3k) × (v1i + v2j + v3k) =u1v1(i × i) + u1v2(i × j) + u1v3(i × k) + u2v1(j × i) + u2v2(j × j) + u2v3(j × k) + u3v1(k × i) + u3v2(k × j) + u3v3(k × k). So, via Examples 10 and 11 it follows that u × v = (u2v3 − u3v2)i + (u3v1 − u1v3)j + (u1v2 − u2v1)k, as required. Another way to calculate the vector product of two vectors u and v is by using a method that is analogous to calculating the determinant of a 3 × 3 matrix (we will see this in Chapter 6). 18 CHAPTER 1. VECTORS Let u = (u1, u2, u3) and v = (v1, v2, v3) denote the vectors in (1.17). Then 3 u × v = ∣ ∣ ∣ ∣ ∣ ∣ ∣ i j k u1 u2 u3 v1 v2 v3 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = i ∣ ∣ ∣ ∣ ∣u2 u3 v2 v3 ∣ ∣ ∣ ∣ ∣ − j ∣ ∣ ∣ ∣ ∣u1 u3 v1 v3 ∣ ∣ ∣ ∣ ∣ + k ∣ ∣ ∣ ∣ ∣ u1 u2 v1 v2 ∣ ∣ ∣ ∣ ∣ = (u2v3 − u3v2)i + (u3v1 − u1v3)j + (u1v2 − u2v1)k. Example 12: Calculate u × v where u = (2, 1, −2) and v = (−4, 3, 1). Hence determine a unit vector perpendicular to u and v. Answer: u × v = ∣ ∣ ∣ ∣ ∣ ∣ ∣ i j k 2 1 −2 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = i ∣ ∣ ∣ ∣ ∣1 −2 3 1 ∣ ∣ ∣ ∣ ∣ − j ∣ ∣ ∣ ∣ ∣ 2 −2 −4 1 ∣ ∣ ∣ ∣ ∣ + k ∣ ∣ ∣ ∣ ∣ 2 1 −4 3 ∣ ∣ ∣ ∣ ∣ = i(1 + 6) − j(2 − 8) + k(6 + 4) = 7i + 6j + 10k. Therefore u × v = 7i + 6j + 10k = (7, 6, 10). Since u × v is perpendicular to u and v we set w = u × v and calculate |w| 2 = 7 2 + 62 + 102 = 185. Therefore a unit vector perpendicular to u and v is given by 7 √ 185i + 6 √ 185j + 10 √185k = 1 √185(7, 6, 10). 1.10 Equations of a Plane 1.10.1 Scalar Equation of a plane The important vectors in relation to a scalar representation of a plane are those which are perpendicular to it. We call a vector which is perpendicular to a given plane a normal vector to the plane. A non-zero vector n is a normal vector to a plane Π if every directed line segment repre- senting n is perpendicular to Π. If n is a normal vector, then αn will be a normal vector for α ∈ R \\ {0}. 1.10. EQUATIONS OF A PLANE 19 S P R Π n Figure 1.9: Depiction of the points P , R and S, and normal vector n to the plane Π. Suppose P is a fixed point in the plane Π and that R is another point in Π which have position vectors p and r respectively with respect to some origin O. Suppose n is a normal vector to the plane Π and is represented by ⃗P S. The vector ⃗P R is r − p and is perpendicular to n. Hence, n · (r − p) = 0 i.e. n · r = n · p. (1.18) Thus, a scalar equation for a plane Π which is perpendicular to the vector n is of the form n · r = d (1.19) for some constant d (i.e. every point R ∈ Π with position vector r satisfies (1.19)). So if we write r = xi + yj + zk, n = ai + bj + ck and p = p1i + p2j + p3k, then via (1.19), (ai + bj + ck) · (xi + yj + zk) = (ai + bj + ck) · (p1i + p2j + p3k) and hence ax + by + cz = d, with d = p1a + p2b + p3c. In relation to the scalar equation for a plane given in (1.18) and (1.19) we have the following comments: (i) Since n ̸= 0, not all of a, b and c can be zero. (ii) If d = 0, then the plane intersects the origin. (iii) In 2-dimensional geometry, the equations x = d, y = d and ax + by = d all represent lines. However, in three dimensions, ax + by = d represents a plane as well as x = d and y = d. (iv) If U , V and W are three non co-linear points in 3-dimensional space, there is exactly one plane containing all three of them. 3Formulas for determinants of 2 × 2 and 3 × 3 matrices are derived in Example 84. 20 CHAPTER 1. VECTORS Example 13: Determine the scalar equation of a plane, denoted by Π, that contains the points U = (1, −2, 6), V = (−2, −3, 4) and W = (4, −1, 7). Answer: Let u, v and w be position vectors of U , V and W . Since, ⃗V U = u − v = (3, 1, 2) and ⃗V W = w − v = (6, 2, 3), it follows that a normal vector to any plane that contains U , V and W is given by n with, n = ⃗V U × ⃗V W = ∣ ∣ ∣ ∣ ∣ ∣ ∣ i j k 3 1 2 6 2 3 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = (3 − 4)i − (9 − 12)j + (6 − 6)k = −i + 3j. Thus, let r = xi + yj + zk be a point on Π. Since U = (1, −2, 6) is on the plane Π, let u = i − 2j + 6k be the position vector of U . Finally, via (1.18), the plane Π is determined by n · r = n · u, i.e. −x + 3y = −1 − 6 ⇐⇒ −x + 3y = −7. (1.20) So Π = {(x, y, z) ∈ R 3 | −x + 3y + 7 = 0}. Note that to check the validity of the answer (1.20), substitution of r equal to U , V or W into (1.20) should satisfy the equation, since these 3 points should be contained within the plane. Additionally, one should also check that the equation in (1.20) is that of a plane (which it is). 1.10.2 Vector Equation of a plane∗ The important vectors in relation to a vector representation of a plane are those which are parallel to it. We can represent a plane in 3-dimensional space using a vector representation. By choos- ing a vector p on the plane and 2 non-colinear vectors parallel to the plane, v1 and v2, we can represent any other point r on the plane as r = p + λ1v1 + λ2v2 for some λ1, λ2 ∈ R. Returning to Example 13 we have Π = {R : ⃗OR = ⃗OU + λ1 ⃗V U + λ2 ⃗V W with λ1, λ2 ∈ R} = {(1, −2, 6) + λ1(3, 1, 2) + λ2(6, 2, 3) : λ1, λ2 ∈ R} = {(1 + 3λ1 + 6λ2, −2 + λ1 + 2λ2, 6 + 2λ1 + 3λ2) : λ1, λ2 ∈ R}. 1.11. INTERSECTION OF PLANES 21 1.11 Intersection of Planes Consider three distinct planes Π1, Π2 and Π3. Then one of the following statements is true: (i) The planes do not intersect. For example, consider the planes x = 0, x = 1 and x = 2. Clearly these 3 planes do not intersect (and are parallel). (ii) The planes intersect at one point. For example, consider the planes x = 0, y = 0 and z = 0. Clearly these planes intersect at one point, (0, 0, 0). (iii) The planes intersect on a straight line. For example, consider the planes z = 0, z = y and z = −y. The intersection of these 3 planes is the set {(x, 0, 0) : x ∈ R}, alternatively known as the x-axis. More-or-less all configurations of three planes. 1.12 Lines in Three Dimensions Let v be a direction vector defined by a line L (this it not unique since although the direction is fixed up to sign of the vector, the length is not) and the position vector p correspond to a point P on L. Let R (with position vector r) be any other point on the line L. Then, ⃗P R = r − p = αv for some α ∈ R. Therefore a vector equation of L is: r = p + αv α ∈ R. (1.21) If r = (x, y, z), p = (p1, p2, p3) and v = (v1, v2, v3), then (1.21) is expressed as, (x, y, z) = (p1 + αv1, p2 + αv2, p3 + αv3). (1.22) Equation (1.22) allows us to define the parametric equations for a line L in 3-dimensions: x = p1 + αv1, y = p2 + αv2 and z = p3 + αv3, 22 CHAPTER 1. VECTORS with parameter α ∈ R. So we have: α = x − p1 v1 , α = y − p2 v2 , α = z − p3 v3 which implies x − p1 v1 = y − p2 v2 = z − p3 v3 . (1.23) (1.24) gives the equations for a line in standard form whenever v1, v2 and v3 are non-zero. In the case that say v2 = 0, we may still write x − p1 v1 = z − p3 v3 and y = p2. (1.24) Example 14: Find the vector equation, parametric form and standard form for the line L through the points A = (3, 2, 0) and B = (5, −2, 3). Answer: Since v = ⃗AB = (2, −4, 3) and for a point on L, we take A, it follows that u = (3, 2, 0). Then a vector equation of L is r = u + αv i.e. r = (3, 2, 0) + α(2, −4, 3) where α ∈ R. Setting r = (x, y, z) implies (x, y, z) = (3, 2, 0) + α(2, −4, 3) which can be written in parametric form: x = 3 + 2α, y = 2 − 4α and z = 3α. By eliminating α ∈ R, we have α = x − 3 2 = 2 − y 4 = z 3 , so in standard form L is given by, x − 3 2 = 2 − y 4 = z 3 . Example 15: Find where the line from the previous example intersects the plane Π with equation x − 4y + 3z = 4. Answer: We can use the parametric form for L, i.e. x = 3 + 2α, y = 2 − 4α and z = 3α 1.13. PERPENDICULAR DISTANCE FROM POINT TO PLANE 23 for α ∈ R. The point on L intersects the plane Π if any only if, (3 + 2α) − 4(2 − 4α) + 3(3α) = 4 3 + 2α − 8 + 16α + 9α = 4 27α = 9 α = 1 3 i.e. the point of intersection between L and Π occurs (when α = 1 3) at ( 11 3 , 2 3 , 1 ) . Note, when considering problems which include various lines defined in parametric form, it is advisable to use a different parameter for each line i.e. σ, τ etc. 1.13 Perpendicular Distance From Point to Plane Let a plane Π be given by r · n = d. (1.25) Let a point P have position vector p and consider a vector perpendicular to Π that connects Π to a point P ′ on the plane Π (P ′ has position vector p′, as depicted in Figure 1.10). We wish to find |p − p′| or alternatively, the distance of P to the the plane Π. P P ′ Π n Figure 1.10: Depiction of the points P and P ′ in relation to the distance of P from the plane Π. Since n is parallel to p − p′, for some α ∈ R p − p′ = αn. (1.26) Taking the scalar product with n, via (1.26) we get p · n − p′ · n = α|n| 2, and moreover, from (1.25) and P ′ ∈ Π, we conclude that p · n − d = α|n| 2. 24 CHAPTER 1. VECTORS Hence, α = p · n − d |n|2 . Then perpendicular distance from P to Π is |p − p′| = |p · n − d| |n|2 |n| = |p · n − d| |n| . (1.27) Perpendicular distance between two parallel planes: If in addition, P lies in a parallel plane (to Π), given by Π ′ : r · n = d′, then p · n = d ′ and hence the perpendicular distance between two parallel planes n · v = d and n · v = d ′ is |d − d′| |n| . Two lines in R2 or R 3: Consider 2 distinct lines L1 and L2. Then at least one of the following statements is true: 1. L1 and L2 are parallel (L1 ∥ L2) and do not intersect. 2. L1 and L2 intersect. 3. L1 and L2 are not parallel and do not intersect (sometimes referred to as skew lines). Angle between lines: Consider the lines defined by vector equations L : r = u + αv L ′ : r = u′ + βv′. Then the acute angle θ between L and L ′ is given by cos θ = ∣ ∣ ∣ ∣ ∣ v · v′ |v||v′| ∣ ∣ ∣ ∣ ∣ . 1.14 Scalar Triple Product Definition 1.17. The scalar triple product of three vectors u, v and w is u · (v × w). 1.14. SCALAR TRIPLE PRODUCT 25 v0 w θv,w u v × w θu,v×w Figure 1.11: A parallelepiped, as described in Definition 1.17. Note that the scalar triple product can be interpreted as the signed volume of a paral- lelepiped (a cuboid with every face a parallelogram), i.e. |u||v||w|| sin (θv,w)| cos (θu,v×w) = u · (v × w). Also note that the order of vectors defining the parallelepiped matters, due to the right hand rule used to define the vector product. If we switch the labels for the vectors v and w in Figure 1.11 (see Figure 1.12), it follows that v × w is oriented in the opposite direction, which changes the sign of the scalar triple product. w0 v θv,w v × w u Figure 1.12: If we re-label v and w in Figure 1.11. Since the parallelogram defined by adjacent edges v and w has the same area it follows that the magnitude of v × w does not change. However, due to the right hand rule, the direction of v × w changes. To calculate the volume of a parallelepiped with adjacent edges u, v and w one can simply compute the absolute value of u · (v × w). Note that if u, v and v have coordinates (u1, u2, u3), (v1, v2, v3) and (w1, w2, w3) with respect to i, j and k, then it follows that4 u · (v × w) = (u1, u2, u3) · ∣ ∣ ∣ ∣ ∣ ∣ ∣ i j k v1 v2 v3 w1 w2 w3 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ ∣ ∣ u1 u2 u3 v1 v2 v3 w1 w2 w3 ∣ ∣ ∣ ∣ ∣ ∣ ∣ . 4At this stage we have not covered determinants (see Chapter 6). Please review this section after we have covered Chapter 6, and this calculation will be clearer. 26 CHAPTER 1. VECTORS Example 16: Calculate the volume of the parallelepiped with adjacent edges ⃗OU , ⃗OV and ⃗OW with ⃗OU = u = (3, 0, 0), ⃗OV = v = (2, 1, 1) and ⃗OW = w = (0, 0, −2). Answer: The volume of the parallelepiped is given by the absolute value of u · (v × w) = (3, 0, 0) · ∣ ∣ ∣ ∣ ∣ ∣ ∣ i j k 2 1 1 0 0 −2 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ ∣ ∣ 3 0 0 2 1 1 0 0 −2 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = 3.1.(−2) = −6, and hence the volume of the parallelepiped is 6. 1.15 An explanation of the distributive rule for the vector product This section contains material which is additional to the course. To show that for any u, v, w ∈ E3, it follows that (u + v) × w = (u × w) + (v × w), we give the following justification. Let v be a non-zero vector and Π be the plane perpendicular to v that intersects 0. Moreover, let u∗ be the projection of u onto Π. Π v 0 u θuv u∗ w Figure 1.13: Illustration of u, v, u∗ and Π. Here w ∥ (u × v) and w ∥ (u∗ × v). Lemma 1.18. u × v = u∗ × v. 1.15. AN EXPLANATION OF THE DISTRIBUTIVE RULE FOR THE VECTOR PRODUCT27 Proof: Let θuv be the angle between u and v. Then |u∗| = |u| sin (θuv). Since u∗ ⊥ v we have, |u × v| = |u||v| sin (θuv) = |v||u∗| = |v||u∗| sin (π/2) = |u∗ × v|. (1.28) Since u, u∗ and v are coplanar (see Figure 1.13), it follows from the right hand rule that u∗ × v and u × v point in the same direction. Therefore, via (1.28), we conclude that u × v = u∗ × v, as required. Lemma 1.19. (u1 + u2)∗ = u∗ 1 + u∗ 2. Proof: Since ui − v |v||ui| cos (θvui) = u∗ i for i = 1, 2, it follows from the distributivity over addition of the scalar product (depicted in Figure 1.5) that (u1 + u2) ∗ = (u1 + u2) − ( v |v| ) |u1 + u2| cos (θu12v) = (u1 + u2) − ( v |v| ) ( v |v| · (u1 + u2) ) = (u1 + u2) − ( v |v| ) ( v |v| · u1 + v |v| · u2 ) = (u1 + u2) − ( v |v| ) (|u1| cos (θvu1) + |u2| cos (θvu2)) = ( u1 − ( v |v| ) |u1| cos (θvu1) ) + ( u2 − ( v |v| ) |u2| cos (θvu2) ) = u∗ 1 + u∗ 2, as required. Lemma 1.20. (u1 + u2) × v = (u1 × v) + (u2 × v). 28 CHAPTER 1. VECTORS u∗ u∗ × v 0 |u∗||v| Π Figure 1.14: Picture of Π with v pointing straight out of the page towards you. Here, u∗ × v is determined by rotating Π clockwise and rescaling. Recall u∗ ⊥ v. Proof: Since u × v = u∗ × v, via Lemma 1.18, it follows that u × v can be determined by rotating u∗ by π/2 radians in the appropriate direction and multiplying the resulting vector by |v| (see Figure 1.14). Therefore, from vector addition in E2 (see Figure 1.15) it follows that (u∗ 1 + u∗ 2) × v = (u∗ 1 × v) + (u∗ 2 × v). (1.29) 0 u∗ 1 u∗ 2 u∗ 1 + u∗ 2 u∗ 1 × v u∗ 2 × v (u∗ 1 + u∗ 2) × v Figure 1.15: Depiction of (1.29). Finally, we have, (u1 + u2) × v = (u1 + u2)∗ × v (via Lemma 1.18) = (u∗ 1 + u∗ 2) × v (via Lemma 1.19) = (u∗ 1 × v) + (u∗ 2 × v) (via (1.29)) = (u1 × v) + (u2 × v) (via Lemma 1.18), as required. Note that v × (u1 + u2) = (v × u1) + (v × u2) follows from Lemma 1.20 and anti-symmetry of the vector product. Chapter 2 Groups and Fields The main purpose of this chapter is to introduce some of the basic algebraic objects and concepts which underlie modern mathematics. ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • check that an operation is an internal binary operation; • determine whether a set with a binary operation is a group; and • determine if a set with two binary operations is a field. 2.1 Binary operation Consider a set V , which contains elements that can be combined by some “operation”. Definition 2.1. A binary operation on a set V to a set C is a function B : V × V → C. For x1, x2 ∈ V , we often denote the operation B(x1, x2) by x1 ∗ x2 or by some other appropriate symbol. Examples 17: • N, Z, Q, R: ‘+’ and ‘×’ are binary operations on all of these sets, but ‘÷’ is generally not (unless we remove 0 from the sets which contain it). 29 30 CHAPTER 2. GROUPS AND FIELDS • E3: ‘+’, ‘·’, ‘×’. All of these operations are binary operations on pairs of vectors in E3 the first and the last have C = E3 and the middle one has C = R. Definition 2.2. An internal binary operation on a set V is a binary operation for which C = V , or equivalently, x1 ∗ x2 ∈ V ∀x1, x2 ∈ V. If a binary operation is internal, then the set V is said to be closed under the binary operation. Examples 18: • N, Z, Q, R: ‘+’ and ‘×’ are internal binary operations for each of these sets, but not ‘÷’ (since it is not necessarily a binary operation and for N ... it is clearly not an internal binary operation). • E3: ‘+’ and ‘×’ are internal binary operations on E3, but ‘·’ is not, since v1 · v2 ∈ R ̸⊆ E3 for all v1, v2 ∈ E3. Definition 2.3. A binary operation on a set V is commutative if and only if for all x1, x2 ∈ V , x1 ∗ x2 = x2 ∗ x1. Definition 2.4. A binary operation on a set V is associative if and only if for all x1, x2, x3 ∈ V , (x1 ∗ x2) ∗ x3 = x1 ∗ (x2 ∗ x3). Definition 2.5. An element e ∈ V is called an identity for a binary operation on a set V if and only if for all x1 ∈ V , e ∗ x1 = x1 ∗ e = x1. Theorem 2.6. If a binary operation on a set V has an identity, then the identity is unique. Proof: Assume there are two elements e1 and e2 in V which are both identities. Then, e1 ∗ e2 = e2 since e1 is an identity. However, e1 ∗ e2 = e1 since e2 is an identity. Hence, e1 = e2, as required. 2.2. GROUPS AND FIELDS 31 Definition 2.7. Consider a binary operation on a set V which has identity e ∈ V . An element a ∈ V has an inverse if there exists an element b ∈ V such that a ∗ b = b ∗ a = e. Example Problem 19: Check whether the following sets with binary operations sat- isfy the properties in Definitions 2.2 (closed), 2.3 (commutative), 2.4 (associative), 2.5 (identity element) and 2.7 (inverse elements). • N, Z, Q, R: ‘+’ and ‘×’. • E3: ‘+’, ‘×’. 2.2 Groups and Fields Special names exist for sets with a binary operation that satisfy a given number of prop- erties. The first structure we define below is that of a group which has been developed into an important branch of modern mathematics called Group Theory. Definition 2.8. A set V endowed with an internal binary operation ∗ is called a group with respect to the binary operation ∗ if and only if the following hold: 1. ∗ is associative; 2. ∗ has an identity V ; and 3. Every element of V have an inverse element. Example Problem 20: Decide which of the following sets with binary operations are groups and give a justification if they are not: • (Z, +); (Q, +); (R, +). • (Q \\ {0}, ×); (R \\ {0}, ×). • (E3, +). Definition 2.9. A group with binary operation ∗ is called abelian if and only if ∗ is commutative. 32 CHAPTER 2. GROUPS AND FIELDS These groups are named after Neils Henrik Abel, a famous Norwegian mathematician after which, the Abel prize 1 is named. This prize, is one of the most prestigious in mathematics. It is possible to define more than one binary operation on a set. If one does, the com- bination of properties of each binary operation gives rise to other structures. The most important one is a field: Definition 2.10. A set F with binary operations + and × is called a field (F, +, ×) if and only if: 1. (F, +) is an abelian group, with identity denoted by 0; 2. (F \\ {0}, ×) is an abelian group, with identity denoted by 1; 3. for all x ∈ F , 0 × x = x × 0 = 0; and 4. distributivity of × with respect to +: for all x1, x2, x3 ∈ F , we have x1 × (x2 + x3) = (x1 × x2) + (x1 × x3). Example 21: The following two sets with binary operations defining addition and multiplication are fields: (Q, +, ×), (R, +, ×). We will often work with the field (R, +, ×) of real numbers. Let’s just check it is a field. 1. (R, +) is an abelian group: (a) R is closed under + since ∀ a, b ∈ R, a + b ∈ R; (b) + is associative since ∀ a, b, c ∈ R, (a + b) + c = a + (b + c); (c) + has an identity since ∀ a ∈ R, 0 ∈ R and 0 + a = a + 0 = a; (d) existence of inverses. ∀ a ∈ R, ∃ b ∈ R such that a + b = b + a = 0; (e) + is commutative since ∀ a, b ∈ R, a + b = b + a. 2. (R \\ {0}, ×) is an abelian group: (a) (R \\ {0}) is closed under × since ∀ a, b ∈ R \\ {0}, a × b ∈ R \\ {0}; (b) × is associative since ∀ a, b, c ∈ R \\ {0}, (a × b) × c = a × (b × c); (c) × has an identity since ∀ a ∈ R \\ {0}, 1 ∈ R \\ {0} and 1 × a = a × 1 = a; (d) existence of inverses. ∀ a ∈ R \\ {0}, ∃ b ∈ R \\ {0} such that a × b = b × a = 1; (e) × is commutative since ∀ a, b ∈ R \\ {0}, a × b = b × a. 3. ∀ a ∈ R, 0 × a = a × 0 = 0; and 1For more details, see [12]. 2.2. GROUPS AND FIELDS 33 4. Distributivity: ∀ a, b, c ∈ R, a × (b + c) = (a × b) + (a × c). There are many different types of fields. You will see how to make some next year if you take 2AC. Example 22: Suppose that F2 = {0, 1}. Define + to be the internal binary operation which makes (F2, +) into an abelian group with identity 0 and 1 + 1 = 0. Define an internal binary operation × on F2 so that (F2 \\ {0}, ×) is a group with identity 1. Then (F2, +, ×) is a field with 2-elements. Example 23: Suppose that Ω is a non-empty set. Let Sym(Ω) be the set of all bijections from Ω to Ω. Define a binary operation on Sym(Ω) by composition of functions. Explain why Sym(Ω) is a group. We call Sym(Ω) the symmetric group on Ω. 34 CHAPTER 2. GROUPS AND FIELDS Chapter 3 Complex Numbers ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • perform basic arithmetic operations (addition, subtraction, multiplica- tion and division) on complex numbers; • define and calculate the complex conjugate of a complex number and be able to cite the basic properties involving the complex conjugate; • plot complex numbers on an Argand diagram; • define and calculate the modulus-argument form of a complex number and perform basic arithmetic operations on complex numbers using the modulus-argument form; • cite, prove and use De Moivre’s theorem; • calculate the n-th roots of complex numbers; • cite and apply properties related to polynomials with real coefficients; • solve quadratic equations, both with real and complex coefficients; and • solve basic inequalities in C. [13, Ch.1] and [8, Ch.1] contain alternative presentations of material in this chapter that you may find helpful. 35 36 CHAPTER 3. COMPLEX NUMBERS 3.1 Introduction You should be familiar with the following sets of numbers: N ⊂ Z ⊂ Q ⊂ R. (3.1) All these sets are closed under addition and multiplication ( + and × are internal opera- tions on V ). We mostly suppress × and use juxtaposition or · whenever we can without confusion. So for any set V as in (3.1) we have: (i) If a ∈ V and b ∈ V then a + b ∈ V . (ii) If a ∈ V and b ∈ V then ab ∈ V . One can imagine that the need for expanding a set of numbers is motivated by the desire to solve problems which cannot be solved in a particular set of numbers: • In N, the equation 7 + x = 3 has no solution. In Z this equation can be solved. • In Z, the equation 7x = 3 has no solution. In Q this equation can be solved. • In Q, the equation x2 = 2 has no solution. A set in which this equation can be solved is R which includes √2. To get all of the real numbers, we also need to add limits so that numbers such as π can be defined. Complex numbers are the extension of R which is required to solve general polynomial equations, for instance x2 + 1 = 0. This equation has no real solutions, and as a con- sequence, we also have cases where the quadratic equation ax2 + bx + c = 0 cannot be solved (for x ∈ R). However every polynomial equation can be solved for values in C. 3.2 C - the field of complex numbers, Following the historic process of expanding the number set to allow the solution of given problems, we will define a new type of number which solves the equation x2 = −1: Definition 3.1. The imaginary number ‘ i ’ is defined such that i 2 = −1. 3.2. C - THE FIELD OF COMPLEX NUMBERS, 37 We can then obtain the set of all purely imaginary numbers as the product of i with all real numbers, e.g., 2i, −3.7i, √3i, . . .. The set of all purely imaginary numbers can be written as Ri. In the set R ∪ Ri, all equations of the form x2 = a, with a ∈ R can be solved. But how do we add a real number and a purely imaginary number? To allow for the definition of this addition, we need to expand the set further to the set of complex numbers: Definition 3.2. The set of complex numbers, denoted C, is defined as C = {a + bi : a ∈ R and b ∈ R} . It follows from Definition 3.2 that we can view R ⊂ C and Ri ⊂ C. Example 24: • 3 + 2i, 7 − 7i, −3 + i and −5 − 2i are in C but not in R or Ri. • 3, −8 and √5 are in R ⊂ C (b = 0 in Definition 3.2). • i, −5i, √7i and 2i/3 are in Ri ⊂ C. Normally write a single label for a complex variable, e.g. z = a + bi. We say that a + bi is the algebraic form of a complex number. Definition 3.3. The real part of a complex number z = a+bi is given by Re(z) = a. Definition 3.4. The imaginary part of a complex number z = a + bi is given by Im(z) = b. Note that both the real and the imaginary part of a complex number are real numbers. Specifically, Re(a + bi) = a ∈ R and Im(a + bi) = b ∈ R. Example 25: Simplify the following expressions: • Re(2 + 3i) + Im(2 + 3i)i. • Re(−3)Im(−3). • Re(5i) + Im(5i). 38 CHAPTER 3. COMPLEX NUMBERS 3.2.1 Definitions We now need to redefine various operations on the set of complex numbers. Definition 3.5. Complex numbers a + bi and c + di are equal if and only if a = c and b = d. Definition 3.6. The sum of two complex numbers a + bi and c + di is the complex number (a + bi) + (c + di) = (a + c) + (b + d)i. Addition is an internal binary operation on C. Example 26: Calculate the following: • (5 + 3i) + (3 + 2i). • (−3 + 2i) + (2 − 4i). • 3i + 5i. So we see that this definition is compatible with the use of the addition symbol in the general complex number notation a + bi. Also, addition is commutative and the complex number 0 (which is shorthand for 0 + 0i) is an identity with respect to addition, i.e. 0 + (a + bi) = a + bi = (a + bi) + 0. We note that the negative of the complex number a + bi is −a − bi. Example 27: Simplify the following expressions: • (5 + 3i) − (3 + 2i). • (−3 + 2i) − (2 − 4i). • 2 − (7 − 3i). Definition 3.7. The product of two complex numbers a+bi and c+di is the complex number (a + bi) × (c + di) = (ac − bd) + (ad + bc)i. 3.2. C - THE FIELD OF COMPLEX NUMBERS, 39 Multiplication is an internal binary operation on C. This definition is compatible with the usual rules of associativity and distributivity concerning addition and multiplication. This can be illustrated as (a + bi) · (c + di) = ((a + bi) · c) + ((a + bi) · di) = (a · c) + (bi · c) + (a · di) + (bi · di) = ac + (bc)i + (ad)i + (bd) · (i 2) = ac + (ad + bc)i + (bd) · (−1) = (ac − bd) + (ad + bc)i. Example 28: Calculate the following: • (−2 + i)(3 − 2i). • 3(2 + i). • (2i)(5i). • (3 + 2i)(3 − 2i). The complex number 1 (which is shorthand for 1 + 0i) is the identity for multiplication, i.e. 1 · (a + bi) = a + bi = (a + bi) · 1. Now suppose that a + bi ∈ C we would like to see that if a + bi ̸= 0, then a + bi has an inverse. Notice that, as a + bi ̸= 0, a2 + b2 > 0. Let z = a a2+b2 − b a2+b2 i. Then (a + bi)z = (a + bi) ( a a2 + b2 − b a2 + b2 i ) = 1 a2 + b2 (a + bi)(a − bi) = a2 + b2 a2 + b2 = 1. Hence every non-zero complex number has a multiplicative inverse. Example 28 indicates there is a special relationship between a complex number a + bi and the complex number a − bi. To explore this further, we need to be able to refer to the two real numbers that appear in a complex number separately. Thus we have: Definition 3.8. The complex conjugate ¯z of a complex number z = a + bi is given by z = a − bi. The complex conjugate of a complex number has the same real part, i.e. Re(z) = Re(z) and opposite imaginary part, i.e. Im(z) = −Im(z). An alternative notation for the complex conjugate of z is z∗. Example 29: Simplify the following expressions: 40 CHAPTER 3. COMPLEX NUMBERS • (2 + 3i). • 5. • −7i. • (a + bi)(a − bi). Lemma 3.9. For z ∈ C \\ {0} a non-zero complex number, the inverse of z is given by z−1 = 1 zz z. Lemma 3.10. The quotient of two complex numbers α = a + bi and β = c + di, with β ̸= 0, is the complex number αβ−1 = α β = 1 ββ αβ. (3.2) Note that the denominator in the right-hand side of (3.2) is a non-zero real number. In detail, (3.2) states that a + bi c + di = α β = α · β β · β = (a + bi) · (c − di) (c + di) · (c − di) = (ac + bd) + (bc − ad)i c2 + d2 = ( ac + bd c2 + d2 ) + (bc − ad c2 + d2 ) i. (3.3) You should remember Lemma 3.10 as a method. Example 30: 3 + 5i 1 − 2i = (3 + 5i)(1 + 2i) (1 − 2i)(1 + 2i) = ( 3 − 10 1 + 4 ) + ( 5 + 6 1 + 4 ) i = −7 5 + 11 5 i. One can also show that addition and multiplication are associative, i.e. (α + β) + γ = α + (β + γ) = α + β + γ, (α · β) · γ = α · (β · γ) = α · β · γ. 3.2. C - THE FIELD OF COMPLEX NUMBERS, 41 Hence for z1, z2, . . . , zn ∈ C, the expressions n∑ i=1 zi and n∏ i=1 zi make sense. In particular, (C, +) and (C \\ {0}, ×) are abelian groups and, as we can also check that the distributive law holds we have Theorem 3.11. We have (C, +, ×) is a field. Example 31: 1. Calculate (2 + 3i) · (3 − 4i) 1 − 5i . 2. Find all the solutions of the equation zz − 2iz − 7 + 4i = 0. 3. By writing α = a + bi, with a, b ∈ R, find all the complex numbers α such that α2 = 3 − 4i. It is useful to take note of the following properties of z: Lemma 3.12. We have the following properties of complex conjugation: 1. (z) = z for any complex number z ∈ C. 2. If z1, z2, . . . , zn ∈ C, then (z1 + z2 + . . . + zn) = z1 + z2 + . . . + zn. 3. If z1, z2, . . . , zn ∈ C, then (z1 · z2 · . . . · zn) = z1 · z2 · . . . ·zn. 4. The complex number z is a real number if and only if z = z. 5. The complex number z is purely imaginary if and only if z = −z. 42 CHAPTER 3. COMPLEX NUMBERS 3.3 The Argand diagram The set of all real numbers R can be visualised by a number line with “no gaps”. The set of complex numbers consists of elements, a + bi, which in general depend on two real numbers, a and b. This requires a two-dimensional graphical representation. In the Argand diagram (also called the complex plane, see Figure 3.1), a complex number z = a + bi is represented by the point with coordinates (a, b). The Argand diagram is named after Jean-Robert Argand. (1768–1822) In other words, the point with x- coordinate equal to a and y-coordinate equal to b. Re(z) Im(z) a1 b1 •z1 = a1 + b1i a2 b2 •z2 = a2 + b2i −b1 •z1 = a1 − b1i Figure 3.1: An illustration of the Argand diagram which can be used to graphically represent points in z1, z1, z2 ∈ C. Here z1 = a1+b1i and z2 = a2+b2i with a1, b1, a2, b2 ∈ R. One should note the following: 1. The x-axis in the Argand diagram represents the real number line. Hence, it is called the real axis. 2. The y-axis in the Argand diagram represents the set of purely imaginary numbers, i.e, Ri. Hence it is called the imaginary axis. Since one often uses z to denote complex numbers, you might also find that the Argand diagram is referred to as the z-plane. Example 32: Plot the following complex numbers on the Argand diagram: 3 + i, 5, −2i, 4 − 2i, −2 + 3i, −1 − 2i. 3.4. MODULUS-ARGUMENT FORM (POLAR FORM) 43 3.4 Modulus-argument form (polar form) We in addition to the standard Cartesian co-ordinates, every point in the plane can also be identified by polar coordinates(r, θ) rather than Cartesian coordinates (x, y). This can be applied to the Argand diagram as well, with the zero on the real axis acting as the origin O and the positive real axis as the initial ray. We set x = r cos(θ) and y = r sin(θ), so that 1 z = x + yi = r cos(θ) + r sin(θ)i = r (cos(θ) + i sin(θ)) , where by basic trigonometry r = √x2 + y2 and tan (θ) = y x. Re(z) Im(z) r1 •z1 = r1(cos (θ1) + i sin (θ1)) θ1 x1 y1 r2 •z2 = r2(cos (θ2) + i sin (θ2)) θ2 x2 y2 Figure 3.2: An illustration of the modulus and argument of z1, z2 ∈ C. Here z1 = x1 + y1i and z2 = x2 + y2i with x1, y1, x2, y2 ∈ R, |z1| = r1, |z2| = r2 and θ1 ∈ arg(z1) and θ2 ∈ arg(z2). Notice that complex numbers of the form cos(θ) + i sin(θ) lie on the unit circle in the Argand diagram. When applied to the representation of complex numbers, we use the following terminology: Definition 3.13. The modulus of a complex number z = x + yi, denoted |z|, is given by |z| = √x2 + y2. 1Do not confuse the complex number i with the vector i here. 44 CHAPTER 3. COMPLEX NUMBERS It is clear that |z| ≥ 0 for all z ∈ C and |z| = 0 if and only if z = 0. The modulus is sometimes referred to as the absolute value of z. Also note that |z| = √x2 + y2 = √ z · z for z = x + yi ∈ C. Definition 3.14. The values of θ ∈ R for which x = |z| cos(θ) and y = |z| sin(θ), are called arguments of the complex number z = x + yi, where x and y are not both equal to 0. The set of all arguments of z is denoted by arg(z). Complex numbers have infinitely many ‘arguments’. If θ ∈ arg(z), then arg(z) = {θ + k2π : k ∈ Z}. This multiplicity of possibilities can be made unique as follows: Definition 3.15. The principal value of arg(z), denoted Arg(z), is the member of arg(z) such that Arg(z) ∈ (−π, π]. This means that {Arg(z)} = arg(z) ∩ (−π, π]. Every complex number z ∈ C \\ {0}, has a unique principal value of its argument. In practice, to determine the principal value of a complex number, one typically starts by solving tan(θ) = y x , (3.4) which yields two possible values of θ in the interval (−π, π], a value of π apart if x ̸= 0 (if x = 0 and y ̸= 0, then θ is either π 2 or − π 2 ). We have learned to restrict the tangent function tan : (− π 2 , π 2 ) → R in order to define its inverse, so the solution we obtain analytically should be in this interval. The value of θ obtained from solving (3.4) needs to be checked against the position in the Argand diagram, and, where necessary, discarded and replaced by the value θ + π or θ − π. It is good practice to plot the complex number in the Argand diagram to check whether the calculated value of θ is the principal value of the argument or not. Example 33: Calculate the modulus, |z|, and principal value of the argument, Arg(z) for: • z = 2 + i. 3.5. PRODUCT AND QUOTIENT USING MODULUS-ARGUMENT FORM 45 • z = 2 − i. • z = −2 + i. • z = −2 − i. From Definition 3.15, it follows that z1, z2 ∈ C\\{0} satisfy z1 = z2 if and only if |z1| = |z2| and Arg(z1) = Arg(z2). 3.5 Product and Quotient using modulus-argument form Proposition 3.16. Given two non-zero complex numbers z1 = r1(cos(θ1)+i sin(θ1)) and z2 = r2(cos(θ2) + i sin(θ2)), i.e. r1 = |z1|, r2 = |z2|, θ1 is a value in arg(z1) and θ2 is a value in arg(z2). Then, (i) |z1 · z2| = |z1||z2|, (ii) θ1 + θ2 is a value in arg(z1 · z2). Proof: We use the definition and trigonometric formulae to calculate z1 · z2 = r1 · r2 · (cos(θ1) + i sin(θ1)) · (cos(θ2) + i sin(θ2)) = r1r2 [(cos(θ1) cos(θ2) − sin(θ1) sin(θ2)) + i(cos(θ1) sin(θ2) + sin(θ1) cos(θ2))] = r1r2(cos(θ1 + θ2) + i sin(θ1 + θ2)). So |z1 · z2| = r1r2 = |z1||z2| and a value of the argument is given by θ1 + θ2, as required. Note that even when θ1 and θ2 are the principal values of the argument of z1 and z2 respectively, θ1 + θ2 is not necessarily the principal value of the argument of z1 · z2, since θ1 + θ2 is not necessarily in the interval (−π, π]. Example 34: For z1 = 2 (cos ( π 3 ) + i sin ( π 3 )) and z2 = 3 (cos ( 3π 4 ) + i sin ( 3π 4 )) calculate |z1 · z2| and Arg(z1 · z2). 46 CHAPTER 3. COMPLEX NUMBERS Proposition 3.17. Given two non-zero complex numbers z1 = r1(cos(θ1)+i sin(θ1)) and z2 = r2(cos(θ2) + i sin(θ2)) ̸= 0. Then, (i) 1 z2 = z2−1 = 1 r2 (cos(θ2) − i sin(θ2)) = 1 r2 (cos(−θ2) + i sin(−θ2)); (ii) ∣ ∣ ∣ ∣ z1 z2 ∣ ∣ ∣ ∣ = |z1| |z2| , (iii) θ1 − θ2 is a value of arg ( z1 z2 ) . Proof: Part (i) follows from Lemma 3.9. To understand part (ii) and (iii), we use Proposition 3.16 and calculate z1z−1 2 = r1(cos(θ1) + i sin(θ1)) 1 r2 (cos(−θ2) + i sin(−θ2)) = r1 r2 (cos(θ1 − θ2) + i sin(θ1 − θ2)). Therefore, ∣ ∣ ∣ ∣z1 z2 ∣ ∣ ∣ ∣ = |z1z−1 2 | = r1 r2 and θ1 − θ2 ∈ arg ( z1 z2 ) , as required. Similarly when θ1 and θ2 are the principal values of the argument of z1 and z2 respectively, θ1 − θ2 is not necessarily the principal value of the argument of z1z−1 2 . Example 35: For, z1 = 2 (cos ( π 3 ) + i sin ( π 3 )) and z2 = 3 ( cos ( −3π 4 ) + i sin (−3π 4 )) , calculate |z1/z2| and Arg(z1/z2). Corollary 3.18. Given non-zero complex numbers of the form zj = rj(cos(θj) + i sin(θj)) for j = 1, . . . , n then z1 · z2 · . . . · zn =   n∏ j=1 rj   (cos(θ1 + θ2 + . . . + θn) + i sin(θ1 + θ2 + . . . + θn)). Proof: Use mathematical induction and Proposition 3.16 to establish the result. In particular, the product of two or more complex numbers with modulus one also has modulus one. The special case of taking the n-th power of a complex number leads to the formulation of De Moivre’s Theorem. Example 36: Let S = {z ∈ C : |z| = 1}. Show that (S, ×) is a group. 3.6. DE MOIVRE’S THEOREM 47 3.6 de Moivre’s Theorem Theorem 3.19. (de Moivre) If n ∈ N and θ ∈ R, then (cos(θ) + i sin(θ)) n = cos(nθ) + i sin(nθ). Proof: Apply Corollary 3.18. This was discovered by Abraham de Moivre (1667, 1754). Example 37: Via de Moivre’s Theorem calculate 1 n, i 2, i 3, (−1) 4 and (−i) 3. Corollary 3.20. If n ∈ N and θ ∈ R, then (cos(θ) + i sin(θ)) −n = cos(nθ) − i sin(nθ) = cos(−nθ)i + sin(−nθ). Proof: Use Proposition 3.17 and de Moivre’s Theorem. 3.7 Euler’s formula If we consider the modulus-argument notation for a complex number with modulus equal to one, together with the MacLaurin series for cos and sin, we discover a peculiar formula. Since the following series are absolutely convergent for all θ ∈ R (see [15]), cos(θ) = 1 − θ2 2! + θ4 4! − θ6 6! + · · · sin(θ) = θ − θ3 3! + θ5 5! − θ7 7! + · · · , one finds that cos(θ) + i sin(θ) = ( 1 − θ2 2! + θ4 4! − θ6 6! + · · · ) + i ( θ − θ3 3! + θ5 5! − θ7 7! + · · · ) , = 1 + (iθ) + (iθ) 2 2! + (iθ) 3 3! + (iθ)4 4! + (iθ) 5 5! + (iθ) 6 6! + (iθ) 7 7! + . . . = e iθ, (3.5) where we recall that the Maclaurin series for e x is given by (at least for x ∈ R) 2 ex = 1 0! + x 1! + x2 2! + x3 3! + · · · . 2Laurent series allows one to expand smooth complex valued functions in a similar way to Tay- lor/Maclaurin expansions for real functions of 1 variable. For details see [13, p.195] 48 CHAPTER 3. COMPLEX NUMBERS So it appears that we can use eiθ as a short notation for cos(θ) + i sin(θ). But does this use of the exponential function satisfy useful properties we associate with ex? A crucial property of the exponential function is the index law, i.e. for x, y ∈ R, ex · ey = e x+y. (3.6) Now, observe that via Corollary 3.18 and (3.5), we have e iθ1 · eiθ2 = (cos(θ1) + i sin(θ1)) · (cos(θ2) + i sin(θ2)) = cos(θ1 + θ2) + i sin(θ1 + θ2) = ei(θ1+θ2) = e iθ1+iθ2. (3.7) So we can indeed use eiθ in the same way as we use the exponential function with respect to the index law in (3.6). The formula eiθ = cos(θ) + i sin(θ), is known as Euler’s formula3. Using Euler’s formula, de Moivre’s Theorem can be written as (eiθ)n = e inθ for all θ ∈ R and n ∈ N. Similarly, (e −iθ)n = e −inθ = (eiθ)−n for all θ ∈ R and n ∈ N. In particular, we have the famous expression eiπ = −1. Hence, we can now write a complex number in three different forms: z = a + bi = r (cos(θ) + i sin(θ)) = reiθ, with r2 = a2 + b2 and θ = arg(z). These forms for a complex number z are respectively referred to as: the algebraic form, the modulus-argument form, and exponential form. When doing calculations with complex numbers, one should choose the most suitable form/notation to increase efficiency and simplification of associated operations. Example 38: Write the following complex numbers in modulus-argument form and in exponential form: 1. z = √ 2 2 + √ 2 2 i. 2. z = √ 3 2 − 1 2 i. 3Named after the Swiss mathematician Leonhard Euler [11]. 3.8. DE MOIVRE’S THEOREM AND TRIGONOMETRIC FORMULAE 49 3.8 De Moivre’s Theorem and trigonometric formu- lae If we combine de Moivre’s Theorem with the Binomial Theorem in the form (a + b)n = a n + nan−1b + n(n − 1) 2! an−2b2 + · · · + n(n − 1) . . . 2 (n − 1)! abn−1 + n(n − 1) . . . 2.1 n! bn = ( n 0 ) an + ( n 1 ) an−1b + ( n 2 ) an−2b2 + · · · + ( n n − 1 ) abn−1 + ( n n ) bn = n∑ k=0 ( n k ) an−kbk then we can construct trigonometric identities which include cos(nθ) and sin(nθ) terms. For example, De Moivre’s Theorem with n = 2 states that (cos(θ) + i sin(θ)) 2 = cos(2θ) + i sin(2θ). (3.8) The Binomial Theorem, which can also be applied to complex quantities, yields (cos(θ) + i sin(θ)) 2 = (cos 2(θ) − sin2(θ)) + 2 cos(θ) sin(θ)i. (3.9) Hence, by equating the real and imaginary parts (3.8) and (3.9) we obtain the familiar identities cos(2θ) = cos 2(θ) − sin2(θ), sin(2θ) = 2 cos(θ) sin(θ). Example 39: Establish that cos(3θ) = 4 cos3(θ) − 3 cos(θ). 3.9 Euler’s formula and trigonometric formulae Euler’s formula allows us to deduce various trigonometric formulae that are more difficult to deduce otherwise. Consider Euler’s formula for θ and −θ: eiθ = cos(θ) + i sin(θ), (3.10) e −iθ = cos(θ) + i sin(−θ) = cos(θ) − i sin(θ). (3.11) Equations (3.10) and (3.11) yield the relation between exponential and trigonometric functions, e iθ + e −iθ = (cos(θ) + i sin(θ)) + (cos(θ) − i sin(θ)) 50 CHAPTER 3. COMPLEX NUMBERS = 2 cos(θ) which allows us to write cos(θ) as cos(θ) = 1 2 (eiθ + e −iθ) ∀θ ∈ R. Similarly, eiθ − e−iθ = (cos(θ) + i sin(θ)) − (cos(θ) − i sin(θ)) = 2i sin(θ) so that sin(θ) = 1 2i ( eiθ − e −iθ) ∀θ ∈ R. Similarly, one can find formulae for cos(nθ) and sin(nθ). For any n ∈ N, via de Moivre’s Theorem, e inθ = cos(nθ) + i sin(nθ), e−inθ = cos(nθ) − i sin(nθ), and hence, einθ + e −inθ = 2 cos(nθ) =⇒ cos(nθ) = 1 2 (e inθ + e −inθ) ∀θ ∈ R. (3.12) Moreover, einθ − e −inθ = 2i sin(nθ) =⇒ sin(nθ) = 1 2i (e inθ − e−inθ) ∀θ ∈ R. (3.13) Equations (3.12) and (3.13) allow us to rewrite powers of cos (θ) and sin (θ) in terms of linear combinations of cos (nθ) and sin (nθ) for n ∈ N. Example 40: cos 3(θ) = ( 1 2 (e iθ + e−iθ))3 = 1 8 (ei3θ + 3e i2θe−iθ + 3e iθe−i2θ + e−i3θ) = 1 8 ((e i3θ + e −i3θ) + 3 ( eiθ + e−iθ)) = 1 8 (2 cos(3θ) + 3 · 2 cos(θ)) = 1 4 cos(3θ) + 3 4 cos(θ). Example 41: Write sin3(θ) in terms of cosines or sines of multiples of θ: 3.10. n-TH ROOTS OF COMPLEX NUMBERS 51 3.10 n-th roots of complex numbers The n-th root of a complex number is defined similarly to that of a real number. Definition 3.21. Suppose that n ≥ 1 is a natural number. A complex number w is an n-th root of a non-zero complex number z, denoted by w = z1/n if wn = z. We say that w is a n-th root of unity if w is a n-th root of 1. For example, i2 = −1 and (−i)2 = −1, so i and −i are square roots (or 2-th roots) of −1. Since 1 4 = i4 = (−1) 4 = (−i)4 = 1, 1, −1, i and −i are all 4-th roots of 1. To find the n-th root in general, we make use of the exponential notation of a complex number. We express z as z = reiθ and determine the n-th roots w in their exponential form w = ρeiϕ. Remember that by definition, r > 0 and ρ > 0 provided that z ̸= 0 (n-th roots of 0 are easy to find ... they consist of ... just 0). According to Definition 3.21, w is then an n-th root of z if and only if z = wn or equivalently reiθ = (ρeiϕ))n = ρneinϕ, by the Euler’s version of de Moivre’s Theorem. The modulus of a complex number is unique, hence we have that r = ρn or ρ = r1/n, i.e. the positive, real n-th root of r. Also, both θ and nϕ are values of the argument of z, or equivalently nϕ = θ + 2kπ, (3.14) for some k ∈ Z. Equation (3.14) can be solved for the unknown ϕ as, ϕ = θ n + k 2π n . (3.15) This yields n distinct values for ϕ in the interval (−π, π], and hence n distinct n-th roots of z, given by = r1/ne i( θ+k2π n ) with k = 0, 1, 2, . . . n − 1 (or any other consecutive sequence of n integer numbers). Note that: • The n-th roots of z = reiθ ̸= 0 have the same modulus, hence they are all on a circle in the Argand diagram with the origin at the centre and radius r1/n. 52 CHAPTER 3. COMPLEX NUMBERS • Unlike with the real n-th roots of real numbers, every z ∈ C \\ {0} has exactly n distinct n-th roots. • When plotted on the Argand diagram, these n n-th roots are equally spaced around the circle. The arguments of two neighbouring roots differ by 2π/n. • As a consequence, the n-th roots of z are the vertices of a regular polygon of n sides inscribed in the circle with centre at the origin and radius r1/n. Example 42: Find all the fifth roots of z = 16 √ 3 + 16i. 3.11 Polynomials The discussion following (3.15) implies that a polynomial equation of the form zn = a with a ∈ C \\ {0} has exactly n distinct solutions. Can the same be said about solutions of general polynomial equations? Definition 3.22. 1. A polynomial of degree n in the complex variable z is an expression of the form p(z) = anzn + an−1zn−1 + . . . + a1z + a0 = n∑ i=0 aizi, where ai ∈ C for i = 0, 1, . . . , n and an ̸= 0. 2. For p(z) a degree n polynomial, a polynomial equation is an equation of the form p(z) = 0. 3. For p(z) a degree n polynomial, a complex number α is called a zero (or a root) of the polynomial p(z) if p(α) = 0. That is, α is a solution to the polynomial equation p(z) = 0. Example 43: The expression 5z4 − 3z2 + 1 is a polynomial of degree 4 with coefficients a4 = 5, a3 = 0, a2 = −3, a1 = 0 and a0 = 1. The equation 5z4 − 3z2 + 1 = 0 is a polynomial equation of degree 4 3.12. QUADRATIC EQUATIONS 53 Theorem 3.23. The complex number α is a root of the polynomial equation p(z) = 0, if and only if z − α is a factor of the polynomial p(z), i.e. p(z) = (z − α)q(z), where q(z) is a polynomial of degree n − 1. Proof: If z − α is a factor of p(z), i.e. p(z) = (z − α)q(z), with q(z) a polynomial of degree n − 1, then it is clear that p(α) = (α − α)q(α) = 0. Hence, α is a root of p(z) = 0. This proves one part of the theorem. We still need to prove that if α is a root of p(z) then z − α is a factor of p(z). Consider calculating p(z)/(z − α) by polynomial division. Then p(z) can be written as p(z) = (z − α)q(z) + R, (3.16) where q(z) is a polynomial of degree n − 1 and the remainder term, denoted by R, is a polynomial of degree less then the degree of z − α, i.e., R is a constant. If α is a root of p(z) = 0 then p(α) = 0 and, consequently via (3.16) (α − α)q(α) + R = 0 ⇐⇒ R = 0. Therefore, p(z) = (z − α)q(z) or equivalently, z − α is a factor of p(z), as required. 3.12 Quadratic equations We can now take a fresh look at the familiar quadratic equation, az2+bz+c with a, b, c ∈ C (a ̸= 0). Zeros of this quadratic equation can be found by completing the square: az2 + bz + c = 0 ⇐⇒ a ( z2 + b az + c a ) = 0 ⇐⇒ a  z2 + b az + ( b 2a )2 − ( b 2a )2 + c a   = 0 ⇐⇒ a   ( z + b 2a )2 − b2 4a2 + c a   = 0 ⇐⇒ a ( z + b 2a )2 = b2 4a − c ⇐⇒ a ( z + b 2a )2 = b2 − 4ac 4a . (3.17) 54 CHAPTER 3. COMPLEX NUMBERS So we calculate ( z + b 2a )2 = b2 − 4ac 4a2 ⇐⇒ z + b 2a = ( b2 − 4ac 4a2 )1/2 ⇐⇒ z = − b 2a + ( b2 − 4ac 4a2 )1/2 . (3.18) Theorem 3.24. Suppose that p(z) = az2 + bz + c with a, b, c ∈ C and a ̸= 0. Let w ∈ C be such that w2 = b2 − 4ac 4a2 . Then counting multiplicities p(z) has two roots z1,2 = − b 2a ± w and we can factor p(z) = az2 + bz + c = a(z − z1)(z − z2). 3.13 The Fundamental Theorem of Algebra What we have seen with polynomials of degree 2 with complex number coefficients gen- eralizes to polynomials of arbitrary degree. Theorem 3.25. (Fundamental Theorem of Algebra) Suppose that p(z) is a polynomial of degree n with n ≥ 1. Then the polynomial equation p(z) = 0 has at least one root in C. Corollary 3.26. Suppose that p(z) is a polynomial of degree n with n ≥ 1. Then p(z) is a product of polynomials of degree 1 which have complex factors. There are various proofs of Theorem 3.25 (see, for instance, [13, p.153-154]) all of which require methods outside the scope of this course, and most of which, require results concerning analysis of functions of a complex variable. In a more sophisticated language the fundamental theorem states that the field of complex numbers in algebraically closed (there are no missing roots of polynomial equations). 3.14. POLYNOMIALS WITH REAL COEFFICIENTS 55 3.14 Polynomials with real coefficients Suppose that p(z) = anzn + an−1zn−1 + . . . + a1z + a0 = n∑ i=0 aizi, has degree n and ai ∈ R for i = 0, 1, . . . , n. Then we say that the polynomial p(z) has real coefficients. In the case of quadratic equations with real coefficients p(z) = az2 + bz + c, a, b, c ∈ R, a ̸= 0, Theorem 3.24 gives roots z1,2 = −b ± √b2 − 4ac 2a . We define D = b2 − 4ac and call D the discriminant. There are three possibilities, D > 0, D = 0 and D < 0. In the case that D > 0, p(z) can be factorised into two linear factors with real coefficients, az2 + bz + c = a(z − z1)(z − z2). When D = 0, there is 1 (repeated) zero, z1 = − b 2a , and the quadratic can be factorised as az2 + bz + c = a(z − z1) 2 = a(z − z1)(z − z1). (3.19) When D < 0, we can define a real variable ν as b2 − 4ac = −ν2, and we have z1,2 = −b ± iν 2a and in particular we observe the famous fact that the roots are complex conjugates of each other. That is z1 = z2. The quadratic polynomial cannot be factorised in two real linear factors, and so we define it to be a real irreducible quadratic. One can write the quadratic using complex linear factors as az2 + bz + c = a(z − z1)(z − z2). Hence, 56 CHAPTER 3. COMPLEX NUMBERS Theorem 3.27. A quadratic polynomial with real coefficients has either two distinct real zeros (positive discriminant), two identical real zeros (discriminant zero) or a pair of complex conjugate non-real complex zeros (negative discriminant). So when looking for zeros in the set of complex numbers, C, the quadratic (or polynomial of degree 2) always has 2 zeros (that are not necessarily distinct). The fact that complex conjugation permutes the roots of a quadratic equation (fixing every real root and swapping the complex roots) leads to the property is given in the following: Theorem 3.28. Suppose that p(z) is a degree n polynomial with real coefficients. Then α ∈ C is a zero of p(z) if and only if the complex conjugate α is a zero of p(z). In particular, non-real zeros of p(z) occur in conjugate pairs. Proof: We have seen the following identities: (z1 + z2 + . . . + zn) = z1 + z2 + . . . + zn, (3.20) (z1 · z2 · . . . · zn) = z1 · z2 · . . . · zn. (3.21) It follows from (3.21) that zn = zn and azn = azn for a ∈ R and n ∈ N. Hence, (p(z)) = (anzn + an−1zn−1 + . . . + a1z + a0) = an (z) n + an−1 (z) n−1 + . . . + a1z + a0 (via (3.20) and (3.21)) = p (z) . (3.22) Therefore, (3.22) yields p(z) = 0 ⇐⇒ (p(z)) = 0 ⇐⇒ p (z) = 0. Hence p(a + bi) = 0 ⇐⇒ p(a − bi) = 0, as required. Please note that the above result does not hold in general for polynomial equations of degree n with complex coefficients. Corollary 3.29. A polynomial p(z) with real coefficients of odd degree n has at least one real zero. 3.14. POLYNOMIALS WITH REAL COEFFICIENTS 57 Proof:4 Suppose that p(z) has only complex roots. Via Theorem 3.28, every root occurs in a complex conjugate pair i.e. n = 2m for some m ∈ N. This implies n is even which is a contradiction. We conclude that p(z) has at least one real root, as required. Corollary 3.30. Suppose that p(z) is a polynomial of degree n with real coefficients. The complex number α = a+bi is a non-real root of the polynomial equation p(z) = 0 if and only if the quadratic polynomial z2 − 2az + a2 + b2 is a factor of p(z). Proof: Let us write α = a + ib. Then p(z) = (z − α)q(z) by Theorem 3.23. Now Theorem 3.28 implies 0 = p(α) = (α − α)q(α) so, as α ̸= α because α is not real, we must have q(α) = 0. Hence Theorem 3.23 yields q(z) = (z − α)r(z). Hence p(z) = (z − α)(z − α)r(z) = (z2 − 2az + a2 + b2)r(z) and this proves the claim. The quadratic polynomial z2 − 2az + a2 + b2 cannot be expressed as a product of two real linear factors and is therefore called a real irreducible quadratic. Example 44: Given that 2 + 7i is a root of the polynomial equation p(z) = 0 where p(z) = z4 − 4z3 + 55z2 − 8z + 106, find the remaining roots. We are now a number of properties of polynomials of degree n with real coefficients. We will state them here without proof. The first one follows from the fundamental theorem of algebra. It demonstrates the deficiency of the real numbers. Theorem 3.31. A polynomial p(z) with real coefficients can always be expressed as a product of real linear and real irreducible quadratic factors. Theorem 3.32. A polynomial p(z) of degree n with real coefficients has exactly n zeros, some of which may be repeated, and some of which may be complex conjugate pairs (the complex root and its complex conjugate). 4Consider for yourself how Corollary 3.29 can be established using properties of real valued functions (polynomials) p : R → R considered in [15]. 58 CHAPTER 3. COMPLEX NUMBERS Note that zeros whose corresponding factor in the factorisation of the polynomial appears with a power µ (see, for example, (3.19) with µ = 2) are counted as µ zeros. We say that such a zero has algebraic multiplicity µ. Specifically, if p(z) is a polynomial of degree n ≥ 1, then p(z) has r distinct roots zi (with 1 ≤ r ≤ n) which have respective algebraic multiplicity mi, for i = 1, . . . , r, and moreover, n = r∑ i=1 mi. Example 45: The degree 8 polynomial p(z) = (z − 1) 2(z − 2) 4(z2 + 1) = z8 − 10z7 + 42z6 − 98z5 + 145z4 − 152z3 + 120z2 − 64z + 16. has distinct roots at 1, 2, i and −i with multiplicities 2, 4, 1 and 1 respectively. Note that although there are only 2 real roots and a pair of complex conjugate roots, the sum of the multiplicities of the roots equals 8 (the order of the polynomial). Alternatively, in terms of Theorem 3.32 we would say that p(z) has roots at 1, 1, 2, 2, 2, 2, i and −i. Example 46: For each of the following polynomials, find all roots and hence factorise them in to real linear factors and real irreducible quadratic factors. 1. p(z) = 2z3 − 12z2 + 22z − 12. 2. p(z) = z4 − 4. 3.15 Basic inequalities in C Theorem 3.33. Solutions of the equation |z| = α, (3.23) where α ∈ R+, are represented in the Argand diagram by the points on a circle with centre at z = 0 and radius α. Proof: Because both sides of equation (3.23) are positive real numbers, we can square each side: |z| = α ⇐⇒ |z| 2 = α2 ⇐⇒ z · z = α2. If we substitute z = x + iy into this equation, we get |z| = α ⇐⇒ (x + iy)(x − iy) = α2 3.15. BASIC INEQUALITIES IN C 59 ⇐⇒ x2 + y2 = α2, which represents a circle in the Argand diagram with centre at z = 0 and radius α, as required. Corollary 3.34. Solutions of the inequality |z| < α, where α ∈ R +, are represented in the Argand diagram by the points inside a circle with centre at z = 0 and radius α. Corollary 3.35. Solutions of the inequality |z| > α, where α ∈ R +, are represented in the Argand diagram by the points outside a circle with centre at z = 0 and radius α. The proofs of Corollary 3.34 and Corollary 3.35 follow the same steps as the proof of Theorem 3.33 with appropriate inequalities replacing equalities. Example 47: Graphically represent (sketch) the set of solutions of: 1. |z| ≤ 3. 2. |z| > 2. Theorem 3.36. The modulus |α − β| , where α, β ∈ C, represents the length of the line segment in the Argand diagram connecting the points representing the complex numbers α and β. Proof: If we write α = a + ib and β = c + id, then it follows that |α − β| = |(a + ib) − (c − id)| = |(a − c) − i(b − d)| = √(a − c) 2 + (c − d) 2. In the Argand diagram, this represents the length of the line segment connecting the points with coordinates a + ib and c + id, i.e. the points representing α, β ∈ C, as required. 60 CHAPTER 3. COMPLEX NUMBERS Corollary 3.37. Solutions of the equation |z − z0| = α, where z0 ∈ C and α ∈ R +, are represented in the Argand diagram by the points on a circle with centre at z0 and radius α. Corollary 3.38. Solutions of the inequality |z − z0| < α, where z0 ∈ C and α ∈ R +, are represented in the Argand diagram by the points inside a circle with centre at z0 and radius α. Corollary 3.39. Solutions of the inequality |z − z0| > α, where z0 ∈ C and α ∈ R +, are represented in the Argand diagram by the points outside a circle with centre at z0 and radius α. Example 48: Graphically represent (sketch) the set of solutions of: 1. |z − 1| < 3. 2. |z + 1 − 2i| ≥ 2. Chapter 4 Linear Equations and Matrices ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • add and multiply matrices; • use matrices to solve systems of linear equations; • prove basic results about matrices. [2, p.602-612] and [4, Chapters SLE and M] contain alternative presentations of material in this chapter that you may find helpful. It is likely that you have seen the equation of a line in R2 (in Cartesian form), specifically, a1x1 + a2x2 = b, (4.1) with constants a1, a2, b ∈ R such that a1 and a2 are not both 0. A line is a 1-dimensional set (intuitively, this can be seen since every point on the line can be described by varying one parameter, say x1 (since x2 is then determined from x1). Similarly,in Chapter 1 we say that given a vector n = (a1, a2, a3) and a real number b ∈ R, then a plane in R 3 is given by the vector equation n · r = b. Therefore in R 3, the equation a1x1 + a2x2 + a3x3 = b (4.2) with constants a1, a2, a3, b ∈ R such that a1, a2 and a3 are not all equal to 0, defines a plane (a 2-dimensional set) in R3. Again, intuitively, this can be seen since every point on the plane can be described by varying two parameters, say x1 and x2 (since x3 is then determined from x1 and x2). Example 49: 61 62 CHAPTER 4. LINEAR EQUATIONS AND MATRICES 1. In R 2, the equation x1 = 0 defines the x2-axis. 2. In R 3, the equation x3 = 1 defines the plane parallel to the x1x2-plane and passing through the point (x1, x2, x3) = (0, 0, 1) as we saw in Chapter 1. 3. In general, in Rn for some n ∈ N (n ≥ 4), the equation a1x1 + a2x2 + · · · + anxn = b (4.3) with constants ai ∈ R for i = 1 . . . n, such that ai are not all equal to 0, defines a (n − 1)-dimensional hyperplane in Rn. 4.1 Simultaneous linear equations The equation, as given in (4.3) is known as a linear equation in the unknowns x1, x2, . . . , xn. Moreover, the real constants ai in (4.3) are referred to as the coefficients of xi (for each i = 1, . . . , n) and b is also a real number. For instance, we would say, “the coefficient of x2 is a2\". Moreover, xi for i = 1, . . . , n are referred to as real unknowns. We can also define linear equations where the unknowns, the coefficients and the candidates for b are rational numbers, complex numbers and more generally members of any field F. For the development in this chapter, we will consider real linear equations. Definition 4.1. A system of simultaneous linear equations in the unknowns x1, x2, . . . , xn is: a11x1 + a12x2 + a13x3 + · · · + a1nxn = b1 a21x1 + a22x2 + a23x3 + · · · + a2nxn = b2 ... ... ... ... ... ... ... ... ... ... am1x1 + am2x2 + am3x3 + · · · + amnxn = bm with bi, aij ∈ R for i, j ∈ Z with 1 ≤ i ≤ m and 1 ≤ j ≤ n. Remark 4.2. The notation a11, amn etc. is traditional. Here a11 represents the coefficient of x1 is the first equation. Similarly, amn is the coefficient of xn in the m-th equation. But what is a111? It is ambiguous! A more precise notation is a1,1 or am,n. Then we would write either a11,1 or a1,11 and we would have no possible confusion. Notice also that the notation for coefficients follows the rule arow,column. 4.1. SIMULTANEOUS LINEAR EQUATIONS 63 Example 50: The system of simultaneous linear equations given by: x1 + 2x2 + 3x3 + 4x4 = 10 x1 − x2 + x3 − x4 = 0 has m = 2 and n = 4, i.e. there are 2 equations in 4 unknowns. Example 51: The system of simultaneous linear equations given by: x1 + x2 = 1 x1 + 2x2 = 3 2x1 + 2x2 = 1 x1 − x2 = 0 has m = 4 and n = 2, i.e. there are 4 equations in 2 unknowns. Here, a11 = 1, a12 = 1, a21 = 1, a22 = 2, a31 = 2, a32 = 2, a41 = 1 and a42 = −1, with b1 = 1, b2 = 3, b3 = 1 and b4 = 0. Definition 4.3. The system of simultaneous linear equations: a11x1 + a12x2 + a13x3 + · · · + a1nxn = b1 a21x1 + a22x2 + a23x3 + · · · + a2nxn = b2 ... ... ... ... ... ... ... ... ... ... ... am1x1 + am2x2 + am3x3 + · · · + amnxn = bm has a solution (x1, x2, . . . , xn) = (c1, c2, . . . , cn) whenever the values xi = ci for i = 1, . . . , n simultaneously satisfy all m equations above. Definition 4.4. The system of simultaneous linear equations: a11x1 + a12x2 + a13x3 + · · · + a1nxn = 0 a21x1 + a22x2 + a23x3 + · · · + a2nxn = 0 ... ... ... ... ... ... ... ... ... ... ... am1x1 + am2x2 + am3x3 + · · · + amnxn = 0 is called homogeneous. If a system of simultaneous linear equations is not homo- geneous, then it’s called non-homogeneous. 64 CHAPTER 4. LINEAR EQUATIONS AND MATRICES Remark 4.5. Homogeneous systems always have at least one solution called “the trivial solution\", given by x1 = x2 = · · · = xn = 0. Indeed, recalling Chapter 1 we know that if there are 3 unknowns, then each linear equation defines a plane which contains the origin (0, 0, 0) and so this is a simulta- neous solution when there are 3 unknowns. We will show that for any such system of simultaneous linear equations, either: • there exist no solutions; • there exists a unique solution; or • there exist infinitely many solutions. 4.2 Introduction to Matrices The essential information in a system of simultaneous linear equations discussed previously is contained in the coefficients, which are naturally displayed as an array of numbers, for example,       a11 a12 a13 · · · a1n a21 a22 a23 · · · a2n ... ... ... ... am1 am2 am3 · · · amn       =       a11 a12 a13 · · · a1n a21 a22 a23 · · · a2n ... ... ... ... am1 am2 am3 · · · amn       . (4.4) Such an array is known as a matrix. Example 52: Both ( 0 0 0 0 ) and [1 2 3 4 ] are matrices. Round brackets or square brackets can be used for matrices. The shape or dimension of a matrix is measured by: • the number of rows, denoted by m; and • the number of columns, denoted by n. In (4.4), we would state that “we have an m × n matrix\", or, “the dimension of the matrix is m × n\". Remember that rows come first. 4.3. MATRIX ADDITION 65 Definition 4.6. Given m, n ∈ N, we denote the set of all m × n matrices by Mmn(R). 1 Example 53: [1 2 ] is a 2 × 1 matrix. (5 6 7 8 ) is a 1 × 4 matrix. When the shape/dimension of a matrix A is understood from context, we often write A = [aij] where aij is understood to be the entry in the i-th row and the j-th column of the matrix. Example 54: What is the (2, 3) entry in A = [aij] given that aij = i + j? Answer: The top left corner of the matrix is given as,           2 3 4 5 · · · 3 4 5 6 · · · 4 5 6 7 · · · 5 6 7 8 · · · ... ... ... ... . . .           The answer is of course a2,3 = 5. Note again that when m and n are small (less than 10), we often write a23 to denote the (2, 3) entry of a matrix, whereas, if m or n are large (larger than 10), then we separate the subscripts with a comma, i.e a11,3. Definition 4.7. Two matrices A and B are equal if and only if: (i) Both have the same dimension - say m × n. (ii) aij = bij for all i, j ∈ Z with 1 ≤ i ≤ m and 1 ≤ j ≤ n. 4.3 Matrix Addition Matrix addition is an internal binary operation on Mmn(R). 1More generally, we define the set Mmn(X ) to be the set of m × n matrices with entries in the set X . In this course, we will primarily consider X = R, but in later courses the sets of matrices Mmn(C) will be very important. 66 CHAPTER 4. LINEAR EQUATIONS AND MATRICES Definition 4.8. Given two m × n matrices A = [aij] ∈ Mmn(R) and B = [bij] ∈ Mmn(R), we define A + B ∈ Mmn(R) to be the m × n matrix, given by: A + B = [aij + bij]. I.e. A + B is the matrix where corresponding entries in A and B are added together, [ 1 2 3 4 ] + [−1 −2 −3 −4 ] = [0 0 0 0 ] . Definition 4.9. 1. The matrix 0m×n ∈ Mmn(R) denotes the m × n matrix with all entries equal to zero. 2. Given an m × n matrix A = [aij] ∈ Mmn(R), the negative of A, −A ∈ Mmn(R), is the m × n matrix with entries [−aij]. I.e. 02×3 = [ 0 0 0 0 0 0 ] . It also follows that A + (−A) = (−A) + A = 0m×n. In future we shall use A − B for A + (−B). 4.4 Properties of Matrix Addition The addition operations on two m × n matrices satisfies a number of key properties: 1. The set Mmn(R) is closed under addition, or, in other words, addition is an inter- nal binary operation in the set Mm×n(R): ∀A, B ∈ Mmn(R), A + B ∈ Mmn(R). 2. Matrix addition is associative in Mmn(R): for all A, B, C ∈ Mmn(R), (A + B) + C = A + (B + C). 3. The matrix 0m×n is an identity in Mmn(R): for all A ∈ Mmn(R), A + 0m×n = 0m×n + A = A. 4.5. SCALAR MULTIPLE OF A MATRIX 67 4. Given A ∈ Mmn(R), −A ∈ Mmn(R) satisfies A + (−A) = (−A) + A = 0m×n. 5. Matrix addition is commutative: for all A, B ∈ Mmn(R), A + B = B + A. These 5 properties together show that Mmn(R) is an abelian group. On the basis of these properties, one can prove many other properties about matrices in Mmn(R). For example, one can prove that the inverse of a matrix with respect to addition is unique. Theorem 4.10. (Mmn(R), +) is an abelian group. 4.5 Scalar Multiple of a Matrix Definition 4.11. Given a m × n matrix A = [aij] and k ∈ R, the scalar multiple kA is the m × n matrix which has (i, j) entry kaij, i.e. kA = [kaij]. So, if A = [ 9 11 3 −1 ] then 2A = [ 18 22 6 −2 ] . Note that k can be any real number, i.e. −1 3A = [ −3 − 11 3 −1 1 3 ] . Save this theorem for your revision. Theorem 4.12. The scalar multiplication defined in Definition 4.11 together with matrix addition makes Mmn(R) into a vector space over R. 68 CHAPTER 4. LINEAR EQUATIONS AND MATRICES 4.6 Matrix Multiplication Definition 4.13. Let A = [aij] ∈ Mmn(R) have dimension m × n and B = [bij] ∈ Mnp(R) have dimension n × p. The matrix product C = [cij] = A · B ∈ Mmp(R) where cij = n∑ k=1 aikbkj = ai1b1j + ai2b2j + ai3b3j + · · · + ainbnj. Note that the number of columns in A is equal to the number of rows in B in Definition 4.13. For example A ︷ ︸︸ ︷[• • • • • • ] ︸ ︷︷ ︸ •×• · B ︷ ︸︸ ︷   • • • • • • • • •    ︸ ︷︷ ︸ •×• = A·B ︷ ︸︸ ︷[ • • • • • • ] ︸ ︷︷ ︸ •×• (4.5) Note in (4.5), B · A is not defined, since the matrix dimensions do not match for matrix multiplication. We now check the key properties for matrix multiplication. 1. When two matrices can be multiplied together, the result is another matrix, but because there is no guarantee that the product of any two given matrices exists, we cannot say that matrix multiplication is an internal operation over all matrices. If we consider the set Mnn(R), i.e. the set of all square n × n matrices for a given n, we can say that Mnn(R) is closed under matrix multiplication (or that matrix multiplication is an internal operation in this set) since any two matrices in Mnn(R) can be multiplied together, to make another n × n matrix. 2. For any three matrices that can be multiplied in a given sequence, matrix multipli- cation is associative, i.e. (A · B) · C = A · (B · C) = A · B · C. 3. When it makes sense, the distributive laws hold. For example, if A ∈ Mmn(R) and B and C are in Mnp(R), then A · (B + C) = A · B + A · C. 4.6. MATRIX MULTIPLICATION 69 4. For any matrix, one can find a matrix that acts like an identity with respect to matrix multiplication. This is called an identity matrix: Definition 4.14. The n × n identity matrix is: In =          1 0 · · · · · · 0 0 1 0 · · · 0 ... . . . . . . . . . ... 0 · · · 0 1 0 0 · · · · · · 0 1          ∈ Mnn(R). That is, the n × n matrix with (i, j) entry given by { 1 : when i = j 0 : when i ̸= j. For example: I2 = ( 1 0 0 1 ) . Additionally, given a 2 × 2 matrix, say A = ( a11 a12 a21 a22 ) then it follows that A · I2 = ( a11 a12 a21 a22 ) = I2 · A. More generally, for any n × n matrix A we have, A · In = A = In · A, and for any m × n matrix A we have, A · In = A = Im · A. The last case clearly shows that there is not necessarily a single matrix that acts as the identity when multiplying to the left or to the right, so there is no identity in the set of all matrices with respect to matrix multiplication. There is a well defined identity in the set of n×n square matrices, Mnn(R), i.e. In, so matrix multiplication has an identity in Mnn(R) (for each n ∈ N). 70 CHAPTER 4. LINEAR EQUATIONS AND MATRICES 5. It is clear that the zero matrix, 0m×n cannot have an inverse, but do all other matrices have an inverse? We will discuss this briefly in the next section, and moreover devote an entire chapter to this issue (see Chapter 5). 6. We have seen earlier that matrix dimension may not allow us to calculate B · A even if the product A · B exists. Furthermore, even if A and B are both n × n matrices, then it need not be the case that A · B = B · A. Hence, matrix multiplication is not commutative. Example 55: Let A = [ 1 2 3 4 ] and B = [0 1 0 0 ] . Then, A · B = [0 1 0 3 ] and B · A = [ 3 4 0 0 ] . Hence matrix multiplication is not commutative in Mnn(R). An indication that other familiar rules do not hold is given by Example 56: Let A = [ 1 1 2 2 ] and B = [ 1 −1 −1 1 ] . Then A · B = [ 0 0 0 0 ] . Some specific types of matrices, given in Definitions 4.15 and 4.16 will appear throughout your studies: Definition 4.15. An n × n matrix A = [aij] ∈ Mnn(R) is diagonal when: ai,j = 0 for i ̸= j i.e. A =          a1,1 0 · · · · · · 0 0 a2,2 0 · · · 0 ... . . . . . . . . . ... 0 · · · 0 an−1,n−1 0 0 0 · · · 0 an,n          . Diagonal matrices are easy to work with, particularly the set of diagonal matrices is closed under both matrix addition and multiplication. Example 57: If A = ( 2 0 0 3 ) , 4.7. MATRIX INVERSES 71 then we have A 2 = ( 2 0 0 3 ) · ( 2 0 0 3 ) = ( 4 0 0 9 ) and also that A10 = ( 2 10 0 0 310 ) . Consider how much harder it is to calculate B10 where B = ( 2 1 0 3 ) . Definition 4.16. An upper triangular matrix A = [aij] ∈ Mnn(R) is an n × n matrix with aij = 0 for all i > j i.e.          a11 a12 a13 · · · a1n 0 a22 a23 · · · a2n 0 0 a33 · · · a3n ... ... . . . . . . ... 0 0 · · · 0 ann          . Similarly, a lower triangular matrix is a n × n matrix with: aij = 0 for all j > i          a11 0 · · · 0 0 a21 a22 0 · · · 0 a31 a32 a33 0 ... ... . . . . . . . . . ... an1 an2 an3 · · · ann          . 4.7 Matrix Inverses Definition 4.17. Let A ∈ Mnn(R) be an n × n matrix. If there exists an n × n matrix B such that A · B = In = B · A then A is said to be invertible and we write B = A −1. B is called the inverse of A. 72 CHAPTER 4. LINEAR EQUATIONS AND MATRICES Note that it is also common to refer to invertible matrices as “non-singular” and matri- ces which are not invertible are then called “singular”. To clarify, the inverse of a matrix A, by definition, refers to the inverse with respect to matrix multiplication. To justify the “the” in Definition 4.17 (which implies uniqueness of the inverse) we prove: Theorem 4.18. Any n × n matrix has at most one inverse. Proof: Suppose that A ∈ Mnn(R) and that B1, B2 ∈ Mnn(R) are both inverses of A. Then A · B1 = In = B1 · A (4.6) and A · B2 = In = B2 · A. (4.7) Now we calculate that B1 = B1 · In = B1 · A · B2 via (4.7) = In · B2 via (4.6) = B2, and hence B1 = B2. Therefore, there can be at most one inverse of an n × n matrix, as required. For the first time, in the next example, we see that it is useful to be able to have inverses of elements in R. We’ll learn more about this in the shortly. Example 58: For A = ( 1 2 3 4 ) is B = ( −2 1 3 2 − 1 2 ) equal to A−1? Answer: We should check if B satisfies the conditions in Definition 4.17 i.e., if A · B = I2 = B · A. Since ( 1 2 3 4 ) · ( −2 1 3 2 − 1 2 ) = ( 1 0 0 1 ) , ( −2 1 3 2 − 1 2 ) · (1 2 3 4 ) = ( 1 0 0 1 ) , we conclude that B = A−1. Note that not every matrix is invertible, for example, 02×2 is clearly not invertible. Question: How was A−1 in Example 58 determined? Given a matrix B it is easy to check if it is equal to A−1 (simply check that it satisfies the conditions of Definition 4.17). But how would we have found it if we were not given it? We address this question in Chapter 5. 4.8. MATRICES AND LINEAR EQUATIONS 73 4.8 Matrices and Linear Equations We can now see that the system of simultaneous linear equations: a11x1 + a12x2 + a13x3 + · · · + a1nxn = b1 a21x1 + a22x2 + a23x3 + · · · + a2nxn = b2 ... ... ... ... ... ... ... ... ... ... am1x1 + am2x2 + am3x3 + · · · + amnxn = bm (4.8) can be written, equivalently, in matrix form:       a11 a12 a13 · · · a1n a21 a22 a23 · · · a2n ... ... ... ... ... am1 am2 am3 · · · amn       ·       x1 x2 ... xn       =       b1 b2 ... bm       , (4.9) or even more concisely as: A · x = b, (4.10) where A = [aij], x =     x1 ... xn     and b =     b1 ... bm    . We refer to A as the matrix associated with the corresponding system of simultaneous linear equations. It stores information about the coefficients. The augmented matrix for the systems of simultaneous linear equations given in (4.8)- (4.10) has an extra column and is given by:       a11 a12 a13 · · · a1n a21 a22 a23 · · · a2n ... ... ... ... ... am1 am2 am3 · · · amn ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ b1 b2 ... bm       . (4.11) The dimension of the augmented matrix is therefore (m × (n + 1)) and the final column stores the information contained on the right hand side of the (4.8) (or (4.9) or (4.10)). 4.9 Elementary Row Operations There are a number of “allowable” things we can do to a system of simultaneous linear equations which will not affect their solution. For example, we might multiply every term in an equation by a non-zero real number. That is, the set {(x, y) ∈ R 2 : 2x + 3y = 10} 74 CHAPTER 4. LINEAR EQUATIONS AND MATRICES is equal to {(x, y) ∈ R 2 : x + 3 2y = 5}. Such manipulations translate to “allowable” operations on the rows of the augmented matrix. These are known as elementary row operations (abbreviated to EROs) and are listed below: (i) interchange/switch two rows; (ii) multiply a row by a non-zero constant; and (iii) add a multiple of one row to another row. Theorem 4.19. EROs do not alter the solution set of a system of simultaneous linear equations. Proof: It is clear that if ERO (i) is applied to a system of simultaneous linear equations, then the solution set remains the same. Now, observe that for a, b, c, d ∈ R with c ̸= 0, a = b ⇐⇒ ac = bc, (4.12) a = b ⇐⇒ a + d = b + d. (4.13) If follows from (4.12) and (4.13) respectively, that if ERO (ii) or ERO (iii) are applied to a system of simultaneous linear equations, then the solution set remains the same, as required. Our strategy to solve systems of simultaneous linear equations will be to use EROs to systematically reduce the associated augmented matrix to a form that is easier to use. Definition 4.20. A matrix is in (row) echelon form when: • all rows consisting of only zeros are at bottom of the matrix; • the first non-zero number (in order from left to right) in any row is ‘1’; and • successive non-zero rows begin with more zeros left of the ‘1’ than the rows above. A matrix is in reduced echelon form if it is in echelon form and the first non-zero entry in each row is the only non-zero entry in its column. 4.10. GAUSSIAN ELIMINATION 75 Example 59: The matrices           1 ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ 0 0 1 ⋆ ⋆ ⋆ ⋆ 0 0 0 1 ⋆ ⋆ ⋆ 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0           ,           1 ⋆ ⋆ ⋆ ⋆ ⋆ 0 1 ⋆ ⋆ ⋆ ⋆ 0 0 1 ⋆ ⋆ ⋆ 0 0 0 1 ⋆ ⋆ 0 0 0 0 1 ⋆ 0 0 0 0 0 1           and [0 0 0 0 1 0 0 0 0 0 0 0 ] are in row echelon form and           1 ⋆ 0 0 ⋆ ⋆ 0 0 0 1 0 ⋆ ⋆ 0 0 0 0 1 ⋆ ⋆ 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0           is in reduced echelon form. 4.10 Gaussian Elimination Gaussian elimination, Gauss-Jordan elimination or simply row reduction are all names 2 given to the process of successive applications of EROs to transform a matrix to echelon form. It should be clear that the equations represented by an echelon matrix are much easier to solve by a process of “backwards substitution”. Thus we have a strategy for solving linear equations: 1. form the augmented matrix; 2. use ERO to reduce the augmented matrix to echelon form; and 3. solve the problem given in echelon form (if possible) by backwards substitution or by reducing to reduced echelon form. Example 60: Determine the solution set for 2x + y = 3, x − y = 3. (4.14) Answer: The augmented matrix associated with the system of simultaneous linear equa- tions is ( 2 1 1 −1 ∣ ∣ ∣ ∣ ∣ 3 3 ) . Applying the following EROs: 2If interested in the history of how this process got its name, see [3]. 76 CHAPTER 4. LINEAR EQUATIONS AND MATRICES r1 → r1 − r2 ( 1 2 1 −1 ∣ ∣ ∣ ∣ ∣ 0 3 ) r2 → r2 − r1 ( 1 2 0 −3 ∣ ∣ ∣ ∣ ∣ 0 3 ) r2 → r2/(−3) ( 1 2 0 1 ∣ ∣ ∣ ∣ ∣ 0 −1 ) demonstrates that the system of simultaneous linear equations in (4.14) is equivalent to x + 2y = 0, y = −1. Therefore, y = −1 and so x = 2, and hence the solution set is {(2, −1)} . Alternatively, we can take the EROs one step further and produce a matrix in reduced echelon form r1 → r1 − 2r2. ( 1 0 0 1 ∣ ∣ ∣ ∣ ∣ 2 −1 ) and we read the solution to be x = 2 and y = 1. Example 61: Determine the solution set for 2x + 2y = 4, x + y = 2. (4.15) Answer: The augmented matrix associated with the system of simultaneous linear equa- tions is ( 2 2 1 1 ∣ ∣ ∣ ∣ ∣ 4 2 ) . Applying the following EROs: r1 → 1 2 r1 ( 1 1 1 1 ∣ ∣ ∣ ∣ ∣ 2 2 ) r2 → r2 − r1 (1 1 0 0 ∣ ∣ ∣ ∣ ∣ 2 0 ) 4.10. GAUSSIAN ELIMINATION 77 demonstrates that the system of simultaneous linear equations in (4.15) is equivalent to x + y = 2, 0 + 0 = 0. Therefore, y can be an arbitrary real number, say y = k ∈ R which implies that x = 2−k. Hence, the solution set is the line {(2 − k, k) : k ∈ R} , i.e. there are infinitely many solutions. [This means that the lines defined by 2x + 2y = 4 and x + y = 2 are equal.] Example 62: Determine the solution set for 2x1 + 8x2 + 6x3 = 20, 4x1 + 2x2 − 2x3 = −2, −6x1 + 4x2 + 10x3 = 30. (4.16) Answer: The augmented matrix associated with the system of simultaneous linear equa- tions is    2 8 6 4 2 −2 −6 4 10 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 20 −2 30    Applying the following EROs: r1 → 1 2r1    1 4 3 4 2 −2 −6 4 10 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 10 −2 30    r2 → r2 − 4r1    1 4 3 0 −14 −14 −6 4 10 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 10 −42 30    r3 → r3 + 6r1    1 4 3 0 −14 −14 0 28 28 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 10 −42 90    r3 → r3 + 2r2    1 4 3 0 −14 −14 0 0 0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 10 −42 6    78 CHAPTER 4. LINEAR EQUATIONS AND MATRICES demonstrates that the system of simultaneous linear equations in (4.16) is equivalent to x1 + 4x2 + 3x3 = 10, − 14x2 − 14x3 = −42, 0 = 6. By considering the last equation, it can be seen that there is no solution to the system of simultaneous linear equations, i.e. the solution set is ∅. Note that in Example 62, although the final step is not in Echelon form, we can conclude at this stage that the system has no solution. Examples 60-62 illustrate the three possibilities that can arise - a solution set with one element, a solution set with infinitely many elements, or a solution set that is empty (i.e. the system cannot be solved). 4.11 Guidance for Reducing to Echelon Form • Obtain a ‘1’ at the top of the first (left to right) non-zero column by applying ERO(i) and then ERO(ii) to the augmented matrix, then use EROs (ii) and (iii) to create 0s in the rest of the column below this ‘1’. • Then move on to the next column. • Solve the system of simultaneous linear equations (if possible) by “backward sub- stitution\". • Avoid build up of many fractions by leaving division throughout a row until the end. Recall that systems of simultaneous linear equations may have: (i) a unique solution; (ii) no solution, if, for example one of the equations is 0 = c where c ̸= 0; or (iii) infinitely many solutions, if, for example an equation reduces to 0 = 0. Equations with solution sets of types (i) or (iii) are referred to as consistent, otherwise they are referred to as inconsistent. Infinitely many solutions occur when there are fewer (consistent) equations than un- knowns. Here, one or more variable(s) must be assigned an arbitrary real value. There is an element of choice here, and so equivalent solutions may appear in different guises. Example 63: Determine the solution set for x + 2y = 10. 4.11. GUIDANCE FOR REDUCING TO ECHELON FORM 79 Answer: Setting y = k where k ∈ R is arbitrary, we have x = 10 − 2k, which gives the solution set as {(10 − 2k, k) : k ∈ R}. However, we could have chosen to set x = t where t ∈ R is arbitrary, which gives y = 5− t 2. Thus, the solution set is equivalently given by {(t, 5 − t 2 ) : t ∈ R } . Example 64: Determine the solution set for x1 + 3x2 − 5x3 + x4 = 4, 2x1 + 5x2 − 2x3 + 4x4 = 6. (4.17) Answer: The augmented matrix associated with the system of simultaneous linear equa- tions is ( 1 3 −5 1 2 5 −2 4 ∣ ∣ ∣ ∣ ∣ 4 6 ) . Applying the following EROs: r2 → r2 − 2r1 (1 3 −5 1 0 −1 8 2 ∣ ∣ ∣ ∣ ∣ 4 −2 ) r2 → (−1)r2 ( 1 3 −5 1 0 1 −8 −2 ∣ ∣ ∣ ∣ ∣ 4 2 ) demonstrates that the system of simultaneous linear equations in (4.17) is equivalent to: x1 + 3x2 − 5x3 + x4 = 4, x2 − 8x3 − 2x4 = 2. These equations are consistent and have infinitely many of solutions. First set x4 = t and x3 = s where s, t ∈ R are arbitrary. Then, x2 = 2 + 8s + 2t, and hence, x1 = 4 − 3x2 + 5x3 − x4 = 4 − 3(2 + 8s + 2t) + 5s − t = −2 − 19s − 7t. Thus the solution set is {(−2 − 19s − 7t, 2 + 8s + 2t, s, t) : s, t ∈ R} . Again, we could choose to compute the reduced echelon form: 80 CHAPTER 4. LINEAR EQUATIONS AND MATRICES r1 → r1 − 3r2 (1 0 19 7 0 1 −8 −2 ∣ ∣ ∣ ∣ ∣ −2 2 ) and read of the solution from this. Example 65: Find conditions upon a, b and c that ensure equations 2x − y + 3z = a 3x + y − 5z = b −5x − 5y + 21z = c (4.18) are consistent. Answer: The augmented matrix associated with the system of simultaneous linear equa- tions is    2 −1 3 3 1 −5 −5 −5 21 ∣ ∣ ∣ ∣ ∣ ∣ ∣ a b c    . Applying the following EROs: r2 → r2 − r1    2 −1 3 1 2 −8 −5 −5 21 ∣ ∣ ∣ ∣ ∣ ∣ ∣ a −a + b c    r1 ⟲ r2    1 2 −8 2 −1 3 −5 −5 21 ∣ ∣ ∣ ∣ ∣ ∣ ∣ −a + b a c    r2 → r2 − 2r1 r3 → r3 + 5r1   1 2 −8 0 −5 19 0 5 −19 ∣ ∣ ∣ ∣ ∣ ∣ ∣ −a + b 3a − 2b −5a + 5b + c    r3 → r3 + r2    1 2 −8 0 −5 19 0 0 0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ −a + b 3a − 2b −2a + 3b + c    demonstrates that the system of simultaneous linear equations in (4.18) is equivalent to x + 2y − 8z = −a + b, − 5y + 19z = 3a − 2b, 0 = −2a + 3b + c. 4.12. BLOCK MATRICES∗ 81 Thus the equations are consistent if and only if −2a + 3b + c = 0. We can make this conclusion since if −2a + 3b + c ̸= 0 then the last equation cannot be satisfied. Moreover, if −2a + 3b + c = 0, then setting z = k for k ∈ R arbitrary, it follows that y = 2b − 3a + 19k 5 , and x = b − a + 8k − 2y = b − a + 8k − 4b − 6a + 38k 5 = a + b + 2k 5 . Thus the solution set is {(a + b + 2k 5 , 2b − 3a + 19k 5 , k ) : k ∈ R } , when −2a + 3b + c = 0, and ∅ when −2a + 3b + c ̸= 0. Again we can produce the reduced echelon form: r2 → −r2/5   1 2 −8 0 1 −19/5 0 0 0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ −a + b (2b − 3a)/5 −2a + 3b + c    r1 → r1 − 2r2,    1 0 −2/5 0 1 −19/5 0 0 0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ (a + b)/5 (2b − 3a)/5 −2a + 3b + c    from which we can directly read the solution. Note that in the above example, we answered the question (when is the system consis- tent?) without reducing the augmented matrix to its Echelon form. 4.12 Block Matrices∗ For some problems involving matrices, it is helpful to represent the matrices in blocks. For example, for linear systems, the augmented matrix in (4.11) is a block matrix consisting of a m × n block (with entries aij) and a m × 1 block (with entries bj). 3 Moreover, in 3In these notes, simple block matrices related to the solution of linear systems, typically, are referred to as augmented matrices instead of block matrices. 82 CHAPTER 4. LINEAR EQUATIONS AND MATRICES the Gaussian elimination algorithm (see Section 5.2), for A ∈ Mnn(R) we manipulate an augmented matrix of dimension n × 2n to determine whether or not an inverse of A exists. For another example, consider the matrix A ∈ M22(R) given by A = (2 3 1 1 ) (4.19) which can be represented with the two M12(R) matrices given by A1 = (2 3) and A2 = (1 1) , (4.20) specifically as A = ( A1 A2 ) . Similarly, B ∈ M23(R) given by B = ( 1 2 3 4 5 6 ) (4.21) can be represented with the three M21(R) matrices given by B1 = ( 1 4 ) , B2 = ( 2 5 ) and B3 = ( 3 6 ) , (4.22) specifically, as B = (B1 | B2 | B3). When it is clear, the lines separating the blocks in a block representation of a matrix are omitted. Example 66: Let A ∈ M22(R) be given by (4.19) and B ∈ M23(R) be given by (4.21) with (4.22). Establish that A · B = (A · B1 | A · B2 | A · B3), i.e. that column j of A · B is A · Bj. Answer: Observe that A · B = ( 14 19 24 5 7 9 ) (4.23) and A · B1 = ( 14 5 ) , A · B2 = ( 19 7 ) , A · B3 = ( 24 9 ) . Therefore, A · B = (A · B1 | A · B2 | A · B3). The general statement highlighted in Example 66 is given in 4.12. BLOCK MATRICES∗ 83 Proposition 4.21. Let A ∈ Mmn(R), B ∈ Mnp(R) and suppose Bj ∈ Mn1(R) are the j − th columns of B for j = 1, . . . , p. Then, A · B = A · (B1 | B2 | . . . | Bp) = (A · B1 | A · B2 | . . . | A · Bp). Proof: Follows directly from Definition 4.13. Similarly, we have Example 67: Let A ∈ M22(R) be given by (4.19) with (4.20) and B ∈ M23(R) be given by (4.21). Establish that A · B = ( A1 · B A2 · B ) , i.e. that row i of A · B is Ai · B. Answer: Observe that A1 · B = (14 19 24 ) A2 · B = (5 7 9 ) . Therefore, via (4.23), we have A · B = ( A1 · B A2 · B ) . Similarly, Example 67 highlights the following result, Proposition 4.22. Let A ∈ Mmn(R), B ∈ Mnp(R) and suppose Ai ∈ M1n(R) are the i − th rows of A for i = 1, . . . , m. Then, A · B =       A1 A2 ... Am       · B =       A1 · B A2 · B ... Am · B       . Proof: Follows directly from Definition 4.13. Propositions 4.21 and 4.22 imply that for A ∈ Mmn(R) and B ∈ Mnk(R), each row/column in A · B is a sum of multiples of the rows/columns of B/A. This notion will be helpful to prove results in later chapters/practice questions. Specifically, when we consider the general solution to linear systems of m equations in n unknowns, we will split a m × n matrix A into 4 blocks in the following form: A = ( A11 A12 A21 A22 ) (4.24) 84 CHAPTER 4. LINEAR EQUATIONS AND MATRICES with A11 a p × p matrix, A12 a p × (n − p) matrix, A21 a (m − p) × p matrix, and A22 is a (m − p) × (n − p) matrix (with 1 ≤ p < min{m, n}). Example 68: For A ∈ M45(R) given by A =      1 2 3 4 5 0 6 7 8 9 0 0 1 2 3 0 0 0 0 0      =      1 2 3 4 5 0 6 7 8 9 0 0 1 2 3 0 0 0 0 0      = ( A11 A12 A21 A22 ) , we can represent A in the block structure detailed in (4.24) (setting p = 3), i.e. with, A11 =    1 2 3 0 6 7 0 0 1    , A12 =    4 5 8 9 2 3    , A21 = (0 0 0 ) and A22 = (0 0) . Chapter 5 The Inverse of an Invertible Matrix ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • use Gaussian elimination to find the inverse of a square matrix when it exists; • use elementary matrices to represent elementary row operations; and • represent invertible matrices as a product of elementary matrices. [2, p.602-612] and [4, Chapters SSLE and M] contain alternative presentations of material in this chapter that you may find helpful. 5.1 Introduction Here we return to the question, first raised in Chapter 4, Section 4.7. Recall from Definition 4.17 that an n × n matrix A ∈ Mnn(R) is invertible with inverse B if and only if there exists an n × n matrix B that satisfies, A · B = In = B · A. We saw in Theorem 4.18 that if the matrix A is invertible then it has a unique inverse and we denote this matrix by A−1. Also recall that we have see that: • not every matrix is invertible, for example A = ( 0 0 0 0 ) ∈ M2×2(R) or even A = (0 ) ∈ M1×1(R). 85 86 CHAPTER 5. THE INVERSE OF AN INVERTIBLE MATRIX • Definition 4.17 does not provide a means of finding A−1 when it exists, but it provides a means of deciding if a matrix B is equal to A−1. In this chapter, we shall seek answers to the following questions for a general n × n matrix A: (i) Can we decide if A is invertible? (ii) For an invertible matrix A, is there a reasonably efficient routine which will allow A−1 to be calculated? Theorem 5.1. Suppose that A and B are invertible n × n matrices, then the matrix A · B is also invertible and (A · B)−1 = B−1 · A −1. Proof: A−1 and B−1 both exist, are n × n matrices, and satisfy: A · A−1 = In = A−1 · A, (5.1) B · B−1 = In = B−1 · B. (5.2) Therefore, (since matrix multiplication is associative) (A · B) · (B−1 · A−1) = A · (B · B−1) · A−1 = A · In · A−1 via (5.2) = A · A−1 = In via (5.1). (5.3) Similarly, (B−1 · A −1) · (A · B) = In, (5.4) and so, from Definition 4.17, (5.3) and (5.4) we conclude that (A · B) −1 exists and is given by (B−1 · A −1), as required. Corollary 5.2. Suppose that A1, A2, . . . , Am are all invertible n×n matrices. Then, (A1 · A2 · . . . · Am) −1 = A−1 m · A −1 m−1 · . . . · A−1 1 . To prove Corollary 5.2, use Theorem 5.1 and mathematical induction (see practice ques- tions). We now establish how inverse matrices can be used to solve systems of simultaneous linear systems of equations. 5.1. INTRODUCTION 87 Theorem 5.3. Let A ∈ Mnn(R) be an n × n matrix. If A is invertible, then for any column vector b ∈ Mn1(R), the system of simultaneous linear equations A · x = b (5.5) has a unique solution and this is given by x = A −1 · b. Proof: Since A is invertible, A−1 · b exists. Substituting x = A−1 · b into (5.5) gives A · x = A · (A −1 · b) = (A · A −1) · b = In · b = b. (5.6) Thus, via (5.6), A−1 · b is a solution to (5.5). Now, suppose that ˜x satisfies A · ˜x = b. Then, A · x = b = A · ˜x =⇒ A−1 · (A · x) = A−1 · (A · ˜x) =⇒ (A−1 · A) · x = (A−1 · A) · ˜x =⇒ In · x = In · ˜x =⇒ x = ˜x. Therefore, A −1 · b is the unique solution to (5.5), as required. From Theorem 5.3 we also have the following results which guarantee that a matrix A is not invertible. Corollary 5.4. Let A ∈ Mnn(R) be an n × n matrix and b be a column vector of length n. If the system of simultaneous linear equations A · x = b (5.7) has no solution, then A is not invertible, i.e. A −1 does not exist. Proof: Suppose that A−1 exists. Then via Theorem 5.3, x = A−1 · b is a solution to (5.7) which contradicts the condition that (5.7) has no solution. Therefore, we conclude that (since the supposition is incorrect) A−1 does not exist, as required. 1 Corollary 5.5. Let A ∈ Mnn(R) be an n × n matrix and b be a column vector of length n. If the system of simultaneous linear equations A · x = b (5.8) has at least 2 distinct solutions, then A is not invertible and there exist infinitely many solutions to (5.8). 1An alternative proof of Corollary 5.4 is: Corollary 5.4 is the contraposition of Theorem 5.3! This type of proof is known as proof by contraposition, namely, (if A ⇒ B) then (¬B ⇒ ¬A). 88 CHAPTER 5. THE INVERSE OF AN INVERTIBLE MATRIX Proof: Suppose that A −1 exists. Then via Theorem 5.3, x = A−1 · b is the unique solu- tion to (5.8) which contradicts the condition that (5.8) has 2 distinct solutions. Therefore, A−1 does not exist. Now suppose that x1 and x2 are 2 distinct solutions to (5.8). Then for λ ∈ R, A · (x1 + λ(x1 − x2)) = A · (x1) + A · (λ(x1)) − A · (λx2)) = b + λ(A · x1) − λ(A · x2) = b + λb − λb = b. Since x1 and x2 are distinct, it follows that for each λ ∈ R, x1 + λ(x1 − x2) is a distinct solution to (5.8) i.e. there are infinitely many solutions to (5.8), as required. 5.2 Gaussian Elimination Algorithm If we wish to determine whether or not an n × n matrix A is invertible, and if invertible, determine A−1 explicitly, we can use the following algorithm. Gaussian Elimination Algorithm: 1. Consider the augmented matrix of dimension n × 2n, given by, (A | In). Note that the line | is merely used to provide a clear separation of the two n × n blocks in the augmented matrix. 2. Perform EROs on the augmented matrix to reduce the n × n block on the left hand side to reduced row echelon form (denoted REch(A)), i.e. (A | In) → (REch(A) | B). 3. (a) If REch(A) = In, then B = A−1. (b) If REch(A) ̸= In, then A is not invertible. Note that for the algorithm defined above, all cases are covered (in step 3a and 3b). Before we justify the conclusions we can develop from the Gaussian Elimination Algorithm, we give 2 examples exhibiting an implementation of the algorithm. We also note that if, by applying EROs to the augmented matrix in the algorithm, the left hand side contains a row of zeros, we can immediately conclude that A is not invertible (to save time). Example 69: Let A =   2 4 3 0 1 −1 3 5 7    . 5.2. GAUSSIAN ELIMINATION ALGORITHM 89 Perform the Gaussian elimination algorithm to determine if A is invertible, and if A−1 exists, calculate it. Answer: 1. We consider the augmented matrix of dimension 3 × 6, given by,    2 4 3 0 1 −1 3 5 7 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 0 0 1 0 0 0 1    . 2. We perform EROs to reduce the left hand side of the augmented matrix to echelon form. Notice that the first “move” avoids creating rational entries early in the algorithm. A computer would not work this way. r3 → r3 − r1    2 4 3 0 1 −1 1 1 4 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 0 0 1 0 −1 0 1    r1 → r1 − r3    1 3 −1 0 1 −1 1 1 4 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 2 0 −1 0 1 0 −1 0 1    r3 → r3 − r1    1 3 −1 0 1 −1 0 −2 5 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 2 0 −1 0 1 0 −3 0 2    r3 → r3 + 2r2    1 3 −1 0 1 −1 0 0 3 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 2 0 −1 0 1 0 −3 2 2    r1 → r1 − 3r2   1 0 2 0 1 −1 0 0 3 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 2 −3 −1 0 1 0 −3 2 2    r3 → 1 3r3    1 0 2 0 1 −1 0 0 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 2 −3 −1 0 1 0 −1 2 3 2 3    r1 → r1 − 2r3    1 0 0 0 1 −1 0 0 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 4 − 13 3 − 7 3 0 1 0 −1 2 3 2 3    90 CHAPTER 5. THE INVERSE OF AN INVERTIBLE MATRIX r2 → r2 + r3    1 0 0 0 1 0 0 0 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 4 − 13 3 − 7 3 −1 5 3 2 3 −1 2 3 2 3    3. Since the left hand side of the augmented matrix is, after EROs, equal to I3, we conclude that A is invertible, and the inverse is A −1 =    4 − 13 3 − 7 3 −1 5 3 2 3 −1 2 3 2 3    . Example 70: Let A =   1 2 3 4 5 6 5 7 9    . Perform the Gaussian elimination algorithm to determine if A is invertible, and if A−1 exists, calculate it. Answer: 1. We consider the augmented matrix of dimension 3 × 6, given by,   1 2 3 4 5 6 5 7 9 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 0 0 1 0 0 0 1    . 2. We perform EROs to reduce the left hand side of the augmented matrix to echelon form. r2 → r2 − 4r1 r3 → r3 − 5r1    1 2 3 0 −3 −6 0 −3 −6 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 0 −4 1 0 −5 0 1    r3 → r3 − r2    1 2 3 0 −3 −6 0 0 0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 0 −4 1 0 −1 −1 1    3. Since the left hand side of the augmented matrix, after EROs, contains a row of zeros, we conclude that A is not invertible. In Example 69 one can directly verify against Definition 4.17 that the conclusion is correct i.e.    2 4 3 0 1 −1 3 5 7    ·    4 − 13 3 − 7 3 −1 5 3 2 3 −1 2 3 2 3    =    1 0 0 0 1 0 0 0 1    =    4 − 13 3 − 7 3 −1 5 3 2 3 −1 2 3 2 3    ·    2 4 3 0 1 −1 3 5 7    i.e, that A · A−1 = In = A−1 · A. The conclusion in Example 70 is more difficult to directly check at this point, but will be addressed in the next sections. 5.3. INTRODUCTION TO ELEMENTARY MATRICES 91 5.3 Introduction to Elementary Matrices We now introduce some ‘machinery’ to allow us to justify that the conclusions in the Gaussian Elimination algorithm (see 3.a and 3.b) are valid. Definition 5.6. An n × n elementary matrix is any matrix obtained by perform- ing one elementary row operation to the identity matrix In. Recall the three types of EROs defined in Chapter 3, Section 8. Here are some examples: r1 ⟲ r2    0 1 0 1 0 0 0 0 1    r1 → (−5) × r1   −5 0 0 0 1 0 0 0 1    r1 ⟲ r3    0 0 1 0 1 0 1 0 0    r2 → r2 + 2r1    1 0 0 2 1 0 0 0 1    r1 → 1 2r1 ( 1 2 0 0 1 ) NOT ELEMENTARY      0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0      Note, that in the final example above, the matrix corresponds to two ERO applied simul- taneously, specifically, r1 ⟲ r4 and r2 ⟲ r3. 92 CHAPTER 5. THE INVERSE OF AN INVERTIBLE MATRIX 5.4 Some Basic Properties of Elementary Matrices (i) Each elementary matrix is invertible (see Proposition 5.10) and the inverse is easy to find (simply undo the appropriate row operation). For the last 3 elementary matrices in the previous section, we have:   0 0 1 0 1 0 1 0 0    −1 =   0 0 1 0 1 0 1 0 0    (r1 ⟲ r3 in I3)   1 0 0 2 1 0 0 0 1    −1 =    1 0 0 −2 1 0 0 0 1    (r2 → r2 − 2r1 in I3) ( 1 2 0 0 1 )−1 = ( 2 0 0 1 ) (r1 → 2r1 in I2) (ii) Multiplying a matrix A on the left by an elementary matrix corresponds to per- forming the corresponding elementary row operation to A. For example, the second example above swaps rows 1 and 3, so multiplying by the associated matrix on the left of a general 3 × 3 matrix can be seen to switch the entries in rows 1 and 3:   0 0 1 0 1 0 1 0 0    ·    a b c d e f g h i    =   g h i d e f a b c    . Additionally, the fourth example in the previous section adds 2 times row 1 to row 2, so multiplying by the associated matrix on the left of a general 3 × 3 matrix can, similarly, be seen to add 2 times the entry in row 1 to the corresponding entry in row 2:   1 0 0 2 1 0 0 0 1    ·    a b c d e f g h i    =    a b c 2a + d 2b + e 2c + f g h i    . That is 2 times row 1 has been added to row 2. Let’s be more precise about the matrix entries. Let Eij represent the n × n matrix which is defined to have every entry zero aside from having a 1 in the (i, j) place (ith row and jth column). Thus Eij = (ers) where ers = 0 for (r, s) ̸= (i, j) and eij = 1. So, for example, the 4 × 4 matrix is E23 =      0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0      . 5.4. SOME BASIC PROPERTIES OF ELEMENTARY MATRICES 93 Let Eij = (ers) and Eℓm = (e ∗ rs). Then EijEℓm = (brs) where brs = n∑ t=1 erte ∗ ts and Eℓm. We first note that brs = 0 unless r = i and m = s and then we have bim = ∑n t=1 eite∗ tm and this is 0 unless t = j = ℓ in which case it is 1. Hence EijEℓm =   Eim j = ℓ 0 j ̸= ℓ . (5.9) For i ̸= j and λ ∈ R, set E(i,j,λ) = In + λEij. For example, the 4 × 4 matrix E(2,4,λ) =      1 0 0 0 0 1 0 λ 0 0 1 0 0 0 0 1      . For an arbitrary such matrix we calculate E(i,j,λ) · E(i,j,−λ) = (In + λEij) · (In − λEij) = In + λEij − λEij − λ2E2 ij = In. Replacing λ by −λ, we also see that E(i,j,−λ) · E(i,j,λ) = In. Hence E(i,j,λ) is invertible. Lemma 5.7. Suppose that A ∈ Mnm(R). Then, for λ ∈ R, left multiplication by E(i,j,λ) adds λ times the jth row of A to row i of A. Furthermore, E−1 (i,j,λ) = E(i,j,−λ). Assume that µ ∈ R is non-zero. Let E[i,µ] = In + (µ − 1)Eii. So E[i,µ] = diag(1, . . . , µ, . . . , 1) is a diagonal matrix with µ in the ith position. We see E−1 [i,µ] = E[i,µ−1] and so E[i,µ] is invertible. 94 CHAPTER 5. THE INVERSE OF AN INVERTIBLE MATRIX Lemma 5.8. Suppose that A ∈ Mnm(R). Then, for non-zero µ ∈ R, left multipli- cation by E[i,µ] scales the ith row of A by µ. Furthermore, E−1 [i,µ] = E[i,µ−1]. For 1 ≤ i < j ≤ n, set E(i,j) = In − Eii − Ejj + Eij + Eji. For example, the 4 × 4 matrix E(2,4) =      1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0      . Then, multiply out term by term in the second factor and using Equation 5.9 we get E2 (i,j) = (In − Eii − Ejj + Eij + Eji) · (In − Eii − Ejj + Eij + Eji) = (In − Eii − Ejj + Eij + Eji) − (Eii − Eii + Eji) −(Ejj − Ejj + Eij) + (Eij − Eij + Ejj) + (Eji − Eji + Eii) = In. Hence E(i,j) is invertible. Lemma 5.9. Suppose that A ∈ Mnm(R). Then left multiplication by E(i,j) swaps row i and row j in A. Furthermore, E−1 (i,j) = E(i,j). Elementary matrices are a useful algebraic device for implementing, through matrix multi- plication on the left, elementary row operations. To establish the validity of the Gaussian elimination algorithm, the following proposition is helpful. We have just seen that Proposition 5.10. Elementary matrices are invertible and the inverse of an ele- mentary matrix is an elementary matrix. Proof: This follows from our previous discussion and Lemmas 5.7, 5.8 and 5.9. 5.5. VALIDITY OF THE GAUSSIAN ELIMINATION ALGORITHM 95 5.5 Validity of the Gaussian Elimination Algorithm To make the proof of the validity of conclusions from the Gaussian Elimination algorithm (GA) more transparent, we split the proof into 2 parts. Proof of conclusion in 3(a) in the GA: Suppose that the elementary row operations ρ1, ρ2, . . . , ρm correspond to elementary matrices E1, E2, . . . , Em. So applying ρ1 to (A|In) replaces A bey E1 · A and In by E1. So ρ1 transforms (A|In) → (E1 · A|E1). Now applying ρ2 we have (E1A|E1) → (E2 · E1 · A|E2 · E1) continuing in this way finally yields (Em · · · · · E2 · E1 · A|Em · · · · · E2 · E1) Since these row operations reduce (A|In) to (In|B), we have Em · · · · · E2 · E1 · A = In and B = Em · · · · · E2 · E1. In particular, B · A = In. (5.10) Proposition 5.10, yields Ej is invertible for 1 ≤ j ≤ m and so Corollary 5.2 implies that B is invertible. Hence multiplying both sides of (5.10) by B−1 yields B−1 · B · A = B−1. Hence A = B−1 and so A · B = B−1 · B = In. (5.11) Together (5.10) and (5.11) imply A is invertible and its inverse in B. Proof of conclusion in 3(b) in GA algorithm: Suppose that for an n × n matrix A and after following steps 1 and 2 of the Gaussian Elimination Algorithm, A is in echelon form with a row of zeros in the bottom row. Therefore, there exist m elementary matrices E1, E2, . . . , Em such that Em · Em−1 · . . . · E1 · A = C (5.12) with C in reduced echelon form but is not In. Then C has bottom row consisting of zeros. Let b be the column n × 1 matrix with every entry 1. Then the system of equations C · x = b 96 CHAPTER 5. THE INVERSE OF AN INVERTIBLE MATRIX has no solutions as the bottom row reads 0 = 1. Hence Corollary 5.7 implies that C is not invertible. However, if A is invertible, then Corollary 5.2 and 5.12 imply C is invertible. As this is not the case, we conclude A is not invertible. Example 71: Show that the matrix A =   1 3 4 0 0 1 2 8 8    is invertible and determine its inverse A−1. Answer: Start with (A∣ ∣ ∣ In) =    1 3 4 0 0 1 2 8 8 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 0 0 1 0 0 0 1    . Then since, ρ1 : r3 → r3 − 2r1    1 3 4 0 0 1 0 2 0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 0 0 1 0 −2 0 1    E1 =    1 0 0 0 1 0 −2 0 1    ρ2 : r2 ⟲ r3    1 3 4 0 2 0 0 0 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 0 −2 0 1 0 1 0    E2 =    1 0 0 0 0 1 0 1 0    ρ3 : r2 → 1 2r2    1 3 4 0 1 0 0 0 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 1 0 0 −1 0 1 2 0 1 0    E3 =    1 0 0 0 1 2 0 0 0 1    ρ4 : r1 → r1 − 3r2    1 0 4 0 1 0 0 0 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 4 0 − 3 2 −1 0 1 2 0 1 0    E4 =    1 −3 0 0 1 0 0 0 1    ρ5 : r1 → r1 − 4r3    1 0 0 0 1 0 0 0 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ 4 −4 − 3 2 −1 0 1 2 0 1 0    E5 =    1 0 −4 0 1 0 0 0 1    . We conclude that A−1 =    4 −4 − 3 2 −1 0 1 2 0 1 0    . 5.5. VALIDITY OF THE GAUSSIAN ELIMINATION ALGORITHM 97 Example 72: Write A−1 and hence A from the previous example as a product of ele- mentary matrices. Answer: From the previous example we have that A−1 = E5 · E4 · E3 · E2 · E1. Thus, A−1 =    1 0 −4 0 1 0 0 0 1    ·    1 −3 0 0 1 0 0 0 1    ·    1 0 0 0 1 2 0 0 0 1    ·    1 0 0 0 0 1 0 1 0    ·    1 0 0 0 1 0 −2 0 1    . Furthermore (from Corollary 5.2) A = E−1 1 · E−1 2 · E−1 3 · E −1 4 · E−1 5 , and hence, A =    1 0 0 0 1 0 2 0 1    ·    1 0 0 0 0 1 0 1 0    ·    1 0 0 0 2 0 0 0 1    ·    1 3 0 0 1 0 0 0 1    ·    1 0 4 0 1 0 0 0 1    . Theorem 5.11. Every invertible matrix in Mnn(R) can be written as a product of a finite number of elementary matrices. 98 CHAPTER 5. THE INVERSE OF AN INVERTIBLE MATRIX Chapter 6 Conics ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • list the types of conics; • sketch (draw) a conic from an equation; • write down the standard equation of a conic; and • understand key properties satisfied by conics. [2, p.458-468] contains an alternative presentation of material in this chapter that you may find helpful. 6.1 Introduction Conic sections are the family of curves defined by the intersection of a cone and a plane in R 3. This intersection can take different forms according to the angle the intersecting plane makes with the cone. The standard conic sections are the circle, the ellipse, the parabola and the hyperbola. Degenerate cases are obtained when the plane intersects the vertex of the cone. These so-called degenerate conics can be either a single point or a single line or pair of intersecting lines. The various possible non-degenerate conic sections are depicted in Figure 6.1. In the following sections, we discuss parabolae, ellipses and hyperbolae in detail. 99 100 CHAPTER 6. CONICS Figure 6.1: The various conic sections, as depicted in [1]. 6.2 Parabola 6.2.1 The standard equation of a parabola Definition 6.1. The standard equation of a parabola with focus at the point F with coordinates (0, p) and directrix y = −p, with p > 0, is given by x2 = 4py. The parabola, focus and directrix are illustrated in Fig 6.2. See Appendix C.2 for a derivation of the standard equation from the geometric definition of a parabola. Example 73: The equation y = x2 8 , represents a parabola with the y-axis as the axis of the parabola and focal length p = 2. It is depicted in Figure 6.4. The focal length p dictates the shape of the parabola. When p is small, the parabola is narrow, but when p is large, the parabola opens up wide. This is also illustrated in Figure 6.4. 6.2. PARABOLA 101 axis of the parabola directrix vertex focus width focal length parabola Figure 6.2: Sketch of main terms related to a parabola. Figure 6.3: The standard equation of a parabola. 6.2.2 The graph with equation y = ax 2 + bx + c The equation of a parabola with vertical axis parallel to the y-axis can be written in the general form (with a ̸= 0), y = ax2 + bx + c. (6.1) Let us now consider whether every equation of the form of equation (6.1) represents a parabola. Therefore, we rewrite the general equation (6.1) as y = a ( x + b 2a )2 + ( c − b2 4a ) , 102 CHAPTER 6. CONICS Figure 6.4: The parabolas with equations y = x2/8, y = x2 and y = x2/16. or y − (c − b2 4a ) = a (x + b 2a )2 . This can be recognised as the equation of a parabola with vertex at P ( − b 2a , c − b2 4a ) , axis x = − b 2a , and focal length p = 1 4a . 6.2.3 Alternative equations Theorem 6.2. Parametric equations for a parabola with focus at the point F with coordinates (0, p) and directrix y = −p, with p > 0, are { x = 2pt, y = pt 2, t ∈ R 6.2. PARABOLA 103 Proof: This can easily be verified by substituting these expressions in the standard equation in Theorem 6.1. When using polar coordinates (r, θ), where x = r cos θ, y = r sin θ, (6.2) the following two results are useful. Theorem 6.3. The polar equation of a parabola with focus at O(0, 0) and directrix y = −p, with p > 0, is r(θ) = p 1 − sin(θ). Proof: Given the focus and directrix of the parabola are at (0, 0) and on y = −p respec- tively, it follows that the parabola has vertex at (0, − p 2 ) and focal length p 2 . Since the axis of the parabola is parallel to (and in the same direction as) the y-axis, the equation of the parabola is x2 = 4 ( p 2 ) (y + p 2 ) ⇐⇒ x2 − 2py − p2 = 0. (6.3) Substituting x = r cos (θ) and y = r sin (θ) into (6.3) gives, cos 2 (θ)r2 − 2p sin (θ)r − p2 = 0. The solutions to the quadratic equation (in r) are given by r = 2p sin (θ) ± √ 4p2 sin2 (θ) + 4p2 cos2 (θ) 2 cos2 (θ) = p(sin (θ) ± 1) cos2 (θ) . (6.4) Since the polar coordinate r is non-negative, this implies that we must add the second term on the RHS of (6.4) and hence, r = p(sin (θ) + 1) cos2 (θ) = p(sin (θ) + 1) 1 − sin2 (θ) = p(sin (θ) + 1) (1 − sin (θ))(1 + sin (θ)) = p 1 − sin (θ). Note that although we can only cancel terms in the last step of the calculation when θ ̸= − π 2 (to avoid dividing by zero), the result still holds when θ = − π 2 , as required. 104 CHAPTER 6. CONICS Theorem 6.4. The polar equation of a parabola with focus at O(0, 0) and directrix x = −p, with p > 0, is r(θ) = p 1 − cos(θ). Proof: Exercise. 6.3 Ellipse 6.3.1 The standard equation of an ellipse Definition 6.5. The standard equation of an ellipse with foci at F1(c, 0) and F2(−c, 0), with c > 0, is given by x2 a2 + y2 b2 = 1, where 2a = | ⃗P F1| + | ⃗P F2| (the sum of the distances of a point on the ellipse to both foci) and b2 = a2 − c2 > 0. This implies that the centre of the ellipse in standard form is located at the origin. This ellipse is illustrated in Figure 6.6. focal axis minor axis directrixdirectrix centre focusfocus vertex vertex centre to focus distance ellipse Figure 6.5: Sketch of main terms related to a ellipse. 6.3. ELLIPSE 105 Figure 6.6: The standard equation of the ellipse. See Appendix C.3 for a derivation of the standard equation from the geometric definition of an ellipse. Example 74: The ellipse with equation x2 9 + y2 4 = 1, is depicted in Figure 6.7. Figure 6.7: The ellipse with equation x2 9 + y2 4 = 1. In this example notice that c < a and b < a. 106 CHAPTER 6. CONICS 6.3.2 Eccentricity Definition 6.6. The eccentricity of an ellipse (when expressed in a coordinate system in standard form) is defined to be, e = c a = √ a2 − b2 a . An ellipse can be almost circular or very flat. Its shape is controlled by the eccentricity e of the ellipse. From Definition 6.5, it is clear that 0 < e < 1. For very small eccentricity, a ≈ b and the ellipse is “close to\" a circle. For an eccentricity close to 1 (b ≪ a) the ellipse is very flat. Figure 6.8 illustrates this by depicting two ellipses with different eccentricities. Figure 6.8: Two ellipses with eccentricities e ≈ 0.99 and e ≈ 0.26 respectively. The first ellipse is given by x2 9 + y2 8.41 = 1, and has eccentricity e = √9 − 8.41 3 = 0.256 . . . . 6.3. ELLIPSE 107 The second ellipse has as equation x2 9 + y2 0.16 = 1, with eccentricity e = √9 − 0.16 3 = 0.991 . . . . 6.3.3 Additional equations and properties Definition 6.7. For an ellipse with foci at F1(c, 0) and F2(−c, 0) and standard equation x2 a2 + y2 b2 = 1, the lines with equations x = ±a e are known as the directrices of the ellipse. Theorem 6.8. The following statements are equivalent: (a) P is a point on the ellipse. (b) the ratio of the distance from P to a focus Fi and the distance from the point P to then corresponding directrix Di is equal to the eccentricity e, i.e. |P Fi| = e|P Di|. Proof: Consider the focus F1 at (c, 0). The corresponding directrix has equation D1 : x = a e . Also from Definition 6.6, c = ae. Using the standard equation for an ellipse, and b2 = a2 − c2 = a2(1 − e 2), (6.5) for a point P (x, y) on the ellipse, y2 = b2 − b2x2 a2 = a2(1 − e2) − a2(1 − e 2)x2 a2 = (a2 − x2)(1 − e2). (6.6) Hence, | ⃗P F1| = √(x − ae)2 + y2 108 CHAPTER 6. CONICS = √x2 − 2aex + a2e2 + (a2 − x2)(1 − e2) (via (6.6)) = √x2e2 − 2aex + a2 = √(xe − a)2 = e ∣ ∣ ∣ ∣x − a e ∣ ∣ ∣ ∣ = e| ⃗P D1|. (6.7) Therefore, from (6.7) if (a) is true, then (b) is true. Now, conversely, if a point P (x, y) satisfies | ⃗P F1| = e| ⃗P D1|, with | ⃗P F1| = √ (x − ae)2 + y2, and | ⃗P D1| = ∣ ∣ ∣ ∣x − a e ∣ ∣ ∣ ∣ , (6.8) then | ⃗P F1| 2 = e2| ⃗P D1| 2 so that via (6.8), (x − ae) 2 + y2 = e2 (x − a e )2 . Hence, (1 − e 2)x2 − a2(1 − e2) + y2 = 0, so via (6.5), x2 a2 + y2 b2 = 1. (6.9) Equation (6.9) states that P is on the ellipse and hence, if (b) is true then (a) is true, so (a) ⇐⇒ (b), as required. Theorem 6.9. Parametric equations for an ellipse with foci at F1(c, 0) and F2(−c, 0), with c > 0, as given by Definition 6.5, are { x = a cos(t), y = b sin(t), t ∈ [0, 2π). Proof: This can easily be verified by substituting the expressions above into the standard equation of an ellipse. When using polar coordinates, the following result is useful. Theorem 6.10. The polar equation of an ellipse with a focus at O(0, 0) and directrix x = d, with d > 0 and eccentricity e ∈ [0, 1), is given by r(θ) = ed 1 + e cos(θ). 6.4. HYPERBOLA 109 Proof: Consider this as an exercise. Example 75: The special case a = b = ρ yields the standard equation of a circle with centre at the origin and radius ρ: x2 + y2 = ρ2. Notice that a circle, is formally and ellipse with eccentricity e = 0, with both foci at the same point, and with directrices ‘at infinity’. The polar equation of a circle is r(θ) = ρ, with parametric equations given by { x = ρ cos(t), y = ρ sin(t). 6.4 Hyperbola 6.4.1 Standard equation Definition 6.11. The standard equation of a hyperbola with foci at F1(c, 0) and F2(−c, 0), where c > 0, is given by x2 a2 − y2 b2 = 1, where 2a is the difference of the distances of a point P on the hyperbola to the two foci, and b2 = c2 − a2 > 0. This means that the centre of the hyperbola in standard form is at the origin. This configuration is illustrated in Fig. 6.10. See Appendix C.4 for a derivation of the standard equation from the geometric definition of a hyperbola. Example 76: The hyperbola with equation x2 4 − y2 9 = 1, is plotted in Figure 6.10. The vertices are located at V (±a, 0) = (±2, 0). The foci are then given by F (± √13, 0) since c2 = a2 + b2 = 13. 110 CHAPTER 6. CONICS focal axis/ major axis minor axis directrixdirectrix centre focusfocus vertex vertex asymptote asymptote hyperbola Figure 6.9: Sketch of main terms related to a hyperbola. Figure 6.10: The hyperbola with equation x2/4 − y2/9 = 1 with c = √13. 6.4. HYPERBOLA 111 6.4.2 Asymptotes We can rewrite the standard equation for a hyperbola as y2 = b2x2 a2 − b2, where for very large values of x, the right hand side will be dominated (this term is much larger than the others) by the x2 term, y2 ∼ b2x2 a2 as x → ∞. This means that for large values of x, the hyperbola is close to the two lines y = b ax and y = − b ax. (6.10) These two lines are called the asymptotes of the hyperbola. Example 77: Figure 6.10 displays the two asymptotes given by (6.10) which, for the hyperbola with equation, x2 4 − y2 9 = 1, have equations y = (3/2)x and y = −(3/2)x. 6.4.3 Eccentricity Definition 6.12. The eccentricity of a hyperbola (in standard form) is defined as, e = c a = √a2 + b2 a . Notice that for a hyperbola, e > 1 (in contrast to an ellipse). Example 78: Figure 6.11 depicts two hyperbola with different eccentricities, i.e. x2 2 − y2 0.2 = 1 and x2 2 − y2 5 = 1. 112 CHAPTER 6. CONICS Figure 6.11: Hyperbolae with different eccentricities. 6.4.4 Additional equations and properties Definition 6.13. For a hyperbola with foci at F1(c, 0) and F2(−c, 0), where c > 0, and standard equation x2 a2 − y2 b2 = 1, the lines with equations x = ±a e , are known as the directrices of the hyperbola. Theorem 6.14. P is a point on the hyperbola if and only if the ratio of the distance from P to a focus Fi and the distance from the point P to the corresponding directrix Di is equal to the eccentricity e, i.e. | ⃗P Fi| = e| ⃗P Di|. Proof: Consider this as an exercise. 6.5. GENERAL POLAR EQUATION OF A CONIC 113 Theorem 6.15. Parametric equations for a hyperbola with foci at F1(c, 0) and F2(−c, 0), with c > 0, and eccentricity e, are given by { x = a sec(t), y = b tan(t), t ∈ (−π, π), or { x = ±a cosh(t), y = b sinh(t), t ∈ R, where 2a is the difference between the distance from a point on the hyperbola to its foci, and b2 = a2(e 2 − 1). Proof: The result can be verified by substituting the equations above into the standard equation for a hyperbola. Theorem 6.16. The polar equation of the right hand arc of a hyperbola with focus F1(0, 0), corresponding directrix D1 : x = −d and eccentricity e > 1 is r(θ) = ed 1 − e cos θ . Proof: Consider this as an exercise. 6.5 General polar equation of a conic We can summarise the results which describe conics via polar coordinates in the previous sections in the following result. Theorem 6.17. The polar equations r(θ) = ed 1 ± e cos(θ), and r(θ) = ed 1 ± e sin(θ), represent a conic with focus at O(0, 0), eccentricity e and with d the distance between the focus and the corresponding directrix. 114 CHAPTER 6. CONICS Proof: Follows from Theorems 6.3, 6.4, 6.10 and 6.16. The conic is an ellipse for e < 1, parabola for e = 1 and hyperbola for e > 1. The equations r(θ) = ed 1 ± e cos(θ), represent conics with a vertical directrix (i.e. x = ±d) and axis of symmetry parallel to the x-axis. The equations r(θ) = ed 1 ± e sin(θ), represent conics with a horizontal directrix (i.e. y = ±d) and axis of symmetry parallel to the y-axis. Chapter 7 Determinants ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • define and calculate the determinant of an n × n matrix; • cite and use the basic properties of a determinant; • cite and use properties related to the determinant and basic row/column operations; • cite and use properties of determinants of elementary matrices; • cite and use properties related to the determinant and the inverse of a matrix; • define and calculate the (i, j)-minor and (i, j)-cofactor of a matrix; • define and calculate cofactor and adjoint matrices; • relate the determinant of a square matrix to the existence of inverse matrices and solvability of linear systems of equations; • solve systems of simultaneous linear equations using Cramer’s rule; and • prove a selection of theorems and corollaries in the notes. 7.1 Introduction We have defined matrices and the basic operations involving matrices in Chapter 3. Here, we introduce the determinant of a square matrix. The determinant of a square matrix 115 116 CHAPTER 7. DETERMINANTS is of particular importance in determining whether or not matrices are invertible and can be used to calculate the inverse of a square matrix (if it exists). The definition of a determinant also allows the determination of closed-form representa- tions for the solution to consistent systems of n equations in n unknowns (which have a unique solution). Hence, the determinant of a matrix contains fundamental information about square matrices. Recall that the set of all m × n matrices with real entries is denoted by Mmn(R). In this chapter, we will primarily consider square matrices, i.e. n × n matrices. The set of all n × n matrices with real entries is written as Mnn(R) and we call the elements of Mnn(R) square n × n matrices. Matrices will be referred to as A or [aij] throughout this chapter. As motivation consider the homogeneous system of linear equations a11x1 + a12x2 + a13x3 = 0 a21x1 + a22x2 + a23x3 = 0 a31x1 + a32x2 + a33x3 = 0. (7.1) Then we have seen in Theorem 5.5 that if the matrix A =    a11 a12 a13 a21 a22 a23 a31 a32 a33    is invertible then (7.1) has a unique solution. Since we know (x1, x2, x3) = (0, 0, 0) is a solution, if there is another solution say (x1, x2, x3) = (c1, c2, c3) at least one of c1, c2 and c3 must be non-zero and A must not be invertible. We may change notation and suppose that c1 ̸= 0. Then we have a11c1 + a12c2 + a13c3 = 0 a21c1 + a22c2 + a23c3 = 0 a31c1 + a32c2 + a33c3 = 0. (7.2) Also assume that not all the coefficients a13, a23 and a33 are 0 as otherwise we are in a degenerate case. From the first equation we obtain a13c3 = −a11c1 − a12c2. (7.3) Multiplying equation two and three in 7.2 by a13 and substituting we get 0 = a21a13c1 + a22a13c2 − a23(a11c1 + a12c2) = a21a13c1 + a22a13c2 − a23a11c1 − a23a12c2 = (a21a13 − a23a11)c1 + (a22a13 − a23a12)c2 (7.4) and 0 = a31a13c1 + a32a13c2 − a33(a11c1 + a12c2) = a31a13c1 + a32a13c2 − a33a11c1 − a33a12c2 = (a31a13 − a33a11)c1 + (a32a13 − a33a12)c2. (7.5) 7.2. AN EXCURSION IN TO GROUP THEORY 117 Hence we may multiply 7.5 by (a22a13 − a23a12) and substitute 7.4 to eliminate c2 and obtain 0 = (a22a13 − a23a12)(a31a13 − a33a11)c1 − (a32a13 − a33a12)(a21a13 − a23a11)c1 = (a22a13a31a13 − a23a12a31a13 − a22a13a33a11 + a23a12a33a11 −a32a13a21a13 + a32a13a23a11 + a33a12a21a13 − a33a12a23a11)c1 = a13(a22a13a31 − a23a12a31 − a22a33a11 − a32a13a21 + a32a23a11 + a33a12a21)c1. Since a13 ̸= 0 ̸= c1 we deduce that the long expression in the middle is zero. That is a11(a22a33 − a32a23) − a12(a33a21 − a23a31) + a13(a32a21 − a22a31) = 0. We define the determinant of the matrix A to be a11(a22a33 − a32a23) − a12(a33a21 − a23a31) + a13(a32a21 − a22a31) and the observation we make is that the matrix A is not invertible implies the determinant of A is zero. You’ll notice that each triple product in the formula for the determinant has one entry from each row and each column of A and there’s something “odd” about the appearance of negative numbers in the description. The objective of this chapter is to make a more precise statement for all square matrices. 7.2 An excursion in to group theory To define the determinant of an n × n matrix we require the following two definitions. 1 Definition 7.1. A permutation of {1, . . . , n} is a bijective function σ : {1, . . . , n} → {1, . . . , n}. The set of all permutations of {1, . . . , n} is denoted by Sn and is called the sym- metric group on {1, . . . , n} . For instance σ : {1, 2, 3} → {1, 2, 3} given by σ(1) = 2, σ(2) = 1 and σ(3) = 3 is a permutation of {1, 2, 3}. We can represent a permutation σ : {1, . . . , n} → {1, . . . , n} in two-row notation or sequence notation, denoted by σ = ( 1 2 · · · n − 1 n σ(1) σ(2) · · · σ(n − 1) σ(n) ) or σ = [σ(1), σ(2), . . . , σ(n − 1), σ(n)]. (7.6) 1Recall that a bijective function (also called a bijection) is a function that is both surjective and injective, i.e. 1-1 (see [15] for details). 118 CHAPTER 7. DETERMINANTS We can also represent permutations in cycle notation (which are considered in 1AC [7]), albeit we do not do so in what follows. Example 79: The permutation σ : {1, 2, 3, 4} → {1, 2, 3, 4} is equivalently written as σ(x) =    2 if x = 1 4 if x = 2 3 if x = 3 1 if x = 4 ⇐⇒ σ = ( 1 2 3 4 2 4 3 1 ) ⇐⇒ σ = [2, 4, 3, 1]. Definition 7.2. The identity permutation in Sn is given by σId = [1, 2, . . . , n]. Additionally, because σ ∈ Sn is a bijection, σ has an inverse σ−1 ∈ Sn which satisfies σ(σ−1(i)) = i = σ−1(σ(i)) ∀i ∈ {1, . . . , n}, or equivalently, σ−1 ◦ σ = σId = σ ◦ σ−1. To find σ−1 given σ, we can write σ in 2 row notation, swap the bottom and top rows, and then swap columns so that the top row is in the form 1 2 3 . . . n i.e. σ = ( 1 2 · · · n σ(1) σ(2) · · · σ(n) ) → (σ(1) σ(2) · · · σ(n) 1 2 · · · n ) → ( 1 2 · · · n σ−1(1) σ−1(2) · · · σ−1(n) ) = σ−1. (7.7) Example 80: Find the inverse of [1, 3, 4, 2]. Write (1 2 3 4 1 3 4 2 ) swap the top and bottom rows (1 3 4 2 1 2 3 4 ) . Reorder the columns (1 2 3 4 1 4 2 3 ) . 7.2. AN EXCURSION IN TO GROUP THEORY 119 Definition 7.3. Suppose that n ≥ 1 is a natural number. 1. Suppose that σ : {1, . . . , n} → {1, . . . , n} is a permutation in Sn. If i, j ∈ {1, . . . , n} with i < j and σ(i) > σ(j), then the pair (i, j) is called an inver- sion of σ. 2. Define the function N : Sn → N0 by setting N (σ) = |{(i, j) : (i, j) is an inversion of σ}|. So N (σ) is the number of inversions in the permutation σ. 3. A permutation σ ∈ Sn is an odd permutation if and only if N (σ) is odd. A permutation which is not odd is even. Example 81: The identity permutation in Sn is given by σId = [1, 2, . . . , n]. It is the only permutation in Sn which has no inversions i.e. N (σ) = 0 if and only if σ = σId. Example 82: For σ = [2, 4, 3, 1] ∈ S4, there are 6 pairs (i, j) to check: 1 < 2, 1 < 3, 1 < 4, 2 < 3, 2 < 4 and 3 < 4. We have (i, j) σ(i) σ(j) inversion (1, 2) 2 4 no (1, 3) 2 3 no (1, 4) 2 1 yes (2, 3) 4 3 yes (2, 4) 4 1 yes (3, 4) 3 1 yes Therefore, N (σ) = |{(1, 4), (2, 3), (2, 4), (3, 4)}| = 4. Hence [2, 4, 3, 1] is an even permutation. Moreover, given a permutation σ ∈ Sn, if you swap two adjacent terms to get a permuta- tion ˜σ, i.e. σ = ( · · · i i + 1 · · · · · · σ(i) σ(i + 1) · · · ) and ˜σ = ( · · · i i + 1 · · · · · · σ(i + 1) σ(i) · · · ) , then N (˜σ) =    N (σ) + 1 if σ(i + 1) > σ(i) ( if (i, i + 1) is not an inversion) N (σ) − 1 if σ(i + 1) < σ(i) ( if (i, i + 1) is an inversion). (7.8) Using (7.8) we have the main result that we need about permutations. 120 CHAPTER 7. DETERMINANTS Proposition 7.4. Let σ ∈ Sn. Then N (σ) = N (σ−1). Proof. For each σ ∈ Sn, let Iσ be the set of inversions of σ. Then Iσ = {(i, j) : i < j and σ(i) > σ(j)} . So |Iσ| = N (σ). For (i, j) ∈ Iσ, we have σ(j) < σ(i) and σ−1(σ(j)) = j > i = σ−1(σ(i)). (7.9) Therefore (σ(j), σ(i)) ∈ Iσ−1 for each (i, j) ∈ Iσ. As σ is a bijection, this shows that N (σ) = |Iσ| = |{(σ(j), σ(i)) : (i, j) ∈ Iσ}| ≤ |Iσ−1| = N (σ−1) for all σ ∈ Sn. Hence, as σ−1 ∈ Sn, we have N (σ−1) ≤ N ((σ−1)−1) = N (σ) and thus N (σ) = N (σ−1). Example 83: Suppose that σ ∈ S3. Let A = (aij) ∈ M33(R). Show that aσ(1),1aσ(2),2aσ(3),3 = a1,σ−1(1)a2,σ−1(2)a3,σ−1(3). We can certainly order numbers being multiplied together in the expression aσ(1),1aσ(2),2aσ(3),3 in any order we want as R is commutative for multiplication. So we choose to order them with respect to rows rather than columns. Then the first term is a1,x where x is such that σ(x) = 1. So x = σ−1(1). That is the first term is a1,σ−1(1). Arguing in this way we get aσ(1),1aσ(2),2aσ(3),3 = a1,σ−1(1)a2,σ−1(2)a3,σ−1(3). 7.3 Definition of the Determinant After our excursion in to group theory, we now define the determinant of an n × n matrix as follows. Definition 7.5. Let A ∈ Mnn(R). The determinant of A = [aij], denoted by det(A) or |A| is given by ∑ σ∈Sn(−1)N (σ) n∏ i=1 aσ(i)i. 7.3. DEFINITION OF THE DETERMINANT 121 In Definition 7.5, note the following points: • the sum is over all permutations of {1, . . . , n} (hence there are n! terms in the sum); • for each fixed σ in the sum, the aij terms in the product are in column i and row σ(i) for i = 1, . . . , n; • for each fixed σ in the sum, the product of n terms contains exactly one entry from each column and each row of A (since σ is a bijective function); • the determinant of a matrix is a real number (more generally is an element of the field where the matrix entries come from); • the determinant of A only exists if A is a square matrix, i.e. an n × n matrix; and • det : Mnn(R) → R is a function. Example 84: Consider A = [aij] ∈ M22(R). Since S2 contains only 2 permutations given by σ1 = [1, 2] and σ2 = [2, 1], with N (σ1) = 0 and N (σ2) = 1, it follows that ∣ ∣ ∣ ∣ ∣ a11 a12 a21 a22 ∣ ∣ ∣ ∣ ∣ = ∑ σ∈S2(−1) N (σ) 2∏ i=1 aσ(i)i = (−1) N ([1,2])a11a22 + (−1) N ([2,1])a21a12 = (−1) 0a11a22 + (−1) 1a21a12 = a11a22 − a12a21. Moreover, consider A = [aij] ∈ M33(R). Since S3 contains only 6 permutations given by σ1 = [1, 2, 3], σ2 = [1, 3, 2], σ3 = [2, 1, 3], σ4 = [2, 3, 1], σ5 = [3, 1, 2] and σ6 = [3, 2, 1], with N (σ1) = 0, N (σ2) = 1, N (σ3) = 1, N (σ4) = 2, N (σ5) = 2 and N (σ6) = 3, it follows that ∣ ∣ ∣ ∣ ∣ ∣ ∣ a11 a12 a13 a21 a22 a23 a31 a32 a33 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∑ σ∈S3(−1) N (σ) 3∏ i=1 aσ(i)i = (−1) N ([1,2,3])a11a22a33 + (−1) N ([1,3,2])a11a32a23 + (−1) N ([2,1,3])a21a12a33 + (−1) N ([2,3,1])a21a32a13 + (−1)N ([3,1,2])a31a12a23 + (−1)N ([3,2,1])a31a22a13 = (−1) 0a11a22a33 + (−1)1a11a32a23 + (−1)1a21a12a33 + (−1) 2a21a32a13 122 CHAPTER 7. DETERMINANTS + (−1) 2a31a12a23 + (−1)3a31a22a13 = a11(a22a33 − a32a23) − a12(a21a33 − a23a31) + a13(a21a32 − a22a31). Note that this is consistent with Examples 12 and 14 from Chapter 1. Example 85: Suppose that A is a 13 × 13 with random integer entries between 1 and 10. Assume that in your head you can multiply 13 numbers between 1 and 10 together in 1 second and add numbers together in no time at all. Then to calculate the determinant of A using Definition 7.5 would take you approximately 197 Years, 5 Months and 2 Weeks. Your great, great, great, great grandchildren would have to inherit the exam question. We have to do better that this! Remark 7.6. Geometrically determinants are related to volumes. We remarked in Chapter 1 that the area of a parallelepiped is given by a scalar triple product and we saw that was described as a determinant in Example 16 in Chapter 1. 7.4 The transpose matrix and its determinant We encountered the transpose of a matrix in the practice questions. We repeat its definition here: Definition 7.7. The transpose matrix, A T = [bij] ∈ Mnm(R), of A = [aij] ∈ Mmn(R) is the matrix with bij = aji for i = 1, . . . , m and j = 1, . . . , n. Notice that in the above example A has shape n × m and AT has shape m × n. Example 86: Suppose that A = (1 2 3 4 6 7 8 6 ) . Then A T =      1 6 2 7 3 8 4 6      . A property illustrated in one of the exercises was: Theorem 7.8. Given matrices A, B ∈ Mnn(R), then (A · B) T = B T · A T . 7.4. THE TRANSPOSE MATRIX AND ITS DETERMINANT 123 Proof. Write A = [aij], B = [bij], C = [cij] = (A · B) T and D = [dij] = A · B and E = [eij] = B T · A T . Then cij = dji = n∑ k=1 ajkbki On the other hand, eij = n∑ k=1 bkiajk = n∑ k=1 ajkbki. Hence the matrices C and E are equal. Using mathematical induction, one can easily deduce the following corollary Corollary 7.9. Given matrices Ai ∈ Mnn(R), with 1 ≤ i ≤ k, then (A1 · A2 · . . . · Ak−1 · Ak) T = A T k · A T k−1 · . . . · A T 2 · A T 1 . Proof. Use Theorem 7.8 and the principle of mathematical induction. Importantly, we can determine the following result concerning the determinant of the transpose of a square matrix. Theorem 7.10. Let A ∈ Mnn(R). Then det(A) = det(AT ). Proof. We start with two observations: First, because every element of Sn has a unique inverse Sn = {σ : σ ∈ Sn} = {σ−1 | σ ∈ Sn}. (7.10) Let θ ∈ Sn. Then n∏ i=1 aθ(i),i = n∏ i=1 ai,θ−1(i). (7.11) Hence using Proposition 7.4 yields det(A) = ∑ σ∈Sn(−1) N (σ) n∏ i=1 aσ(i),i by Definition 7.5 = ∑ σ−1∈Sn(−1)N (σ−1) n∏ i=1 aσ−1(i),i by (7.10) 124 CHAPTER 7. DETERMINANTS = ∑ σ−1∈Sn(−1)N (σ) n∏ i=1 ai,σ(i) by Proposition 7.4 and (7.11) = ∑ σ∈Sn(−1) N (σ) n∏ i=1 ai,σ(i) by (7.10) = det(AT ). 7.5 Determinants and row/column operations Theorem 7.11. Suppose that A = [aij] ∈ Mnn(R), with n ≥ 2. Assume that A has two equal columns or two equal rows. Then det(A) = 0. Proof. Suppose that columns r and s of A = [aij] are equal and assume without loss of generality that r < s. This means that ai,r = ai,s for all 1 ≤ i ≤ n. For σ ∈ Sn write σ = ( 1 2 . . . r . . . s . . . n σ(1) σ(2) . . . σ(r) . . . σ(s) . . . σ(n) ) and define σ = ( 1 2 . . . r . . . s . . . n σ(1) σ(2) . . . σ(s) . . . σ(r) . . . σ(n) ) . Then N (σ) = N (σ) ± 1 by repeated use of 7.8 and so (−1)N (σ) + (−1) N (σ) = 0. (7.12) Indeed, we transform σ to σ. Let k = s − r. Swap σ(r) and σ(r + 1) on the bottom row of σ. ( 1 2 . . . r r + 1 . . . s . . . n σ(1) σ(2) . . . σ(r + 1) σ(r) . . . σ(s) . . . σ(n) ) Now swap σ(r) and σ(r + 2) and so on until we swap σ(r) and σ(s). This requires k swaps. We have ( 1 2 . . . r r + 1 . . . s − 2 s − 1 s . . . n σ(1) σ(2) . . . σ(r + 1) σ(r + 2) . . . σ(s − 1) σ(s) σ(r) . . . σ(n) ) Now we do it in reverse. Swap σ(s) and σ(s − 1) on the bottom row and continue until we swap σ(s) and σ(r + 1). This requires k − 1 swaps. Once we have done this, we have 7.5. DETERMINANTS AND ROW/COLUMN OPERATIONS 125 obtained σ. In total, we have performed 2k − 1 swaps of adjacent images and 2k − 1 is odd. Thus N (σ) and N (σ) are not of the same parity by 7.8. Hence 7.12 holds. Since columns r and s are the same and using the definition of σ we get, aσ(r),r = aσ(r),s = aσ(s),s, aσ(s),s = aσ(s),r = aσ(r),r and for all i ̸∈ {r, s}, aσ(i),i = aσ(i),i. We conclude that n∏ i=1 aσ(i),i = n∏ i=1 aσ(i),i and so (−1) N (σ) n∏ i=1 aσ(i),i + (−1) N (σ) n∏ i=1 aσ(i),i = 0. Hence by pairing σ with σ, we obtain det(A) = 0. The result for matrices with two equal rows follows as det(AT ) = det(A) by Theorem 7.10. This means that we can always transform a matrix with two equal rows into one with two equal columns and keep the determinant unchanged. Theorem 7.12. For any matrix A = [aij] ∈ Mnn(R), with n ≥ 2, if the matrix P is obtained from A by multiplying column j1 (or row i1) of A by λ ∈ R, then det(P) = λ det(A). Proof. Observe that for any σ ∈ Sn that ∏n i=1 pσ(i)i contains exactly 1 entry from column j1 of P. Since pij =   aij if j ̸= j1 λaij if j = j1 , for i, j = 1, . . . , n, it follows that |P| = ∑ σ∈Sn(−1) N (σ) n∏ i=1 pσ(i)i = ∑ σ∈Sn(−1) N (σ)λ ( n∏ i=1 aσ(i)i ) = λ ∑ σ∈Sn(−1) N (σ) n∏ i=1 aσ(i)i 126 CHAPTER 7. DETERMINANTS = λ|A|. The result for multiplication of row i1 by λ follows from the argument above and Theorem 7.10. Corollary 7.13. If the matrix A = [aij] ∈ Mnn(R) has a row of zeros or a column of zeros, then det(A) = 0. Proof. Because of Theorem 7.10, it suffices to prove the result for the case when A has a row of zeros. Suppose that the jth row of A consists of zeros. Let λ ∈ R be such that λ ̸= 1. Then multiplying row j be λ gives us a new matrix P and of course we know that P = A. By Theorem 7.12 we have |A| = λ|P| = λ|A|. Since λ ̸= 1, we conclude that |A| = 0. Corollary 7.14. Consider the matrix A = [aij] ∈ Mnn(R). Then det(λA) = λ n det(A). Proof. Starting with A we multiply the first row by λ and obtain a matrix A1 with determinant λ det(A) by Theorem 7.12. Now multiply row 2 of A1 by λ to obtain a matrix A2 with determinant λ 2 det(A). After n steps, we have a matrix An with det(An) = λ n det(A). Since An = λA, we have proved the result. We can also prove the result directly as follows: observe that |λA| = ∑ σ∈Sn(−1) N (σ) n∏ i=1 (λaσ(i)i) = ∑ σ∈Sn(−1) N (σ) ( λ n n∏ i=1 aσ(i)i ) = λ n ∑ σ∈Sn(−1) N (σ) n∏ i=1 aσ(i)i = λ n|A|, as required. Corollary 7.15. Consider the matrix A = [aij] ∈ Mnn(R). Then, det(−A) = (−1) n det(A). 7.5. DETERMINANTS AND ROW/COLUMN OPERATIONS 127 Proof. Follows from Corollary 7.14 with λ = −1. Theorem 7.16. Consider A = [aij] ∈ Mnn(R), with n ≥ 2 and the matrix P = [pij] which is obtained from A by adding λ times column j1 of A to column j2 of A (or λ times row i1 of A to row i2 of A), where λ ∈ R. Then det(P) = det(A). Proof. Since pij =   aij if j ̸= j2 aij + λaij1 if j = j2, we have |P| = ∑ σ∈Sn(−1) N (σ)   ∏ i∈{1,...,n}\\{j2} aσ(i)i   (aσ(j2)j2 + λaσ(j2)j1) =   ∑ σ∈Sn(−1)N (σ) n∏ i=1 aσ(i)i   + λ   ∑ σ∈Sn(−1)N (σ)   ∏ i∈{1,...,n}\\{j2} aσ(i)i   aσ(j2)j1   = |A| + λ   ∑ σ∈Sn(−1)N (σ)   ∏ i∈{1,...,n}\\{j2} aσ(i)i   aσ(j2)j1   . (7.13) Consider the matrix Q with column j1 equal to column j2, given by qij =   aij if j ̸= j2 aij1 if j = j2. Then, det(Q) = 0 by Theorem 7.11. Hence 0 = |Q| = ∑ σ∈Sn(−1) N (σ) n∏ i=1 qσ(i)i = ∑ σ∈Sn(−1) N (σ)   ∏ i∈{1,...,n}\\{j2} aσ(i)i   (aσ(j2)j1). (7.14) Substitution of (7.14) into (7.13) yields |P| = |A|. The result for rows follows from the above argument and Theorem 7.10. Theorem 7.17. For any matrix A = [aij] ∈ Mnn(R), with n ≥ 2, if the matrix P = [pij] is obtained from A by swapping columns j1 and j2 (or rows i1 and i2) of A, then det(P) = − det(A). 128 CHAPTER 7. DETERMINANTS Proof. For 1 ≤ k ≤ n, let’s write rk = (ak1 . . . akn) for the kth row of A. Then A =                r1 ... ri ... rj ... rn                . We now use Theorem 7.16 for the second, third and fifth equality and Theorem 7.12 for the second last equality and obtain |A| = ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣                r1 ... ri ... rj ... rn                ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣                r1 ... ri + rj ... rj ... rn                ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣                r1 ... ri + rj ... rj − (ri + rj) ... rn                ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣                r1 ... ri + rj ... −ri ... rn                ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣                r1 ... ri + rj − ri ... −ri ... rn                ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣                r1 ... rj ... −ri ... rn                ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = − ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣                r1 ... rj ... ri ... rn                ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = −|P|. This establishes the result for exchanged rows and using Theorem 7.10, the result follows for columns. 7.6 Row/Column expansion of the determinant Recall that for A = [aij] ∈ Mnn(R) each of the n! terms in the sum |A| = ∑ σ∈Sn(−1)N (σ) n∏ i=1 aσ(i)i has exactly one entry from each row and column of A (in the product). Therefore, we can bring the entry in row one to the front of each product ∏n i=1 aσ(i)i and the group them 7.6. ROW/COLUMN EXPANSION OF THE DETERMINANT 129 by columns to write |A| = n∑ j=1 a1jC1j(A) (7.15) for some cofactor C1j(A) of the term a1j. We say that (7.15) is the expansion of |A| along row 1. The main results in the section give a representation for C1j(A) and demonstrate that |A| can be represented as an expansion along any column or row. To begin, we have Definition 7.18. A submatrix of A ∈ Mmn(R) is obtained from A by deleting ˜m rows from A or ˜n columns from A with 0 ≤ ˜m ≤ m − 1, 0 ≤ ˜n ≤ n − 1 and max{ ˜m, ˜n} ≥ 1. Example 87: Let A =    1 2 3 4 5 6 7 8 9 10 11 12    . By deleting the first row and column we obtain the submatrix ( 6 7 8 10 11 12 ) . Definition 7.19. The (i, j)-minor of a matrix A ∈ Mnn(R), written Mij(A), is the determinant of the (n − 1) × (n − 1)-submatrix of A obtained by deleting row i and column j of A. Example 88: For A =    2 −6 1 3 1 −4 −1 −2 5    find M12(A) and M31(A). We now have Lemma 7.20. Let A = [aij] ∈ Mnn(R) with n ≥ 2. Then |A| = n∑ j=1 a1j(−1) j−1M1j(A). 130 CHAPTER 7. DETERMINANTS Proof. We first find the cofactor of a11 in the expansion (7.15). In the n! terms in sum in |A|, the terms which contain a11 have a permutation that satisfies σ(1) = 1. By considering the map : {σ ∈ Sn : σ(1) = 1} → Sn−1 given by σ(i) = σ(i + 1) − 1 ∀i = 1, . . . , n − 1 (7.16) it follows that is a bijection. Also, since σ(1) = 1, it follows that N (σ) = N (σ) ∀σ ∈ Sn. (7.17) To find the cofactor of a11, we consider the submatrix A = [aij] which is obtained by deleting row 1 and column 1 from A, and consider a11C11(A) = ∑ σ∈Sn σ(1)=1 (−1) N (σ) n∏ i=1 aσ(i)i = a11 ∑ σ∈Sn σ(1)=1 (−1)N (σ) n∏ i=2 aσ(i)i = a11 ∑ σ∈Sn−1(−1) N (σ) n−1∏ i=1 aσ(i)i = a11|A| = a11M11(A). (7.18) To determine C1j(A) we essentially repeat the argument above. In the n! terms in sum in |A|, the terms which contain a1j have a permutation that satisfies σ(j) = 1. Similarly to (7.16), we consider the bijection : {σ ∈ Sn : σ(j) = 1} → Sn−1 given by σ(i) =    σ(i) − 1 i < j σ(i + 1) − 1 i ≥ j (7.19) and note that N (σ) = N (σ) + (j − 1). The j − 1 appears since there are j − 1 inversions present in σ which take into account the fact that σ(j) = 1, which are not present in σ. Indeed (ℓ, j) is an inversion for σ for all 1 ≤ ℓ < j. Therefore, by considering the submatrix A = [aij] obtained from A by deleting row 1 and column j, we have a1jC1j(A) = ∑ σ∈Sn σ(j)=1 (−1) N (σ) n∏ i=1 aσ(i)i 7.6. ROW/COLUMN EXPANSION OF THE DETERMINANT 131 = a1j ∑ σ∈Sn σ(j)=1 (−1) N (σ) ∏ 1≤i≤n i̸=j aσ(i)i = a1j(−1) j−1 ∑ σ∈Sn−1(−1) N (σ) n−1∏ i=1 aσ(i)i = a1j(−1) j−1|A| = a1jM1j(A). (7.20) Hence, via (7.18) and (7.20), we have |A| = n∑ j=1 a1j(−1) j−1M1j(A), as required. Definition 7.21. The (i, j)-cofactor of a matrix A ∈ Mnn(R), written as Cij(A), is defined as Cij(A) = (−1) i+jMij(A). An alternative reference to the (i, j)-cofactor of A is the term “cofactor of the (i, j) entry, aij, of A\" and denoted as Aij. The number (−1) i+j, which is equal to 1 or -1 when i + j is even or odd respectively, is called the sign of the (i, j) position. In a square matrix, the sign corresponding to the (i, j) position is as follows:          + − + − · · · − + − + · · · + − + − · · · − + − + · · · ... ... ... ... . . .          . Example 89: For A =    2 −6 1 3 1 −4 −1 −2 5    find C12(A) and C31(A). Recall Example 88. One can show that the determinant can also be found in terms of the entries of any row and their cofactors or alternatively the entries of any column and their cofactors. 132 CHAPTER 7. DETERMINANTS Theorem 7.22. For any matrix A = [aij] ∈ Mnn(R), det(A) = n∑ j=1 aijCij(A) = ai1Ci1(A) + ai2Ci2(A) + . . . + ainCin(A), for any i such that 1 ≤ i ≤ n and det(A) = n∑ i=1 aijCij(A) = a1jC1j(A) + a2jC2j(A) + . . . + anjCnj(A), for any j such that 1 ≤ j ≤ n. Sketch of the proof: Suppose we consider an expansion of row i. Then by swapping successive rows, we can define a new matrix B with row i of A in row 1 and every other row in the previous order. This requires i − 1 swaps of successive rows. Using Lemma 7.20 (and following steps in its proof) it follows that |A| = (−1) i−1|B| = (−1) i−1   n∑ j=1 aij(−1) j−1Mij(A)   = n∑ j=1 aij(−1) i+jMij(A) = n∑ j=1 aijCij(A). (7.21) The result for column expansions follows from (7.21) with |AT | = |A|. The sum of products of entries with their cofactor is often referred to as an expansion of det(A) in terms of entries of row i (or column j) and their cofactors. This means we can choose which row or column to use for the expansion needed to calculate the determinant. In practice, this means we can make use of the possible presence of zero values in a row or column. Example 90: Calculate |A| when A =    2 5 0 35 41 13 4 11 0    . Corollary 7.23. If In is the identity matrix in Mnn(R), then det(In) = 1 ∀n ∈ N. Proof. Use induction on n together with the first row expansion of det(In). 7.7. TRIANGULAR MATRICES 133 7.7 Triangular matrices There are a number of corollaries related to triangular matrices that follow from Theorem 7.22. Corollary 7.24. Let n ∈ N with n ≥ 2. If A = [aij] ∈ Mnn(R) is an upper triangular matrix, i.e. aij = 0 when i > j, then det(A) = n∏ i=1 aii = a11a22 . . . ann. If A = [aij] ∈ Mnn(R) is a lower triangular matrix, i.e. aij = 0 when i < j, then det(A) = a11a22 . . . ann. Proof. Consider the statement P (n) for n ∈ N \\ {1} given by: If A = [aij] ∈ Mnn(R) is an upper triangular matrix then det(A) = a11a22 . . . ann. (7.22) P (2) is true since ∣ ∣ ∣ ∣ ∣a11 a12 0 a22 ∣ ∣ ∣ ∣ ∣ = a11a22 − 0a12 = a11a22. (7.23) Now, assume P (k) is true for some k ∈ N \\ {1}, i.e. for any upper triangular matrix A ∈ Mkk(R), we have |A| = ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ a11 a12 a13 · · · a1k 0 a22 a23 · · · a2k 0 0 a33 · · · a3k ... ... ... . . . ... 0 0 0 · · · akk ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = a11a22 . . . akk. (7.24) By expanding along row k + 1, we have, ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ a11 a12 a13 · · · a1(k+1) 0 a22 a23 · · · a2(k+1) 0 0 a33 · · · a3(k+1) ... ... ... . . . ... 0 0 0 · · · a(k+1)(k+1) ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = a(k+1)(k+1)(−1) k+1+k+1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ a11 a12 a13 · · · a1k 0 a22 a23 · · · a2k 0 0 a33 · · · a3k ... ... ... . . . ... 0 0 0 · · · akk ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = a(k+1)(k+1)a11a22a33 · · · akk. 134 CHAPTER 7. DETERMINANTS and hence P (k) is true =⇒ P (k + 1) is true for all k ∈ N \\ {1}. From the principle of mathematical induction, (Theorem B.2) we conclude that P (n) given by (7.22) is true for all n ∈ N \\ {1}, as required. The proof for lower triangular matrices follows from the result for upper triangular ma- trices and Theorem 7.10 (since the transpose of a lower triangular matrix is an upper triangular matrix). Corollary 7.25. If D = [dij] ∈ Mnn(R) is a diagonal matrix, i.e. aij = 0 when i ̸= j, then det(D) = n∏ i=1 dii = d11d22 . . . dnn. Proof. Since a diagonal matrix is a special case of both an upper triangular matrix and a lower triangular matrix, the result follows from the Corollary 7.24. Example 91: Find the determinant of the following matrices: A1 =      2 5 0 7 0 5 13 23 0 0 −2 37 0 0 0 1      , A2 =      2 0 0 0 3 11 0 0 9 −5 −3 0 7 −8 11 −1      and A3 =      2 5 0 −13 35 41 0 29 4 11 0 −17 11 −9 0 89      . 7.8 Calculating determinants The results in the previous sections will be useful when calculating determinants by hand or by computer. In particular: • We can swap two rows or two columns of a matrix A to obtain matrix B, but we must change the sign of the determinant, i.e. |B| = −|A|. • We can multiply a single row (or a single column) of A with a real number λ to obtain B but we must change the determinant by multiplying it by the same number i.e. |B| = λ|A|. • We can add the multiple of a row of A to another row of A (or add the multiple of a column of A to another column of A) without changing the value of the determinant. • To calculate the determinant of A, we can use row (column) operations to con- vert the matrix to a triangular matrix which has a determinant which is given by Corollary 7.24. 7.9. DETERMINANTS OF ELEMENTARY MATRICES 135 Example 92: Find the determinant of the following matrices: A1 =      2 5 0 7 0 0 −2 37 0 5 13 23 0 0 0 1      , A2 =      2 3 5 4 0 1 10 −1 1 −2 15 0 0 0 −5 −1      and A3 =      2 5 9 −13 3 41 47 29 4 11 19 −17 11 −9 13 89      . In more general cases, we may need to expand matrices along a row or column, in terms of determinants of matrices of smaller sizes until we are left with determinants of 2 × 2 matrices. Example 93: Calculate ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 0 −1 3 4 1 3 5 2 2 1 9 6 3 2 4 6 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ . You are likely to encounter the following type of problem frequently later in your degree programme. Example 94: Find all real values of λ, if any, for which |A − λI3| = 0, with A =   2 2 1 1 3 1 1 2 2    . The polynomial in λ given by |A − λI3| = 0 is referred to as the characteristic poly- nomial of A. The zeros of this polynomial are referred to as the eigenvalues of A. 7.9 Determinants of elementary matrices Since row operations can be achieved by multiplying a matrix with an appropriate ele- mentary matrix on the left, we should deduce the determinants of elementary matrices. Lemma 7.26. Let E(i,j) ∈ Mnn(R) be the elementary matrix corresponding to the swapping of rows i and j. Then, det(E(i,j)) = −1. Proof. E(i,j) is obtained from the identity matrix by swapping rows i and j, hence, via Theorem 7.17 and Corollary 7.23, det(E(i,j)) = − det(In) = −1. 136 CHAPTER 7. DETERMINANTS Lemma 7.27. Let E[i,λ] ∈ Mnn(R) be the elementary matrix corresponding to the multiplication of row i with λ ∈ R \\ {0}. Then, det(E[i,λ]) = λ. Proof. E[i,λ] is obtained from the identity matrix by multiplying row i by λ, hence, via Theorem 7.12 and Corollary 7.23, det(E[i,λ]) = λ det(In) = λ. Lemma 7.28. Let E(i,j,λ) ∈ Mnn(R) be the elementary matrix corresponding to adding λ times row j to row i, where λ is non-zero real number. Then, det(E(i,j,λ)) = 1. Proof. E(i,j,λ) is obtained from the identity matrix by multiplying row j by λ and then adding this result to row i, hence via Theorem 7.16 and Corollary 7.23, det(E(i,j,λ)) = det(In) = 1. Notably, elementary matrices have non-zero determinants. Example 95: Find the determinants of the following matrices defined by elementary row operations: E1 =    1 0 0 0 0 1 0 1 0    , E2 =    1 0 0 0 5 0 0 0 1    and E3 =   1 0 3 0 1 0 0 0 1    Theorem 7.29. Given a matrix A ∈ Mnn(R) and an elementary matrix E ∈ Mnn(R), then det(E · A) = det(E) · det(A). Proof. We split the proof into 3 cases, for each type of elementary row operation. 7.9. DETERMINANTS OF ELEMENTARY MATRICES 137 1. Consider the elementary matrix E(i,j) which corresponds to swapping row i and row j. Then B = E(i,j) · A is the matrix obtained from A by swapping rows i and j. Hence, via Lemma 7.26, det(E(i,j)) = −1 and via Theorem 7.17, det(E(i,j) · A) = det(B) = − det(A) = det(E(i,j)) · det(A). 2. Consider the elementary matrix E[i,λ] which corresponds to multiplying row i with the non-zero real number λ. Then B = E[i,λ] · A is the matrix obtained from A by multiplying row i by λ. Hence, via Lemma 7.27, det(E[i,λ]) = λ and via Theorem 7.12, det(E[i,λ] · A) = det(B) = λ det(A) = det(E[i,λ]) · det(A). 3. Consider the elementary matrix E(i,j,λ) which corresponds to multiplying row j with the non-zero real number λ and adding this result to row i. Then B = Eij(λ) · A is the matrix obtained from A by adding to row i, row j multiplied by λ. Hence, via Lemma 7.28, det(E(i,j,λ)) = 1 and via Theorem 7.16, det(Eij(λ) · A) = det(B) = det(A) = det(E(i,j,λ)) · det(A). Example 96: Evaluate the determinant of E · A where E =    1 0 0 0 1 2 0 0 1    and A =    1 2 −1 −1 1 1 2 0 −1    . Corollary 7.30. Given a matrix A ∈ Mnn(R) and elementary matrices Ei ∈ Mnn(R), with 1 ≤ i ≤ k then det(Ek · Ek−1 · . . . · E1 · A) = det(Ek) · det(Ek−1) . . . · det(E1) · det(A). Proof. Consider the statement P (k) for k ∈ N given by det(Ek · Ek−1 · . . . · E1 · A) = det(Ek) · det(Ek−1) . . . · det(E1) · det(A), (7.25) 138 CHAPTER 7. DETERMINANTS where Ei ∈ Mnn(R) are elementary matrices and A ∈ Mnn(R). Via Theorem 7.29 det(E1 · A) = det(E1) · det(A), and hence, P (1) is true. (7.26) Now assume that the statement P (m) is true, i.e. det(Em · Em−1 · . . . · E1 · A) = det(Em) · det(Em−1) . . . · det(E1) · det(A), (7.27) is true for any elementary matrices Ei for 1 ≤ i ≤ m and A ∈ Mnn(R). Then via Theorem 7.29 and (7.27), det(Em+1 · Em · . . . · E1 · A) = det(Em+1 · ( Em · . . . · E1 · A)) = det(Em+1) · det(Em · . . . · E1 · A) = det(Em+1) · det(Em) · . . . · det(E1) · det(A).(7.28) Therefore, via (7.27) and (7.28) P (m) is true =⇒ P (m + 1) is true (7.29) for all m ∈ N. Finally, via (7.26), (7.29) and the principle of mathematical induction (Theorem B.1), the statement P (k) given by (7.25) is true for all k ∈ N, as required. Corollary 7.31. Given elementary matrices Ei ∈ Mnn(R), with 1 ≤ i ≤ k then det(Ek · Ek−1 · . . . · E1) = det(Ek) · det(Ek−1) · . . . · det(E1). Proof. This follows from Corollary 7.30 by choosing A = In. Corollaries 7.30 and 7.31 are important to allow us to prove that |A · B| = |A||B| for A, B ∈ Mnn(R). 7.10 Two major theorems about determinants We can now generalize the observation we made in the introduction. Theorem 7.32. Suppose that A ∈ Mnn(R). Then 1. A is not invertible if and only if det(A) = 0. 2. A is invertible if and only if det(A) ̸= 0. 7.10. TWO MAJOR THEOREMS ABOUT DETERMINANTS 139 Proof. Via the Gaussian Elimination algorithm there exist elementary row operations which convert A into a matrix which is in reduced echelon form U. In particular, U an upper triangular matrix U i.e. U = Ek · Ek−1 · . . . · E2 · E1 · A. (7.30) Using Corollary 7.30 and (7.30), we have det(U) = det(Ek · Ek−1 · . . . · E2 · E1 · A) = det(Ek) · det(Ek−1) · . . . · det(E2) · det(E1) · det(A). (7.31) Since the determinant of an elementary matrix is either −1, 1 or λ ̸= 0 (follows from Lemmas 7.26-7.28), the equation in (7.31) yields det(A) = 0 if and only if det(U) = 0. We know from the Gaussian Elimination Algorithm that A is not invertible, if and only if there is a row of zeros in the bottom row of U. If U has a row of zeros, then det(U) = 0 by Corollary 7.13. If U does not have a row of zeros, then, as U is in reduced echelon form U = In and so U has determinant 1. It follows that det(U) = 0 if and only if A is invertible. As det(A) = 0 if and only if det(U) = 0 we have proved the result. Theorem 7.33. (Determinant product theorem) Let A, B ∈ Mnn(R). Then, det(A · B) = det(A) · det(B). Proof. We consider two cases, when A is invertible, and when A is not invertible. 1. Suppose that A is invertible. Then Theorem 5.11 implies that A = E1 · · · · · Ek is a product of elementary matrices. Therefore A · B = E1 · E2 · . . . · Ek−1 · Ek · B. Thus Corollary 7.30 det(A · B) = det(E1 · E2 · . . . · Ek−1 · Ek · B) = det(E1) · det(E2) · . . . · det(Ek−1) · det(Ek) · det(B) = det(E1 · E2 · . . . · Ek−1 · Ek) · det(B) = det(A) · det(B). 140 CHAPTER 7. DETERMINANTS 2. When A is not invertible then a sequence of elementary row operations can convert A into an upper triangular matrix with a row of zeros on the bottom row. So, we have U = Ek · Ek−1 · . . . · E2 · E1 · A, which means that A = E−1 1 · E −1 2 · . . . · E−1 k−1 · E−1 k · U. (7.32) Hence, via (7.32), A · B = E −1 1 · E−1 2 · . . . · E −1 k−1 · E−1 k · U · B = E−1 1 · E−1 2 · . . . · E −1 k−1 · E−1 k · C, (7.33) where C = U · B also has a row of zeros on the bottom row. Therefore, (via Corollary 7.13) det(C) = 0, and hence, via Corollary 7.30 and (7.33), det(A · B) = det(E −1 1 · E−1 2 · . . . · E−1 k−1 · E−1 k · C) = det(E−1 1 ) · det(E−1 2 ) · . . . · det(E−1 k−1) · det(E−1 k ) · det(C) = 0. Since A is not invertible, det(A) = 0 so that in this case det(A · B) = det(A) · det(B), which completes the proof. Example 97: Determine det(A · B) with A =    0 1 3 1 0 5 1 1 1    and B =    1 1 0 −3 1 −1 1 1 1    . Corollary 7.34. Let Ai ∈ Mnn(R), with 1 ≤ i ≤ k. Then, det(A1 · A2 · . . . · Ak−1 · Ak) = det(A1) · det(A2) · . . . · det(Ak−1) · det(Ak). Proof. Consider the statement P (k) for k ∈ N \\ {1} given by det(A1 · A2 . . . Ak−1 · Ak) = det(A1) · det(A2) · . . . · det(Ak−1) · det(Ak). (7.34) Using Theorem 7.33, we have det(A1 · A2) = det(A1) · det(A2), 7.10. TWO MAJOR THEOREMS ABOUT DETERMINANTS 141 and hence P (2) is true. (7.35) Now assume that P (m) is true for some m ∈ N \\ {1}, i.e., det(A1 · A2 · . . . · Am−1 · Am) = det(A1) · det(A2) . . . det(Am−1) · det(Am), (7.36) for any Ai ∈ Mnn(R). Then, via Theorem 7.33 and (7.36) it follows that det(A1 · A2 · . . . · Am · Am+1) = det((A1 · A2 · . . . · Am) · Am+1) = det (A1 · A2 · . . . · Am) · det(Am+1) = (det(A1) · det(A2) · . . . · det(Am−1) · det(Am)) · det(Am+1) = det(A1) · det(A2) · . . . · det(Am) · det(Am+1). (7.37) Therefore, it follows from (7.36) and (7.37) that P (m) is true =⇒ P (m + 1) is true (7.38) for all m ∈ N \\ {1}. Therefore, via (7.35) and (7.38), via the principle of mathematical induction (Theorem B.2), it follows that statement P (k) given by (7.34) is true for all k ∈ N \\ {1}, as required. Corollary 7.35. Let A ∈ Mnn(R). Then, for k ∈ N, det(Ak) = det(A) k. Proof. Follows from Corollary 7.34 with Ai = A for 1 ≤ i ≤ k. Example 98: Suppose that A =   0 1 3 1 0 5 1 1 1    . Find det(Ak) for k ∈ N. A very important property links the determinant of a matrix with whether or not a matrix is invertible. Theorem 7.36. A matrix A ∈ Mnn(R) is invertible if and only if det(A) ̸= 0. Furthermore, det(A −1) = 1 det(A). 142 CHAPTER 7. DETERMINANTS Proof. We know from Theorem 7.32 that A is invertible if and only if det(A) ̸= 0. If A is invertible then A−1 exists and A · A−1 = In. Therefore, det(A · A −1) = det(In) = 1. (7.39) Hence, via Theorem 7.33 and (7.39), det(A · A −1) = det(A) · det(A −1) = 1. So det(A) ̸= 0 and det(A −1) = 1 det(A). Corollary 7.37. A homogeneous system A · x = 0 of n linear equations in n unknowns has a non-trivial solution if and only if det(A) = 0. Proof. If det(A) ̸= 0 then via Theorem 7.36, A is invertible and the only possible solution to the homogeneous linear system of equations, is the trivial one. This follows since, x = A−1 · A · x = A−1 · 0 = 0. Therefore, a non-trivial solution to a homogeneous linear system of equations can only exist if det(A) = 0. To show that when det(A) = 0 there exist a non-trivial solution, we examine the outcome of the sequence of elementary row operations transforming A into its echelon form. When det(A) = 0, we can reduce A using ERO to an upper triangular matrix with m ≥ 1 rows of zeros at the bottom. Since the RHS of a homogeneous system of equations only contains zeros, this means that m entries of x remain undetermined by the system of equations, and hence, a non-trivial solution exists. Therefore, a non-trivial solution to A · x = 0 exists if and only if A is not invertible, which is the case if and only if det(A) = 0, as required. 7.11 Cofactor and adjoint matrices Definition 7.38. The cofactor matrix, C(A) = [cij] of A = [aij] ∈ Mnn(R) is the matrix in Mnn(R) with (i, j)-th entry given by the cofactor of aij in A, i.e. cij = Cij(A). 7.11. COFACTOR AND ADJOINT MATRICES 143 Definition 7.39. The adjoint matrix, adj(A) of A = [aij] ∈ Mnn(R) is the transpose of the cofactor matrix of A, i.e. adj(A) = (C(A)) T . Example 99: Determine the cofactor matrix and the adjoint matrix of A =    0 1 2 3 4 5 7 8 10    . Example 100: Determine the cofactor matrix and the adjoint matrix of A = ( a11 a12 a21 a22 ) . Answer: From the definition, it follows that C(A) = ( a22 −a21 −a12 a11 ) adj(A) = ( a22 −a12 −a21 a11 ) . (7.40) Observe that for A ∈ M22(R), A · adj(A) = ( a11 a12 a21 a22 ) · ( a22 −a12 −a21 a11 ) = ( a11a22 + a12(−a21) a11(−a12) + a12a11 a21a22 + a22(−a21) a21(−a12) + a22a11 ) = ( a11a22 − a12a21 0 0 a11a22 − a12a21 ) = (a11a22 − a12a21) ( 1 0 0 1 ) = det(A)I2. Similarly, one finds that adj(A)·A = det(A)I2. In full generality, this type of result allows us to represent A−1 (for invertible A ∈ Mnn(R)) solely in terms of adj(A) and det(A). 144 CHAPTER 7. DETERMINANTS Theorem 7.40. For a matrix A ∈ Mnn(R) , A · adj(A) = adj(A) · A = det(A)In. In particular, for det(A) ̸= 0, A −1 = 1 det(A) adj(A). Proof. 1. First we show that A · adj(A) = det(A)In. If adj(A) = [fij], then fij = Cji(A). Denote Q = A · adj(A) with Q = [qij], then for i = 1, . . . , n, qii = n∑ k=1 aikfki = n∑ k=1 aikCik(A) = det(A). (7.41) Also, for i, j = 1, . . . , n with i ̸= j, we find that qij = n∑ k=1 aikfkj = n∑ k=1 aikCjk(A). (7.42) The RHS of (7.42) is the sum of entries in row i multiplied with the cofactors of entries in row j. Therefore, the RHS of (7.42) can be seen to be the determinant of a matrix where row j has been replaced by row i, i.e. a matrix where row i and j are identical. Such a matrix has determinant equal to 0 (via Corollary ??) and so qij = 0. (7.43) We conclude from (7.43) that A · adj(A) is a diagonal matrix, and via (7.41), all diagonal entries are equal to det(A). Hence, A · adj(A) = det(A)In. 2. To show that adj(A) · A = det(A)In, we follow a similar proof, but using columns of A instead of rows. It then follows that provided det(A) ̸= 0, ( 1 det(A) adj(A) ) · A = A · ( 1 det(A) adj(A) ) = In, 7.12. CRAMER’S RULE 145 and since the inverse matrix of A is unique (see Theorem 4.18), A−1 = 1 det(A) adj(A), as required. Example 101: Using Theorem 7.40 find the inverse (when it exists) of A = ( a11 a12 a21 a22 ) . Answer: Observe that det(A) = a11a22 − a12a21 and via (7.40), adj(A) = ( a22 −a12 −a21 a11 ) . Therefore, it follows from Theorem 7.40 that provided det(A) = a11a22 − a12a21 ̸= 0, then A−1 = 1 a11a22 − a12a21 ( a22 −a12 −a21 a11 ) . (7.44) Example 102: Use (7.44) to find the inverse of A, given by A = ( 5 6 8 7 ) . Theorem 7.40 yields a closed form representation for the inverse of a matrix. It does not generally provide the best method to find the inverse in practice but has significant implications for further theoretical results. 7.12 Cramer’s rule Theorem 7.41. (Cramer’s rule) If A ∈ Mnn(R) is invertible, then the (unique) solution of the system A · x = b of n linear equations in n unknowns, where x =       x1 x2 ... xn       and b =       b1 b2 ... bn       , is given by x1 = det(A1) det(A) , x2 = det(A2) det(A) , . . . , xn = det(An) det(A) . For each k = 1, 2, . . . n, the matrix Ak is obtained from A by replacing the entries in column k of A by the entries in the column vector b. 146 CHAPTER 7. DETERMINANTS Proof. If A is an invertible matrix, then via Theorem ?? the system A·x = b has a unique solution given by x = A−1 · b. Using the formula for the inverse obtained in Theorem 7.40 we find that x = A−1 · b = ( adj(A) det(A) ) · b = 1 det(A) (adj(A) · b) . From Definition 7.39 the adjoint matrix is of the form adj(A) =       C11(A) C21(A) . . . Cn1(A) C12(A) C22(A) . . . Cn2(A) ... ... ... ... C1n(A) C2n(A) . . . Cnn(A)       so that the result for the k-th entry in x is given by xk = 1 det(A) n∑ i=1 Cik(A)bi = 1 det(A) (b1C1k(A) + b2C2k(A) + . . . + bnCnk(A)) . Observe that the sum b1C1k(A) + b2C2k(A) + . . . + bnCnk(A) has the form of an expansion for a determinant, with the difference that the cofactors belong to column k. So this can be seen as the expansion for the determinant (around column k) of the matrix       a11 a12 · · · a1,k−1 b1 a1,k+1 . . . a1n a21 a22 · · · a2,k−1 b2 a2,k+1 . . . a2n ... ... ... ... ... ... ... ... an1 an2 · · · an,k−1 bn an,k+1 . . . ann       This holds for all values of k = 1, . . . , n, as required. Again, this is not the most efficient way to calculate the solution to a system of linear equations by hand (in general), but having a closed form representation for the solution is of significant theoretical importance. Example 103: Solve the system of equations below using Cramer’s rule: 2x1 − 3x2 + 4x3 = 5, 3x1 + 2x2 + x3 = 7, 5x1 + x2 − 2x3 = −3. 7.13. EIGENVALUES OF SQUARE MATRICES 147 7.13 Eigenvalues of square matrices Eigenvalues and eigenvectors of square matrices can be used to infer information about matrices and the objects that they may represent. Eigenvalues and eigenvectors of square matrices will appear frequently in mathematical theory developed in courses later in your degree programme (and highlighted in this course in the practice questions). Definition 7.42. Let A ∈ Mnn(R). The polynomial in λ of degree n given by |A − λIn| = 0, is referred to as the characteristic polynomial of A. The zeros of the character- istic polynomial of A are referred to as the eigenvalues of A. Via Theorem 3.32 and Definition 7.42, it follows that every matrix A ∈ Mnn(R) has m distinct eigenvalues in C (1 ≤ m ≤ n). Note that the eigenvalues of A ∈ Mnn(R) are in C and not necessarily in R. Example 104: Find the characteristic polynomial, and hence, the eigenvalues of the following matrices: A = (2 0 3 3 ) and B = ( 0 −1 1 0 ) . For each real eigenvalue2 of A ∈ Mnn(R), we can also introduce Definition 7.43. Let A ∈ Mnn(R) and suppose that λ ∈ R is an eigenvalue of A. Then a non-zero vector x ∈ Mn1(R) (or vector in R n) is an eigenvector for A corresponding to eigenvalue λ if A · x = λx. Using results in Chapter 9, one can establish that eigenvectors (as described above) exist. Example 105: Find all eigenvectors for: A = ( 2 0 3 3 ) . 2A similar definition can be given for complex eigenvalues. 148 CHAPTER 7. DETERMINANTS Chapter 8 Vector Spaces ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • state and verify the definitions of a vector space; • define and identify subspaces of a vector space; • define linear (in)dependence, a spanning set and a basis, and, apply these concepts on given vector spaces or subspaces and the row space and column space of matrices; • cite and apply given properties relating to linear (in)dependence, span- ning sets and bases; • define and use the dimension of a vector space; • cite and apply given properties related to the dimension of vector spaces and subspaces, including row rank, column rank and rank of matrices; • cite an apply rank-related properties to systems of simultaneous linear equations, determinants and existence of inverse matrices; • define and calculate the coordinates of a vector; and • prove a selection of theorems and corollaries in the notes. [4, p.197-255] contains an alternative presentation of material in this chapter that you may find helpful. 149 150 CHAPTER 8. VECTOR SPACES 8.1 Introduction In this chapter, we introduce important mathematical structures related to sets with a given operation. These are initially formulated in a general way, but we will soon focus on a limited number of examples. It is important to understand that the structures introduced are common to a variety of mathematical entities and that their abstract formulation is a powerful way of developing mathematical theory. One can motivate the theory of vector spaces by looking at systems of linear equations from different viewpoints. First, we presented systems of equations in association with their geometrical interpretation. Namely,    a11x + a12y + a13z = b1 a21x + a22y + a23z = b2 a31x + a32y + a33z = b3, (8.1) can be seen as the representation of the intersection of 3 planes in 3-dimensional space, Π1, Π2 and Π3, with equations Π1 : a11x + a12y + a13z = b1, Π2 : a21x + a22y + a23z = b2, Π3 : a31x + a32y + a33z = b3. A second viewpoint is obtained by writing the system of linear equations in (8.1) in matrix notation, A · x = b, (8.2) with A =    a11 a12 a13 a21 a22 a23 a31 a32 a33    , b =    b1 b2 b3    and x =   x y z    by considering it as an equation within the set of matrices where the objective is to find the 3 × 1 matrix x (or length 3 vector) which satisfies this equation. A third viewpoint is obtained by rewriting the system of linear equations in (8.1) as x    a11 a21 a31    + y   a12 a22 a32    + z    a13 a23 a33    =    b1 b2 b3    , (8.3) and considering it as the problem to identify the coefficients x, y, and z such that the combination of the vectors (or 3 × 1 matrices)   a11 a21 a31    ,    a12 a22 a32    and   a13 a23 a33    , 8.2. DEFINITION OF A VECTOR SPACE 151 yields the vector    b1 b2 b3    . It is the last interpretation, which regards the matrix A as a function mapping vectors to vectors, requires the theory of vector spaces introduced in this chapter. 8.2 Definition of a vector space Until now, we only discussed binary operations, i.e. operations involving two elements from within a given set. But we have already encountered operations which don’t involve two elements from the same set, i.e. in expressions like λ · v, the “product” of a real number with a vector, which results in a vector. Another example is λ · A, the “product” of a real number with a matrix, resulting in a matrix. These are examples of a scalar multiplication. Scalar multiplication typically uses numbers from a field, in the previous examples (R, +, ·). Other fields can also be used, for instance (C, +, ·). Let us now consider a set V with an internal binary operation ⊕ and a scalar multiplication with elements from a field (F, +, ·). Then a vector space is defined a follows: 152 CHAPTER 8. VECTOR SPACES Definition 8.1. A set non-empty V with a binary operation ⊕ and a scalar mul- tiplication ⊙ with elements from a field (F, +, ·) is called a vector space over F when the following properties are satisfied. 1. (V, ⊕) is an abelian group with identity 0; 2. V is closed under the scalar multiplication ⊙: ∀ a ∈ V, ∀ λ ∈ F : λ ⊙ a ∈ V ; 3. Distributivity for scalar multiplication with respect to ⊕ in V : ∀ a, b ∈ V, ∀ λ ∈ F : λ ⊙ (a ⊕ b) = (λ ⊙ a) ⊕ (λ ⊙ b); 4. Distributivity for scalar multiplication with respect to + in F : ∀ a ∈ V, ∀ λ, ν ∈ F : (λ + ν) ⊙ a = (λ ⊙ a) ⊕ (ν ⊙ a); 5. Mixed associativity for · and ⊙: ∀ a ∈ V, ∀ λ, ν ∈ F : λ ⊙ (ν ⊙ a) = (λ · ν) ⊙ a; and 6. The identity of (F \\ {0}, ·) is also the identity for scalar multiplication: ∀ a ∈ V : 1 ⊙ a = a. Note that the definition of a vector space involves four different operations: the internal binary operation ‘ ⊕ ’, often referred to as an addition, the scalar multiplication ‘ ⊙ ’ and the two binary operations of the associated field, ‘ + ’ and ‘ · ’. In the remainder of this course, the symbols ⊕ and ⊙ will be replaced by the more usual + and ·, with the multiplication symbol often omitted. But it should be clear from the context which of the four operations is being referred to. One refers to vector spaces over the field of real numbers, (R, +, ·), as real vector spaces. Let us consider explicitly some examples of real vector spaces. 8.2.1 E2: Vectors in the plane First consider the set vectors in the plane, E2, with the vector addition and scalar mul- tiplication as defined in Chapter 5. We will use the familiar + notation for the vector addition (instead of ⊕), and omit the multiplication sign (⊙) for the scalar multiplication. Then: 8.2. DEFINITION OF A VECTOR SPACE 153 1. (E2, +) is an abelian group, with identity 0: (a) E2 is closed under + since ∀ a, b ∈ E2, a + b ∈ E2; (b) + is associative since ∀ a, b, c ∈ E2, (a + b) + c = a + (b + c) = a + b + c; (c) + has an identity since ∀ a ∈ E2, 0 ∈ E2 and 0 + a = a + 0 = a; (d) Existence of inverse. Since ∀ a ∈ E2, ∃ b ∈ E2 such that a + b = b + a = 0; (e) + is commutative since ∀ a, b ∈ E2, a + b = b + a; 2. E2 is closed under scalar multiplication since ∀ a ∈ E2, ∀ λ ∈ R, λa ∈ E2; 3. Distributivity for scalar multiplication with respect to addition in E2, ∀ a, b ∈ E2, ∀ λ ∈ R, λ(a + b) = (λa) + (λb); 4. Distributivity for scalar multiplication with respect to + in R, ∀ a ∈ E2, ∀ λ, ν ∈ R, (λ + ν)a = (λa) + (νa); 5. Mixed associativity for multiplication in R and scalar multiplication, ∀ a ∈ E2, ∀ λ, ν ∈ R, λ(νa) = (λν)a; and 6. The identity of (R \\ {0}, ·) is also the identity for scalar multiplication, ∀ a ∈ E2, 1a = a. One can also verify that the set of vectors in three dimensions, E3 forms a vector space over the field (R, +, ·). The term vector space derives from the fact that it describes the properties of the set of vectors with addition and scalar multiplication over the field of real numbers. In general, the elements of a vector space will be referred to as vectors. 8.2.2 R3: Points in 3-dimensional space As a second example, consider the set of ordered triples of real numbers, R3. Addition is then defined as (x1, x2, x3) + (y1, y2, y3) = (x1 + y1, x2 + y2, x3 + y3), and scalar multiplication with a real number as λ(x1, x2, x3) = (λx1, λx2, λx3). Then R 3 is a real vector space (in full detail, we mean (R 3, +) with the field (R, +, ·)) since: 154 CHAPTER 8. VECTOR SPACES 1. (R3, +) is an abelian group, with identity (0, 0, 0). This follows from: (a) R 3 is closed under + since ∀ (x1, x2, x3), (y1, y2, y3) ∈ R 3 we have (x1, x2, x3) + (y1, y2, y3) ∈ R 3; (b) + is associative since ∀ (x1, x2, x3), (y1, y2, y3), (z1, z2, z3) ∈ R we have ((x1, x2, x3) + (y1, y2, y3)) + (z1, z2, z3) = (x1, x2, x3) + ((y1, y2, y3) + (z1, z2, z3)) = (x1, x2, x3) + (y1, y2, y3) + (z1, z2, z3); (c) + has an identity since ∀ (x1, x2, x3) ∈ R 3, (0, 0, 0) ∈ R3 we have (0, 0, 0) + (x1, x2, x3) = (x1, x2, x3) + (0, 0, 0) = (x1, x2, x3); (d) Existence of inverse. Since ∀ (x1, x2, x3) ∈ R 3, ∃ (y1, y2, y3) ∈ R 3 such that (x1, x2, x3) + (y1, y2, y3) = (y1, y2, y3) + (x1, x2, x3) = (0, 0, 0); (e) + is commutative since ∀ (x1, x2, x3), (y1, y2, y3) ∈ R3 we have (x1, x2, x3) + (y1, y2, y3) = (y1, y2, y3) + (x1, x2, x3); 2. R 3 is closed under scalar multiplication since ∀ (x1, x2, x3) ∈ R3, ∀ λ ∈ R, we have λ(x1, x2, x3) ∈ R 3; 3. Distributivity for scalar multiplication with respect to addition in R 3. Since ∀ (x1, x2, x3), (y1, y2, y3) ∈ R 3, ∀ λ ∈ R we have λ((x1, x2, x3) + (y1, y2, y3)) = (λ(x1, x2, x3)) + (λ(y1, y2, y3)); 4. Distributivity for scalar multiplication with respect to + in R. Since ∀ (x1, x2, x3) ∈ R3, ∀ λ, ν ∈ R we have (λ + ν)(x1, x2, x3) = (λ(x1, x2, x3)) + (ν(x1, x2, x3)); 5. Mixed associativity for multiplication in R and scalar multiplication. Since ∀ (x1, x2, x3) ∈ R 3, ∀ λ, ν ∈ R we have λ(ν(x1, x2, x3)) = (λν)(x1, x2, x3); and 6. The identity of (R \\ {0}, ·) is also the identity for scalar multiplication. Since ∀ (x1, x2, x3) ∈ R 3 we have 1(x1, x2, x3) = (x1, x2, x3). This result for R 3 can easily be extended to the set of n-tuples (n ∈ N), R n, with elements of the form (x1, x2, . . . , xn). Previously we used calculations in the vector space R 3 to solve problems posed in terms of vectors in three dimensions, i.e. elements of the vector space E3. One can do this because the two sets are both vector spaces with similar properties. The link between a vector, its components and an ordered triple is expressed in the following notation: a = ⃗OA = x1i + x2j + x3k = (x1, x2, x3). 8.3. PROPERTIES OF VECTOR SPACES 155 8.3 Properties of vector spaces We now have introduced an abstract mathematical structure, a vector space, which is characterised by a number of given properties. We have seen examples of vector spaces, and more will be introduced in the practice questions and in later years. We now establish general results which hold in any arbitrary vector space. We have already seen that the identity of a binary operation is unique, hence Lemma 8.2. The zero vector (identity), 0, in a vector space is unique. Proof: From Definition 8.1 point 1, ⊕ is a binary operation with identity 0. Therefore, via Theorem 2.6, 0 is unique, as required. Lemma 8.3. In a real vector space, V , the inverse with respect to addition in V , of a vector v is unique. Proof: Assume that v ∈ V has two inverses (negatives), ˆv and ˜v. We have (ˆv + v) + ˜v = 0 + ˜v = ˜v, and ˆv + (v + ˜v) = ˆv + 0 = ˆv. Hence as + is associative, ˆv = (ˆv + v) + ˜v = ˆv + (v + ˜v) = ˜v, and hence, both inverses are identical. Since v ∈ V has an inverse with respect to vector addition, it follows that the inverse is unique, as required. Lemma 8.4. Consider m vectors in a vector space, v1, v2, . . . , vm ∈ V , with m ≥ 2 and λ ∈ F, then λ(v1 + v2 + . . . + vm) = λv1 + λv2 + . . . + λvm. Proof: We use the principle of mathematical induction. Consider the statement P (m) for m ∈ N \\ {1} given by λ(v1 + v2 + . . . + vm) = λv1 + λv2 + . . . + λvm. (8.4) 156 CHAPTER 8. VECTOR SPACES Note that P (2) is true (8.5) since V is a vector space (see Definition 8.1 point 3). Now assume that P (q) is true for some q ∈ N \\ {1} i.e. λ(v1 + v2 + . . . + vq) = λv1 + λv2 + . . . + λvq (8.6) is true. Then, via (8.5) and (8.6), respectively (and Definition 8.1 point 3) λ(v1 + v2 + . . . + vq+1) = λ((v1 + v2 + . . . + vq) + vq+1) = λ(v1 + v2 + . . . + vq) + λvq+1 = λv1 + λv2 + . . . + λvq + λvq+1. (8.7) So, via (8.6) and (8.7), P (q) is true =⇒ P (q + 1) is true (8.8) for any q ∈ N \\ {1}. Finally, via (8.5) and (8.8), from the principle of mathematical induction (see Theorem B.2), P (m) given by (8.4) is true for all m ∈ N \\ {1}, as required. Similarly, Lemma 8.5. Consider a vector v ∈ V , with V a vector space, and m real numbers, λ1, λ2, . . . , λm ∈ F, with m ≥ 2. Then, (λ1 + λ2 + . . . + λm)v = λ1v + λ2v + . . . + λmv. Proof: This can again be proved by mathematical induction. Consider the statement P (m) for m ∈ N \\ {1}, given by (λ1 + λ2 + . . . + λm)v = λ1v + λ2v + . . . + λmv. (8.9) Note that P (2) is true (8.10) since V is a vector space (via Definition 8.1 point 4). Now assume that P(q) is true for some q ∈ N \\ {1} i.e. (λ1 + λ2 + . . . + λq)v = λ1v + λ2v + . . . + λqv. (8.11) is true. Then, via (8.10) and (8.11), respectively (and Definition 8.1 point 4) (λ1 + λ2 + . . . + λq+1)v = ((λ1 + λ2 + . . . + λq) + λq+1)v = (λ1 + λ2 + . . . + λq)v + λq+1v = λ1v + λ2v + . . . + λqv + λq+1v. (8.12) So, via (8.11) and (8.12), P (q) is true =⇒ P (q + 1) is true (8.13) for any q ∈ N \\ {1}. Finally, via (8.10) and (8.13), from the principle of mathematical induction (Theorem B.2), P (m) given by (8.9) is true for all m ∈ N \\ {1}, as required. 8.3. PROPERTIES OF VECTOR SPACES 157 Theorem 8.6. Assume that V is a vector space F. 1. If v ∈ V and v = v + v, then v = 0. 2. If v ∈ V , then 0v = 0; 3. If λ ∈ F, then λ0 = 0; 4. If λ ∈ F and v ∈ V , then λv = 0 if and only if either λ = 0 or v = 0; and 5. If λ ∈ F and v ∈ V then (−λ)v = λ(−v) = −(λv). Here (−v) denotes the additive inverse element of v. Proof: 1. Since every element in V has an inverse under vector addition, there exists an additive inverse (−v) ∈ V of v. Suppose that v = v + v. Then 0 = v + (−v) = (v + v) + (−v) = v + (v + (−v)) = v + 0 = v. This proves (1). 2. For any v ∈ V , we have 0v = (0 + 0)v = 0v + 0v. Hence 0v = 0 by (1). 3. We have λ0 = λ(0 + 0) = λ0 + λ0. So the result follows from (1). 4. Suppose that λv = 0 and λ ̸= 0. Then, there exists an inverse ν ∈ R of λ (with respect to multiplication in the scalar field) such that λν = 1. Hence, ν(λv) = (νλ)v = 1v = v. Since λv = 0, the left hand side of the equation is the zero vector (via point 3). Therefore, ν(λv) = 0 which implies that v = 0. Alternatively, supposing that λv = 0 and λ = 0 satisfies the conclusion. The if and only if result is completed by points 2 and 3 above. 5. Let λ ∈ R. Note here that −v ∈ V is the additive inverse of v. We have (−λ)v + λv = (−λ + λ)v = 0v = 0 by (2) and so (−λ)v = −(λv). Similarly, λ(−v) + λv = λ(v + (−v)) = λ0 = 0 158 CHAPTER 8. VECTOR SPACES by 3. So λ(−v) = −(λv). It follows that (−λ)v = λ(−v) = −(λv), as required. □ Corollary 8.7. Let V be a vector space over F and v ∈ V . Then (−1)v = −v. Proof: Follows from Theorem 8.6 point 5 with λ = 1. Observe that since (i) vector spaces over F are closed under addition, (ii) there is an identity for addition, i.e. 0, (iii) every element v of a vector space has an inverse −v, we can define a new binary operation, called subtraction as Definition 8.8. A subtraction is a binary operation on the real vector space V defined as follow: u − v = u + (−v) ∀u, v ∈ V with (−v) denoting the inverse of v. Example 106: Simplify the following expressions: 1. 5v − 7w + 3v − 9v + w, 2. 2(2, 4, 5) − 6(1, 3, 5) + (4, 8, 10) + 5(1, 3, 5) − 4(2, 4, 5). 8.4 Subspaces of vector spaces Consider the vector space R 3 of all ordered triples of real numbers. This can be visualised as the set of all coordinates of points in a three-dimensional space, or the components of three-dimensional (position) vectors. Consider the subset U = {(x1, x1, x1) : x1 ∈ R}. Then U ⊂ R 3. Also, we can verify that (U, +) is an abelian group: 8.4. SUBSPACES OF VECTOR SPACES 159 1. U is closed under +: ∀ (x1, x1, x1), (x2, x2, x2) ∈ U : (x1, x1, x1) + (x2, x2, x2) ∈ U ; 2. + is associative: ∀ (x1, x1, x1), (x2, x2, x2), (x3, x3, x3) ∈ R3 : ((x1, x1, x1) + (x2, x2, x2)) + (x3, x3, x3) = (x1, x1, x1) + ((x2, x2, x2) + (x3, x3, x3)) = (x1, x1, x1) + (x2, x2, x2) + (x3, x3, x3); 3. + has an identity: ∀ (x1, x1, x1) ∈ U, (0, 0, 0) ∈ U : (0, 0, 0) + (x1, x1, x1) = (x1, x1, x1) + (0, 0, 0) = (x1, x1, x1); 4. Existence of inverse: ∀ (x1, x1, x1) ∈ U, ∃ (x2, x2, x2) ∈ U : (x1, x1, x1) + (x2, x2, x2) = (x2, x2, x2) + (x1, x1, x1) = (0, 0, 0); 5. + is commutative: ∀ (x1, x1, x1), (x2, x2, x2) ∈ U : (x1, x1, x1) + (x2, x2, x2) = (x2, x2, x2) + (x1, x1, x1). Note that in the five requirements for (U, +) to be an abelian group, associativity and commutativity are obviously satisfied since these properties are satisfied by any set of elements of R 3 operated on by the binary operation +. The fact that U is closed under addition can be derived from the definition of the sum in R 3: (x1, x1, x1) + (x2, x2, x2) = (x1 + x2, x1 + x2, x1 + x2), and since x1 + x2 ∈ R, the sum is an element of U . Also, the identity (0, 0, 0) ∈ U , by choosing x1 = 0 ∈ R, and since (0, 0, 0) is the identity in R 3 it will also be the identity in U . So the only thing that needed to be checked here is that the identity is an element of the subset U . Every element in U has an inverse in (R3, +), but we need to establish that that inverse is also an element of U . We find that −(x1, x1, x1) = (−x1, −x1, −x1) ∈ U. So the inverse of an element in U is also an element in U . If we consider the properties of a vector space established in the previous section, then all but one will hold in U since they hold for all real numbers and all sets of vectors in 160 CHAPTER 8. VECTOR SPACES R 3. The only one that needs to be checked, is whether or not the set U is closed under scalar multiplication: ∀ λ ∈ R, ∀ (x1, x1, x1) ∈ U : λ(x1, x1, x1) ∈ U. This can be verified using the definition of scalar multiplication in R3, λ(x1, x1, x1) = (λx1, λx1, λx1), where λx1 ∈ R. So we see that U satisfies all conditions of a real vector space with binary operation + in R3, and hence, is a vector space in its own right. We will call U a subspace of R 3. Geometrically, all points of the form (x1, x1, x1) with x1 ∈ R constitute a line through the origin and in the direction given by the vector with components (1, 1, 1). Let us now extend the idea of a subspace from this specific example. Definition 8.9. A subset U of a vector space V over F is called a subspace of V if U is non-empty and U is itself a vector space over F under the same operations of addition and scalar multiplication of V as a vector space over F. We write U ≤ V to indicate that U is a subspace of V and not just a subset of V . Example 107: The simplest subspaces that satisfy Definition 8.9 are: (i) {0} is a subspace of V ; (ii) V is a subspace of V . These two examples are known as improper subspaces. All other subspaces of V are known as proper subspaces, if they exist. How do we recognise a subspace? Theorem 8.10. A subset U of a vector space V over F is a subspace of V if and only if U is non-empty and 1. u + v ∈ U for all u, v ∈ U ; and 2. λu ∈ U for all λ ∈ F, u ∈ U . Equivalently, U is a subspace of V if and only if U is closed under addition and scalar multiplication i.e. λu + µv ∈ U for all u, v ∈ U and λ, µ ∈ F. 8.4. SUBSPACES OF VECTOR SPACES 161 Proof. Most conditions for a vector space apply to any set of vectors in V and any real numbers and hence automatically apply to the elements of the subset U . Aside from the two conditions in the theorem statement above, the only conditions that are not automatically satisfied are: 1. 0 ∈ U , 2. If u ∈ U then −u ∈ U for all u ∈ U . Both of these follow from condition 2 and Theorem 8.6. If one chooses an element u ∈ U with λ = 0 ∈ R, then 0u = 0 ∈ U . Similarly, since −u = (−1)u, for any u ∈ U , choosing λ = −1 establishes that −u = (−1)u ∈ U . Hence, all conditions of a vector space are satisfied if the subset is closed under both addition and scalar multiplication, as required. When determining if a subset of a vector space, is a vector space, it is useful to quickly check whether or not the identity 0 is in the subset. If the additive identity is not an element of the subset, then the subset is not a subspace. If the additive identity is an element of the subset, then we can check whether the subset is closed under addition and scalar multiplication to conclude that the set is a subspace. Lemma 8.11. A subset U of a vector space V over F is a subspace of V if and only if U is non-empty and u + λv ∈ U for all u, v ∈ U and λ ∈ F. Proof. It is easy to see that if U is a subspace, then U is non-empty and u + λv ∈ U . Suppose that u + λv ∈ U for all u, v ∈ U and λ ∈ F. Then taking λ = 1, we have u + v ∈ U for all u and v ∈ U . Similarly, taking u = 0 we have λv ∈ U for all v ∈ V and λ ∈ F. Hence U is a subspace by Theorem 8.10. Example 108: Check whether the following subsets U are subspaces of the real vector spaces V : 1. V = R 4; U = {(x1, x2, x3, x4) ∈ R 4| x1, x2, x3, x4 ∈ Z }; 2. V = R 5; U = {(x1, x2, x3, x4, x5) ∈ R 5| x1 + 2x2 + 3x3 + 2x4 + x5 = 1}; and 3. V = R 3; U = {(x1, x2, x3) ∈ R 3| 3x1 − x2 − 2x3 = 0}. 162 CHAPTER 8. VECTOR SPACES Definition 8.12. Consider two subspaces U and W of a vector space V over F. The intersection U ∩ W , of U and W is defined by U ∩ W := {u ∈ V | u ∈ U and u ∈ W }. The sum U + W , of U and W is defined by U + W := {v ∈ V | v = u + w, with u ∈ U and w ∈ W }. Theorem 8.13. Consider a vector space V over F. Let U and W be subspaces of V . Then U ∩ W and U + W are both subspaces of V . Proof: See practice questions. 8.5 Spanning set We have seen how any vector in E3 can be written in the form v = ai + bj + ck. This can be extended to general vector spaces. Definition 8.14. A vector v ∈ V , with V a vector space over F, is a linear combination of the vectors u1, u2, . . . , uk ∈ V if v can be written in the form v = λ1u1 + λ2u2 + . . . + λkuk = k∑ i=1 λivi where λ1, λ2, . . . , λk ∈ F. Notice that λi can be zero and there may be more than one choice for λ1, λ2, . . . , λk. Example 109: Show that the vector (0, 1, −2) ∈ R3 is a linear combination of the vectors (−2, 1, 0), (1, 3, 4) and (3, −2, 1). All possible linear combinations of a given set of vectors can be used to define a subset of the vector space. We call this subset the span: Definition 8.15. If u1, u2, . . . , uk ∈ V , with V a vector space over F, then the subset of V consisting of all possible linear combinations of u1, u2, . . . , uk is called their span and is denoted by span{u1, u2, . . . , uk} = {λ1u1 + λ2u2 + . . . + λkuk | λ1, λ2, . . . , λk ∈ F}. If U = span{u1, u2, . . . , uk}, then we say that {u1, u2, . . . , uk} is a spanning set for U or that {u1, u2, . . . , uk} spans U . 8.5. SPANNING SET 163 We will also use the notation ⟨u1, . . . , uk⟩ to denote the span of the set {u1, . . . , uk} ⊆ V . Also, if X is a set of vectors in V , we will use the alternative notation ⟨X⟩ for the set of vectors spanned by X. Example 110: In R3, determine the span of {(1, 0, 1), (0, 0, 1)}. What properties does such a spanning set have? Theorem 8.16. Suppose that U = span{u1, u2, . . . , uk}, where for a vector space V over F, we have u1, u2, . . . , uk ∈ V . Then, 1. u1, u2, . . . , uk ∈ U, 2. U is a subspace of V , and 3. U is the smallest subspace of V that contains u1, u2 . . . , uk in the sense that if ˜U is another subspace of V which contains u1, u2 . . . , uk, then U ⊆ ˜U . Proof: 1. ui ∈ U for all i = 1, 2, . . . , k since ui = 0u1 + 0u2 + . . . + 1ui + . . . + 0uk. 2. We first show that the span is closed under addition. Consider v1, v2 ∈ U , with v1 = λ1u1 + λ2u2 + . . . + λkuk, v2 = ν1u1 + ν2u2 + . . . + νkuk. where λ1, λ2, . . . , λk, ν1, ν2, . . . , νk ∈ F. Then, v1 + v2 = (λ1u1 + λ2u2 + . . . + λkuk) + (ν1u1 + ν2u2 + . . . + νkuk) = λ1u1 + λ2u2 + . . . + λkuk + ν1u1 + ν2u2 + . . . + νkuk = (λ1 + ν1) u1 + (λ2 + ν2) u2 + . . . + (λk + νk) uk ∈ U. (8.14) since λ1 + ν1, λ2 + ν2, . . . , λk + νk ∈ R. Similarly, for α ∈ F, αv1 = α (λ1u1 + λ2u2 + . . . + λkuk) = αλ1u1 + αλ2u2 + . . . + αλkuk 164 CHAPTER 8. VECTOR SPACES = (αλ1) u1 + (αλ2) u2 + . . . + (αλk) uk ∈ U, (8.15) since αλ1, αλ2, . . . , αλk ∈ F. Hence (8.14) and (8.15) show that U is closed under addition and scalar multiplication, and therefore, Theorem 8.10 yields U is a subspace of V . 3. Assume ˜U is a subspace of V which contains the vectors u1, u2, . . . , uk. Then for any λ1, λ2, . . . , λk ∈ R, we have λ1u1, λ2u2, . . . , λkuk ∈ ˜U , since as a subspace, ˜U is closed under scalar multiplication. Additionally, because ˜U is also closed under addition, λ1u1 + λ2u2 + . . . + λkuk ∈ ˜U , for all λ1, λ2, . . . , λk ∈ F. Hence, all elements in span{u1, u2, . . . , uk} = U are also in ˜U and hence U ⊆ ˜U . This completes the proof, as required. In fact, a very nice way to define the span of a set of vectors X ⊆ V is as the intersection of all subspaces of V which contain X. That is ⟨X⟩ = ⋂ X⊆U ≤V U. With this definition Theorem 8.13 implies that ⟨X⟩ is a subspace. Notice that this way of defining the span does not require the notion of linear combinations. It’s a good definition for proofs, but not for actual calculations. Example 111: In R 4, determine span{(1, 0, 0, 0), (0, 1, 0, 0)} and compare this with the subspace ˜U ⊂ R 4, with ˜U = {(x1, x2, x3, x4) ∈ R 4 | x4 = 0}. Now, how can we determine a spanning set for a given subspace U ⊂ V ? First, we will need to find a set of vectors such that every vector in U can be written as a linear combination of these vectors, i.e., U ⊆ span{u1, u2, . . . , uk}. If we can choose each of these vectors so that they also belong to U , i.e. u1, u2, . . . , uk ∈ U , then span{u1, u2, . . . , uk} ⊆ U, 8.6. LINEAR (IN)DEPENDENCE 165 and hence U = span{u1, u2, . . . , uk}. So to find a spanning set of U , we need to find a set of vectors in U such that every vector in U can be written as a linear combination of elements of the set. Example 112: Find a spanning set for the subspace of R3 given by U = {(x1, x2, x3) ∈ R3 | 3x1 − x2 − 2x3 = 0} of R 3. 8.6 Linear (In)dependence Consider the vectors a = 2i + j, b = 4i + 2j and c = 2i, then one can easily verify that 2a − b = 0. However there is only one linear combination of a and c that equals 0: αa + βc = 2αi + αj + 2βi = 2(α + β)i + αj, which equals 0 if and only if β = α = 0. The following definition addresses this observa- tion: Definition 8.17. A set of vectors {u1, u2, . . . , uk} in a vector space V over F is said to be linearly independent if and only if for λ1, . . . , λk ∈ F, λ1u1 + λ2u2 + . . . + λkuk = 0 ⇐⇒ λ1 = λ2 = . . . = λk = 0. This means that {u} with u ̸= 0 is linearly independent. The linear combination 0u1 + 0u2 + . . . + 0uk is called the trivial linear combination. To complement Definition 8.17, we have: Definition 8.18. A set of vectors {u1, u2, . . . , uk} in a vector space V is linearly dependent if there is a non-trivial linear combination which is equal to the zero vector, i.e. λ1u1 + λ2u2 + . . . + λkuk = 0 ̸=⇒ λi = 0 for all i = 1, . . . , k. 166 CHAPTER 8. VECTOR SPACES According to Definition 8.18, the set {0} in V is a linearly dependent set. Example 113: Show that the set S = {(0, 0, 1), (0, 1, 1), (1, 1, 1)} in R 3 is linearly independent. Example 114: Show that the set S = {(2, −3, 0, 1), (−1, 0, 0, 1), (0, −2, 0, 2)} of vectors in R4 is linearly dependent. Example 115: Show that if one of the vectors in the set S = {u1, u2, . . . , uk} of k vectors in a vector space V is the zero vector 0 of V , then S is a linearly dependent set. Before we consider the related theory in full generality, it is worth considering sets that contain 2 vectors (see practice questions). Theorem 8.19. Consider a vector space V over F and assume that V ̸= {0}. Then any set of finitely many non-zero vectors in V which spans V contains a linearly independent subset which spans V . Proof: We are given a finite set T of vectors which spans V . Choose a subset S of T with as few elements as possible so that S spans V . That is V = ⟨S⟩. Since T is finite, S is finite and we write S = {u1, u2, . . . , uk}. Plainly, 0 ̸∈ S as otherwise we could remove it and get a smaller set that spans V . If S is a linearly independent set, then the conclusion is satisfied. Hence we assume that S is linearly dependent and aim to find a contradiction. If S consists of one non-zero vector, then S is linearly independent and S is not empty as otherwise V = {0} against the assumption of the result. Hence k ≥ 2. Since S is linearly dependent, there exists λ1, . . . , λk ∈ F not all zero, such that λ1u1 + · · · + λkuk = 0. Pick ℓ ∈ {1, . . . , k} such that λℓ ̸= 0. Then −λℓuℓ = λ1u1 + · · · + λℓ−1uℓ−1 + 0uℓ + λℓ+1uℓ+1 + · · · + λkuk. multiplying by −λ −1 ℓ yields uℓ = −λ−1 ℓ λ1u1 + . . . λ −1 ℓ λℓ−1uℓ−1 + 0uℓ + λ −1 ℓ λℓ+1uℓ+1 + · · · − λ −1 ℓ λkuk. To simplify the expression let’s write µj = −λ−1 ℓ λℓ−1 ∈ F for 1 ≤ j ≤ k and j ̸= ℓ. So uℓ = µ1u1 + . . . µℓ−1uℓ−1 + 0uℓ + µℓ+1uℓ+1 + . . . + µkuk. Let v ∈ V . Then, as S spans V , there exists α1, α2, . . . αk ∈ F such that v = α1u1 + α2u2 + . . . + αℓ−1uℓ−1 + αℓuℓ + . . . + αkuk 8.6. LINEAR (IN)DEPENDENCE 167 = α1u1 + α2u2 + . . . + αℓ−1uℓ−1 + αℓ(µ1u1 + µ2u2 + . . . + µℓ−1uℓ−1) + . . . + αkuk = (α1 + αℓµ1)u1 + (α2 + αℓµ2)u2 + . . . + (αℓ−1 + αℓµℓ−1)uℓ−1 + αℓ+1uℓ+1 + . . . + αkuk. Hence, S \\ {uℓ} spans V and this contradicts the minimal choice of S. If this set of k − 1 vectors is linearly independent, the property is satisfied. This proves the theorem. Here’s a more sophisticated proof of the last result. Begin exactly as before, so S = {u1, u2, . . . , uk} ⊆ T is as small as possible subject to V = ⟨S⟩. Again we assume that the set is not linearly independent and we write uℓ = µ1u1 + . . . µℓ−1uℓ−1 + 0uℓ + µℓ+1uℓ+1 + . . . + µkuk. Then uℓ ∈ ⟨S \\ {uℓ}⟩. This means that ⟨S \\ {uℓ}⟩ ⊃ S \\ {uℓ} ∪ {uℓ} = S. But then by definition, V ≥ ⟨S \\ {uℓ}⟩ ≥ ⟨S⟩ = V. and this contradicts the minimal size of S. Hence S is linearly independent. Note that, to remove the word ‘finitely’ in Theorem 8.19, we require a definition for linear combinations of infinitely many vectors to extend Definition 8.14 (which will be considered in courses later in your degree programme). We can now formulate the fundamental theorem: Theorem 8.20. Let S be a set of k vectors in a vector space V which spans V . Let T be a linearly independent set of m vectors in V . Then, m ≤ k. Alternatively, Theorem 8.20 states that the number of vectors in a linearly independent set in a vector space V cannot exceed the number of vectors in a spanning set of V . Proof: Assume that k < m (to obtain a contradiction). We denote the spanning set S to be, S = {u1, u2, . . . , uk}, and the linearly independent set T as T = {t1, t2, . . . , tm}. Therefore λ1t1 + λ2t2 + . . . + λmtm = 0 ⇐⇒ λi = 0 for i = 1, . . . , m. (8.16) 168 CHAPTER 8. VECTOR SPACES Theorem 8.19 states that if the vectors in S are linearly dependent, then there is a subset S1 of S with ¯k elements (¯k ≤ k) which is also a spanning set and contains linearly independent vectors. If we can prove that the assumption k < m is false for the smaller spanning set S1, then it also is false for the larger spanning set S. Since S1 is a spanning set, every vector in T can be written as a linear combination of the vectors in S1, i.e. ti = α1iu1 + α2iu2 + . . . + α¯kiu¯k, (8.17) for i = 1, 2, . . . , m. Substitution of t in (8.17) into (8.16) yields λ1(α11u1 + α21u2 + . . . + α¯k1u¯k) + λ2(α12u1 + α22u2 + . . . + α¯k2u¯k) + ... λm(α1mu1 + α2mu2 + . . . + α¯kmu¯k) = 0 (8.18) ⇐⇒ (λ1α11 + λ2α12 + . . . + λmα1m)u1 + (λ1α21 + λ2α22 + . . . + λmα2m)u2 + ... (λ1α¯k1 + λ2α¯k2 + . . . + λmα¯km)u¯k = 0. (8.19) Since the spanning set S1 is linearly independent, each of the coefficients in the sum on the LHS of (8.19) has to be zero, yielding the following system of ¯k equations in m unknowns:    α11λ1 + α12λ2 + . . . + α1mλm = 0 α21λ1 + α22λ2 + . . . + α2mλm = 0 ... ... ... ... α¯k1λ1 + α¯k2λ2 + . . . + α¯kmλm = 0 (8.20) Since, by the assumption ¯k ≤ k < m, there are fewer equations than unknowns in (8.20), so we can add equations of our choice to generate a system of m equations in m unknowns. By doing this, we can always make the determinant of the coefficient matrix in (8.20) equal to zero (for example, by using an already existing equation, hence generating two identical rows in the matrix of coefficients). Such a system has a non-trivial solution via Corollary 7.37, and hence, we can solve (8.18) with at least one λi ̸= 0. Therefore, via (8.17)-(8.20), we have shown that there exists a linear combination λ1t1 + λ2t2 + . . . + λmtm = 0, which has at least one λi ̸= 0. This contradicts the condition that T is linearly inde- pendent. Finally, we conclude that the assumption m > k is false, and hence m ≤ k, as required. 8.7. BASIS 169 8.7 Basis The ideas of a spanning set and linear independence can be combined to define a basis, a very important notion in the study of vector spaces. Definition 8.21. A set of vectors {u1, u2, . . . , uk} in a vector space V is called a basis of V if (i) V = span{u1, u2, . . . , uk}; and (ii) {u1, u2, . . . , uk} is a linearly independent set. Definition 8.22. A vector space V is called finite-dimensional if there exists a basis of V which contains a finite number of vectors. A vector space which is not finite-dimensional is called infinite-dimensional. Example 116: {(1, 0), (0, 1)} is a basis of R 2. Example 117: {i, j, k} is a basis of E3. We primarily consider finite-dimensional vector spaces in this module 1. Theorem 8.23. Any two bases of a finite-dimensional vector space V contain the same number of vectors. Proof: Consider two bases2, S and T of V , with m and k number of vectors respectively. Both S and T are spanning sets as well as linearly independent. Since S is a spanning set and T a set of k linearly independent vectors, the fundamental theorem (Theorem 8.20) states that k ≤ m. Similarly, since T is a spanning set and S a set of m linearly independent vectors, again, the fundamental theorem states that m ≤ k. Hence, k = m, as required. Definition 8.24. The number of vectors in any basis of a finite-dimensional vector space V is called the dimension of V , also written as dim(V ). 1Infinite dimensional vector spaces appear in [15] and the practice questions. 2“bases\" is the plural of basis. 170 CHAPTER 8. VECTOR SPACES These definitions require clarification in one special case. The subset {0} of any vector space forms a subspace, but has no basis since {0} is not linearly independent. For completeness, we say that the dimension of {0} is zero. A proper subspace U of a finite-dimensional vector space V will be also finite-dimensional. Example 118: Show that the set of vectors {e1, e2, . . . , en} with e1 = (1, 0, 0, . . . , 0), e2 = (0, 1, 0, . . . , 0), ... en = (0, 0, 0, . . . , 1), is a basis of R n. The basis of R n introduced in the previous example is referred to as the standard or- dered basis of Rn, since the vectors are listed in a particular order. Example 119: Find a basis of the subspace U = {(x1, x2, x3) ∈ R 3| 3x1 − x2 − 2x3 = 0}, of R 3. 8.8 More properties of vector space bases Theorem 8.25. Consider a vector space V and a basis B = {u1, u2, . . . , un} of V . Then any vector v ∈ V can be written as a linear combination of the vectors of B in exactly one way. Proof: Since v ∈ V , v can be written as a linear combination of the basis vectors in at least one way thus we have v = α1u1 + α2u2 + . . . + αnun where αi ∈ F for 1 ≤ i ≤ n. Suppose that there exist β1, . . . , βn such that v = β1u1 + β2u2 + . . . + βnun. Then subtracting one from the other we obtain (α1 − β1)u1 + (α2 − β2)u2 + . . . + (αn − βn)un = 0. Since the vectors in a basis B are linearly independent, it follows that αi = βi, for all values of i. Hence, v can be written as a linear combination of the basis vectors in exactly one way. 8.8. MORE PROPERTIES OF VECTOR SPACE BASES 171 Definition 8.26. Consider the vector space V with an ordered basis B = {u1, u2, . . . , un} and a vector v ∈ V with v = λ1u1 + λ2u2 + . . . + λnun, where λi ∈ R for all values of i. Then the uniquely determined real numbers λ1, λ2, . . . , λn (in the given order) are known as the coordinates of v with respect to the basis B. We call (λ1, λ2, . . . , λn) the coordinate vector of v with respect to the basis B. Example 120: (i) Show that B = {(1, 0, 1), (0, 0, 1), (0, 2, 0)} is an ordered basis of R 3. (ii) Determine the coordinates of the vector (2, 1, 3) ∈ R 3 with respect to the basis B. Theorem 8.27. Let V be a vector space of dimension n ∈ N and assume that S is a set of vectors in V . Then: (i) if |S| > n, then S is linearly dependent; and (ii) If |S| < n, then V ̸= span(S). Proof: (i) Assume |S| = k > n. If S is linearly independent, then n < k ≤ n by Theorem 8.20 which is impossible. Hence S is linearly dependent. (ii) Assume |S| = k < n and S spans V . Then Theorem 8.19 implies that S contains a basis for V . Hencen ≤ k < n which is nonsense. Lemma 8.28. Consider a linearly independent set of k vectors in a vector space V , {u1, u2, . . . , uk}, and a vector v ∈ V . Then the set {v, u1, u2, . . . , uk} is linearly independent if and only if v ̸∈ span{u1, u2, . . . , uk}. Proof: Suppose that S = {v, u1, u2, . . . , uk} is linearly independent. If v ∈ span{u1, u2, . . . , uk}, Then, by definition we can write λ1u1 + λ2u2 + . . . + λkuk = v 172 CHAPTER 8. VECTOR SPACES and this implies v − λ1u1 − λ2u2 − . . . − λkuk = 0 which means that S is linearly dependent and contradicts our supposition. So, if S = {v, u1, u2, . . . , uk} is linearly independent, then v ̸∈ span{u1, u2, . . . , uk}. Conversely, suppose that v ̸∈ span{u1, u2, . . . , uk} and that S = {v, u1, . . . , uk} is linearly dependent. Then there are λi, λ ∈ F not all zero such that λv + λ1u1 + λ2u2 + . . . + λkuk = 0. If λ = 0, then λi = 0 for 1 ≤ i ≤ k as S is linearly independent. So we may suppose that λ ̸= 0. Then, v + λ1λ−1u1 + λ2λ −1u2 + . . . + λkλ −1uk = 0 ⇐⇒ v = −λ1λ−1u1 − λ2λ−1u2 − . . . − λkλ−1uk ∈ U. This contradicts the supposition that v ̸∈ span{u1, u2, . . . , uk}. Hence if v ̸∈ span{u1, u2, . . . , uk}, then S is linearly independent. Theorem 8.29. Let V be a vector space over F and assume that V ̸= 0 is spanned by ℓ vectors (ℓ ∈ N). Then: (i) each linearly independent set of vectors is part of a basis of V ; (ii) if X ⊆ V spans V , then there exists B ⊆ X such that B is a basis for V ; and (iii) V has a basis and dim(V ) ≤ ℓ. Proof: Since V is spanned by ℓ elements, we have dim V ≤ ℓ by Theorem 8.19 and by Theorem 8.27 any linearly independent subset of V has at most ℓ elements. (i) Suppose that there exists a linearly independent subset of V which does not extend to a basis. From amongst all possibilities choose S = {u1, u2, . . . , uk} with a max- imal number of vectors. Then S is linearly independent and k ≤ ℓ. If V = ⟨S⟩, thenS is a basis, a contraction as we chose it not to be part of a basis. Therefore S does not span V , choose any vector uk+1 ∈ V such that uk+1 ̸∈ span(S). Then the set S ∪ {uk+1} is linearly independent by Lemma 8.28. The maximal choice of S implies S ∪ {uk+1} is part of a basis. But then so is S, a contradiction. We conclude that such an S could not be chosen. Hence (i) holds. (ii) Consider a set S = {u1, u2, . . . , uk} of k vectors which spans V . Then Theorem 8.19 states that S contains a subset which also spans V but is linearly independent. This subset is a basis of V . 8.8. MORE PROPERTIES OF VECTOR SPACE BASES 173 (iii) From (ii), we conclude that as V is spanned by ℓ vectors, dim(V ) ≤ ℓ. Example 121: Find a basis of the subspace U ⊂ R 3 which is spanned by {u1, u2, u3} with u1 = (0, 1, 1), u2 = (2, 4, 4) and u3 = (1, 1, 1). Theorem 8.30. Let V be a vector space V of dimension n ≥ 1 and let S = {u1, . . . , un} be a set of n vectors in V . (i) If S spans V , then S is a basis for V . (ii) If S is linearly independent, then S is a basis for V . Proof: (i) Assume S spans V . Then by Theorem 8.29(ii) there is a subset of S which is a basis of V . Since every basis of V contains exactly n vectors, this subset must be S itself. (ii) Assume S is a linearly independent set of vectors in V . Then Theorem 8.29(i) implies S ⊆ B where B is a basis for V . Hence n = |S| ≤ |B| = n and so S = B. So, if we know the dimension n of a vector space, then we do not need to check both conditions for a basis in Definition 8.21 in the case of a set of n vectors. It is usually easier to show that a set of n vectors is linearly independent than it is to show that it is a spanning set. Example 122: Verify that the set U = {(1, 2, 1, 3), (2, 5, 3, 9)} ⊂ R 4 is linearly independent and extend it to a basis of R4. Theorem 8.31. Consider a real vector space V of dimension n ∈ N. Let U and W be subspaces of V . Then, (i) U and W are finite-dimensional with dim(U ) ≤ n and dim(W ) ≤ n; (ii) any basis of U and any basis of W can be extended to a basis of V ; and (iii) if U ⊆ W and dim(U ) = dim(W ) then U = W . Proof: (i) Let S be a linearly independent subset of U . Then Theorem 8.29(i) implies |S| ≤ n. Hence dim(U ) ≤ n. The same is true for W . 174 CHAPTER 8. VECTOR SPACES (ii) Since any basis of U is a linearly independent set in V , it is part of a basis of V via Theorem 8.29. The same justification establishes the result for W . (iii) Suppose U ⊆ W and dim(U ) = dim(W ). Let B be basis of U . Then |B| = dim(U ) = dim(W ). Hence B is a basis for W . Therefore U = ⟨B⟩ = W. 8.9 Vector spaces arising from linear ODEs∗ Consider the 2nd order ordinary differential equation (ODE) u ′′ + u = 0 on R (8.21) for u : R → R. Notably, there are 2 (there are many more) distinct solutions u1, u2 : R → R to the ODE in (8.21) given by u1(x) = sin (x) and u2(x) = cos (x) ∀x ∈ R. (8.22) Now, consider the set V = {f : R → R} . Also consider the binary operation ⊕ given by (g ⊕ f )(x) = g(x) + f (x) ∀x ∈ R, f, g ∈ V and scalar multiplication ⊙ with elements in the field R, +, · defined as (λ ⊙ f )(x) = λf (x) ∀x ∈ R, λ ∈ R, f ∈ V. Then via results considered in [15], (V, ⊕) is a real vector space (of infinite dimension). Since u1, u2 ∈ V , it follows from (8.22) that U = span{u1, u2} = {f ∈ V : f = λ1u1 + λ2u2, λ1, λ2 ∈ R} = {f ∈ V : f (x) = λ1 sin (x) + λ2 cos (x), λ1, λ2 ∈ R, ∀x ∈ R}. is a 2-dimensional subspace of V . This follows from Theorem 8.10 since U is closed under the scalar multiplcation and vector space addition operations in V , and Definition 8.24 since λ1 sin (x) + λ2 cos (x) = 0 ∀x ∈ R =⇒    λ1 sin (0) + λ2 cos (0) = 0 λ1 sin ( π 2 ) + λ2 cos ( π 2 ) = 0 =⇒    λ2 = 0 λ1 = 0. i.e. u1 and u2 are linearly independent elements of U , and hence, form a basis of U . 8.10. THE DIMENSION OF A SUM OF SUBSPACES 175 Now, since the ODE in (8.21) is linear, any linear combination of solutions to the ODE is also a solution to the ODE. Thus, U is contained in the general solution set for the ODE in (8.21). In fact, U is the general solution set for the ODE in (8.21) since U is a solution set that is 2-dimensional and the ODE is of order 2 with constant coefficients. Now, if we wish to solve initial value problems for the ODE in (8.21), for example, find u that satisfies (8.21) and   u(0) = a u ′(0) = b, for some a, b ∈ R, then we would solve   λ1 sin (0) + λ2 cos (0) = a λ1 cos (0) − λ2 sin (0) = b ⇐⇒ (sin (0) cos (0) cos (0) − sin (0) ) · (λ1 λ2 ) = ( a b ) ⇐⇒   λ1 = b λ2 = a. (8.23) Using this idea, we can see that solutions to boundary value problems for the ODE in (8.21) can be found by simply finding a suitable element of the subspace U (which involves solving a system of simultaneous linear equations). In general, for ‘nice’ linear ODE of order n on R, there exist n linearly independent solutions (in V ) which can be used as basis vectors that span the solution set to the ODE. One can then use these bases to solve boundary value problems associated with ODEs. As an exercise, find the solution to the ODE in (8.21) such that u satisfies   u(x∗) = a u ′(x∗) = b, for fixed x∗ ∈ R (hint - consider the inverse of a 2 × 2 matrix like that in (8.23)). 8.10 The dimension of a sum of subspaces Theorem 8.32. Let V be a vector space and U and W be finite dimensional sub- spaces of V . Then, dim(U + W ) = dim(U ) + dim(W ) − dim(U ∩ W ). Proof: Since U ∩ W is a subspace of V , consider a basis {i1, . . . , iq} of U ∩ W , where q is the dimension of U ∩ W . Then {i1, . . . , iq} is a set of linearly independent vectors in U , so we can extend it to obtain a basis of U , {i1, . . . , iq, u1, . . . , up}, where p + q is the dimension of U . Similarly, {i1, . . . , iq} is a set of linearly independent vectors in W , so we can extend it to obtain a basis of W , {i1, . . . , iq, w1, . . . , wr}, where r + q is the dimension of W . Set B = {i1, . . . , iq, u1, . . . , up, w1, . . . , wr}. 176 CHAPTER 8. VECTOR SPACES Then |B| = p + q + r = dim(U ) + dim(W ) − dim(U ∩ W ). Hence to prove the theorem we show that B is a basis for U + W . Now, by definition, any vector in U + W can be written as u + w, with u ∈ U and w ∈ W . Each of these two vectors can uniquely be written as a linear combination of the vectors in the basis of U and W respectively: u = α1i1 + · · · + αqiq + αq+1u1 + · · · + αq+pup, w = β1i1 + · · · + βqiq + βq+1w1 + · · · + βq+rwr, (8.24) with αi, βj ∈ R for 1 ≤ i ≤ q + p and 1 ≤ j ≤ q + r. Therefore, u + w = α1i1 + · · · + αqiq + αq+1u1 + · · · + αq+pup + β1i1 + · · · + βqiq + βq+1w1 + · · · + βq+rwr = γ1i1 + · · · + γqiq + αq+1u1 + · · · + αq+pup + βq+1w1 + · · · + βq+rwr,(8.25) with γj = αj + βj for i = 1, 2, . . . , q. Hence B spans U + W . To establish that B is linearly independent, consider a linear combination of the vectors in B which equals the zero vector: 0 = α1i1 + · · · + αqiq + γ1u1 + · · · + γpup + β1w1 + · · · + βrwr. (8.26) Equation (8.26) can be rewritten as −γ1u1 − · · · − γpup = α1i1 + · · · + αqiq + β1w1 + · · · + βrwr. (8.27) The vector on the left-hand side of (8.27) is an element of the subspace U and the vector on the right-hand side of (8.27) is a vector in the subspace W , hence, this vector must be in the intersection of U and W and can therefore be written as a unique linear combination of the basis vectors in U ∩ W . So, for some d ∈ U ∩ W , we have, d = −γ1u1 − · · · − γpup, d = τ1i1 + · · · + τqiq. However, {i1, . . . , iq, u1, . . . , up} is a basis for U and d has two expressions as a linear combination of elements in this basis. This contradicts Theorem 8.25 unless γ1 = · · · = γp = τ1 = · · · = τq = 0. Hence 0 = α1i1 + · · · + αqiq + γ1u1 + · · · + γpup + β1w1 + · · · + βrwr = α1i1 + · · · + αqiq + β1w1 + · · · + βrwr Since {i1, . . . , iq, w1, . . . , wr}, is linearly independent we have β1 = · · · = βr = 0 and we have demonstrated that B is linearly independent. This proves the claim. 8.11. ROW SPACE, COLUMN SPACE, ROW RANK AND COLUMN RANK OF A MATRIX177 8.11 Row space, column space, row rank and column rank of a matrix Consider a matrix A ∈ Mmn(F). Each row is an ordered set of n numbers, and hence can be considered as a vector in the vector space Fn. Hence, the matrix A defines an ordered set of m vectors in Fn. We can then consider the subspace of Fn which is spanned by this set of vectors. Definition 8.33. Consider a matrix A ∈ Mmn(F) and let ui denote the vector in Fn associated with the i-th row in A. Then the row space of A, denoted by row(A) is the subspace of Fn given by row(A) = span{u1, u2, . . . , um}. Similarly, one can consider the n columns in A as vectors in Fm and we can define the column space of a matrix: Definition 8.34. Consider a matrix A ∈ Mmn(F) and let ui denote the vector in Fm associated with the i-th column in A. Then the column space of A, denoted by col(A) is the subspace of Fm given by col(A) = span{uT 1 , uT 2 , . . . , uT n }. Example 123: Determine the row space and column space of A =    1 2 −3 −1 −2 −6 5 −2 3 −5 −1 2    . The dimensions of the row space and column space are of particular importance. Therefore we introduce: Definition 8.35. Consider A ∈ Mmn(F). The row rank of A is given by dim(row(A)), and the column rank of A is given by dim(col(A)). 178 CHAPTER 8. VECTOR SPACES Theorem 8.36. Consider matrices A ∈ Mmn(F), M ∈ Mmm(F) and N ∈ Mnn(F). Then, (i) row(M · A) ⊆ row(A) where row(M · A) = row(A) if M is invertible; and (ii) col(A · N) ⊆ col(A) where col(A · N) = col(A) if N is invertible. Proof: (i) Definition 4.13 implies that each row of M · A is a linear combination of rows of A. Thus, row(M · A) ⊆ row(A). If M −1 exists, then row(A) = row(M−1 · M · A) ⊆ row(M · A) ⊆ row(A), which implies that row(M · A) = row(A). (ii) We apply (i) and use the transpose. col(A · N) = row((A · N) T ) = row(N T · AT ) ⊆ row(AT ) = col(A) and if N is invertible, then so is N T and we obtain equality. Note that in the proof of Theorem 8.36, we used that in the matrix A · B, each row is a linear combination of the rows of B and each column in A · B is a linear combination of the columns of A. Corollary 8.37. Consider matrices A ∈ Mmn(F), M ∈ Mmm(F) and N ∈ Mnn(F), with M and N invertible matrices. Then (i) row rank of M · A = the row rank of A; and (ii) column rank of A · N = the column rank of A. Proof: This follows from the conclusion of Theorem 8.36, namely that row(M · A) = row(A) and col(A · N) = col(A) when M and N are invertible, respectively. Lemma 8.38. Suppose that P ∈ Mmm(F) is invertible and A ∈ Mm,n(F). Then A and PA have the same column rank. 8.11. ROW SPACE, COLUMN SPACE, ROW RANK AND COLUMN RANK OF A MATRIX179 Proof: We will come back to this after we have seen linear transformations. Lemma 8.39. Let R ∈ Mmn(F). Assume that R is in echelon form. Then the row rank of R equals the column rank of R is equal to the number of non-zero rows in R. Proof: Consider the echelon matrix R. It will contain a number of non-zero rows and possibly a number of zero rows. The non-zero rows are of a specific form, e.g.,    1 a b c . . . d 0 1 e f . . . g 0 0 0 1 . . . h    . The first row cannot be written as a linear combination of the lower rows, since none of these has a non-zero entry in the first column. Similarly, the second row cannot be written as a linear combination of the others below. Hence, the non-zero rows in the echelon matrix are linearly independent. Therefore, the non-zero rows in the echelon matrix span the row space of R and form a linearly independent set, hence they form a basis of row(R). The dimension of these row spaces is given by the number of vectors in a basis, hence if there are r non-zero rows in the matrix in echelon form, the row rank of R = row rank of A = r. It is also straight forward to see the columns corresponding to the positions were the leading ones sit in R form a basis for the column space of R. Hence the column rank of R is also r. Theorem 8.40. Consider the matrix A ∈ Mmn(F) and let R be an echelon form of A obtained by elementary row operations. Then row(A) = row(R) and the rows of R are a basis for row(A). Proof: Any elementary row operation on the matrix A is equivalent to the multiplication (on the left) with an elementary matrix. So, if R is matrix A in echelon form, obtained after performing a sequence of elementary row operations, we can write R = Eq · Eq−1 · . . . · E1 · A = P · A, where P = Eq ·Eq−1 ·. . .·E1. Because every elementary matrix is invertible, P is invertible and hence using Theorem 8.36 row(R) = row(P · A) = row(A). Now use Lemma 8.39 to finish the proof. 180 CHAPTER 8. VECTOR SPACES Theorem 8.40 can be used to verify whether or not a set of vectors in a vector space Fn is linearly independent. Consider the set of vectors {u1, u2, . . . , uk}. If we construct a matrix A such that row i is given by the elements in the vector ui, then row(A) = span{u1, u2, . . . , uk}. By reducing the matrix A to echelon form, we can determine the row rank of A. If the row rank of A, m, is equal to k, then the k vectors span a vector space of dimension k and hence have to be linearly independent. If, on the other hand, m < k, then the k vectors span a vector space with dimension less then k and hence are linearly dependent. Corollary 8.41. Let A ∈ Mnn(F). Then A is invertible if and only if A has row rank n. Proof: This follows as A is invertible if and only if the reduced echelon form of A is the identity matrix In (see Section 5.5). Example 124: Verify that the set of vectors in F3, U = {(0, 0, 1), (0, 1, 1), (1, 1, 1)} is linearly independent. Example 125: Investigate whether or not the set of vectors {u1, u2, u3} in F4 is linearly independent, where u1 = (1, −3, 0, 2), u2 = (0, −3, 0, 3), u3 = (0, −2, 0, 2). Theorem 8.42. For any matrix A ∈ Mmn(F), row rank of A = column rank of A. Proof:Let R be an echelon matrix obtained from A. Then R = PA where P is a product of elementary matrices and is invertible. By Lemma 8.38 and Theorem 8.40, A has the same row rank and column rank as R. By Lemma 8.39, the row rank of R equals the column rank of R. This proves the result. Definition 8.43. For any matrix A ∈ Mmn(F), rank of A = row rank of A = column rank of A. 8.12. SYSTEMS OF LINEAR EQUATIONS 181 It is, however, worthwhile to remember that when m ̸= n, row(A) ⊆ Fn, and col(A) ⊆ Fm. So the row space and column space are not generally identical, even if their dimensions are the same. To determine the rank of a matrix, one can transform it into echelon form, using elemen- tary row operations, and then count the number of non-zero rows in the matrix in echelon form. Proposition 8.44. For any matrix A ∈ Mmn(F), rank of A ≤ min(m, n). Proof: Since row(A) and col(A) are spanned by a set of m and n vectors respectively, it follows that rank(A) ≤ min{m, n}, as required. 8.12 Systems of linear equations We can now consider properties of systems3 of linear equations from a new perspective. Consider the system of equations a11x1 + a12x2 + . . . + a1nxn = d1, a21x1 + a22x2 + . . . + a2nxn = d2, ... ... ... ... ... am1x1 + am2x2 + . . . + amnxn = dm, (8.28) which can be written as, A · x = d, (8.29) where A = [aij], x =       x1 x2 ... xn       and d =       d1 d2 ... dm       . Alternatively, the system of equations can also be written in the form x1v1 + x2v2 + . . . + xnvn = d, where v1 =       a11 a21 ... am1       , v2 =       a12 a22 ... am2       , . . . , vn =       a1n a2n ... amn       . (8.30) 3Note that we have stopped writing simultaneous ... in general from now onwards we don’t. 182 CHAPTER 8. VECTOR SPACES So the vector vj represents the j-th column in the matrix A. Example 126: Express x1 − 3x2 + 4x3 − 2x4 = 5, 2x2 + 5x3 + x4 = 2, x2 − 3x3 = 4. in the equivalent forms in (8.29) and (8.30). Theorem 8.45. The system of equations A · x = d of m equations in n unknowns has at least one solution if and only if dT ∈ col(A). Proof: If d ∈ col(A), then there exists at least one set on real numbers λ1, λ2, . . . , λn such that λ1vT 1 + λ2vT 2 + . . . + λnvT n = dT . Hence, the system A · x = d has at least one solution. If the system has at least one solution, e.g., x =       µ1 µ2 ... µn       , then µ1vT 1 + µ2vT 2 + . . . + µnvT n = dT . Hence dT ∈ col(A), as required. Corollary 8.46. The system of equations A · x = d of m equations in n unknowns has at least one solution if and only if col([A | d]) = col(A), where [A | d] is the augmented m by n + 1 matrix obtained by adding a column, consisting of the elements of d to the right of the matrix A. Proof: This is equivalent to Theorem 8.45. 8.12. SYSTEMS OF LINEAR EQUATIONS 183 Theorem 8.47. A system of equations A · x = d of m equations in n unknowns has at least one solution if and only if the coefficient matrix A and the augmented matrix [A | d] have the same rank. Proof: Via Corollary 8.46 the system has a solution if and only if col([A | d]) = col(A), which implies that the (column) rank of A is equal to the (column) rank of [A | d] and via Theorem 8.42, rank[A | d] =rank(A). Conversely, assume that rank[A | d] =rank(A). Note that col(A) ⊆ col([A | d]). In Theorem 8.31, we have seen that two subspaces with the same dimension and where one is a subset of the other have to be equal, so col(A) = col([A | d]) and hence the system has a solution, as required. Theorem 8.48. A system of equations A · x = d of m equations in n unknowns has a unique solution if the coefficient matrix A and the augmented matrix [A | d] have rank equal to n. Proof: Since the rank of a matrix is also the column rank, the fact that the rank(A) = n means that the set of n vectors formed by the columns of A is a basis for the column space. Hence, d can be expressed as a linear combination of these vectors, uniquely, i.e. the expansion d = λ1v1 + λ2v2 + . . . + λnvn, is unique. Therefore there is a unique solution x = (λ1, λ2, . . . , λn) to the system of linear equations, as required. If rank(A) (and rank([A | d])) is r with 0 < r < n, then the system has infinitely many solutions. One can see that there will then be n − r degrees of freedom, i.e. n − r of the unknowns can be chosen without restriction. Example 127: For all real values of α, find all solutions, if any, of the system of linear equations x + 3y + 2z = 1, 2x + 7y + αz = 5, (α − 3)x + y − 10z = 11. 184 CHAPTER 8. VECTOR SPACES Chapter 9 Linear Transformations ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • define a linear transformation between two vector spaces; • verify that a mapping between two vector spaces is a linear transforma- tion; • cite and use a number of properties of linear transformations; • define and identify the kernel and image of a linear transformation; • represent a linear transformation by a matrix; • define and use coordinate transformations; • apply the composition of two linear transformations and its equivalent on their matrix representations; • define and use transition matrices; and [4, p.323-384] contains an alternative presentation of material in this chapter that you may find helpful. 9.1 Introduction In this chapter, we will study mappings (functions) between vector spaces, represent these mappings by matrices and study mappings from a vector space to itself. Such mappings encompass coordinate transformations when different bases are considered within the same vector space. 185 186 CHAPTER 9. LINEAR TRANSFORMATIONS 9.2 Definition Consider two real vector spaces V and W . One can then consider a relationship between the elements of the first vector space and elements of the second. For example, one can consider a mapping between vectors in E2 and ordered pairs of real numbers in R 2. It is particularly important to define mappings that treat addition in the vector space and scalar multiplication with entries in the field, in a consistent way. Definition 9.1. Let V and W be vectir spaces over F. A function T : V → W is called a linear transformation if: (i) T (v1 + v2) = T (v1) + T (v2) for all v1, v2 ∈ V ; and (ii) T (λv) = λT (v) for all λ ∈ F, v ∈ V . Note that the vector spaces V and W in Definition 9.1 can be equal. A linear trans- formation is also referred to as a linear mapping or a homomorphism. Notice that the addition and scalar multiplication on both sides of the identities in Definition 9.1 refer to operations in different vector spaces. Specifically, on the LHS and RHS of the equations in (i) and (ii), the operations are in V and W , respectively. In other words, if T is a linear transformation, then the image of the sum of two vectors in V is the sum (in W ) of the images of these two vectors, and, the image of a scalar multiple of a vector in V is the scalar multiple (in W ) of the image of the vector. Example 128: Show that T : R2 → R 2 given by T ((x, y)) = (y, x) ∀(x, y) ∈ R 2 is a linear transformation and give its geometric interpretation when R2 is represented by the set of all points in a plane. Example 129: Show that O : V → W given by O(v) = 0 ∀v ∈ V is a linear transformation (known as the zero transformation.) Example 130: Show that IV : V → V given by IV (v) = v ∀v ∈ V is a linear transformation (known as the identity transformation.) 9.3. PROPERTIES OF LINEAR TRANSFORMATIONS 187 9.3 Properties of Linear Transformations Theorem 9.2. Consider vector spaces V and W over F and let T : V → W be a linear transformation. Then: (i) T (0) = 0 ∈ W ; (ii) T (−v) = −T (v) for all v ∈ V with −v ∈ V the additive inverse of v; and (iii) T (λ1v1 + λ2v2 + . . . + λkvk) = λ1T (v1) + λ2T (v2) + . . . + λkT (vk) for all v1, v2, . . . , vk ∈ V , λ1, λ2, . . . , λk ∈ F. Proof: The proof is an exercise. However, we note that the proofs of (i)-(iii) follow almost immediately from Definition 9.1 (mathematical induction should also be used for (iii)). Thus, a necessary condition for a function T : V → W to be a linear transformation is that T (0) = 0. Theorem 9.3. Consider two real vector spaces V and W . Let B = {v1, v2, . . . , vn} be a basis of V with dim(V ) = n. Let T1 : V → W and T2 : V → W be linear transformations such that T1(vi) = T2(vi) ∀vi ∈ B. (9.1) Then T1 = T2. Proof: Since B = {v1, v2, . . . , vn} is a basis for V , it follows that for each v ∈ V , there exists λi ∈ R for i = 1, . . . , n such that v = n∑ i=1 λivi. (9.2) Since T1(vi) = T2(vi) for each i = 1, . . . , n, it follows that T1(v) = T1 ( n∑ i=1 λivi ) (via (9.2)) = n∑ i=1 λiT1 (vi) (via Theorem 9.2) = n∑ i=1 λiT2 (vi) (via (9.1)) 188 CHAPTER 9. LINEAR TRANSFORMATIONS = T2 ( n∑ i=1 λivi ) (via Theorem 9.2) = T2(v), as required. So if two linear transformations map all vectors of a basis into the same images, then the linear transformations are the same. Therefore a linear transformation T : V → W is known if the images under T of the vectors in a basis of V are known. Theorem 9.4. Consider two real vector spaces V and W . Let B = {v1, v2, . . . , vn} be a basis of V with dim(V ) = n. Let w1, w2, . . . , wn be n (not necessarily distinct) vectors in W . Then there exists a unique linear transformation T : V → W such that T (v1) = w1, T (v2) = w2, . . . , T (vn) = wn. Proof: We define the transformation using the basis for V . For all v ∈ V , there exist µi ∈ R such that1 v = n∑ i=1 µivi. (9.3) Define T : V → W , using (9.3), to be T (v) = n∑ i=1 µiwi ∀v ∈ V. (9.4) Firstly, note that by (9.3) and (9.4), T (vi) = wi (9.5) since vj = n∑ i=1 µivi ⇐⇒ µi =   1 ; i = j 0 ; i ̸= j. Now, for u, w ∈ V we can write u = ∑n i=1 αivi and w = ∑n i=1 βivi. Hence for λ ∈ F, λu + w = n∑ i=1 λαivi + n∑ i=1 βivi = n∑ i=1(λαi + βi)vi. it follows from (9.3) that T (λu + w) = T ( n∑ i=1(λαi + βi)vi ) 1These are the coordinates of v with respect to the basis B. 9.3. PROPERTIES OF LINEAR TRANSFORMATIONS 189 = n∑ i=1(λαi + βi)wi = λ n∑ i=1 αiwi + n∑ i=1 βiwi = λT (u) + T (w). Therefore, T is a linear transformation from V to W . In addition, we’ve seen that T maps vi to wi directly from (9.5). Finally, the transformation in (9.4) is unique by Theorem 9.3 since T (vi) is defined for all basis vectors vi ∈ B. This completes the proof, as required. Example 131: Consider the basis {v1, v2, v3, v4} of R 4 with v1 = (1, 1, 1, 1), v2 = (0, 1, 1, 1), v3 = (0, 0, 1, 1) and v4 = (0, 0, 0, 1), and its property (x1, x2, x3, x4) = x1v1 + (x2 − x1)v2 + (x3 − x2)v3 + (x4 − x3)v4 for all (x1, x2, x3, x4) ∈ R 4. Find an expression for T : R4 → R 3, such that T (v1) = (1, 1, 0), T (v2) = (0, 1, 1), T (v3) = (1, 0, 1) and T (v4) = (1, 1, 1). Consider a linear transformation T : U → V and a linear transformation S : V → W . Then for a vector in U , map it by T to V and then use S to map it to W . This is called a composition of two mappings. Definition 9.5. Let U, V and W be vector spaces over F and T : U → V and S : V → W be linear transformations. Then the composition of T and S is defined by the mapping S ◦ T : U → W , with S ◦ T = S(T (u)) ∀u ∈ U. Theorem 9.6. Let U, V and W be vector spaces over F and T : U → V and S : V → W be linear transformations. Then the composition of T and S, denoted by S ◦ T : U → W , is a linear transformation from U to W . Proof:2 Observe that (S ◦ T ) : U → W . Additionally, 2As an exercise, write this proof in 1 paragraph by instead considering (S ◦ T )(λu1 + u2) directly for u1, u2 ∈ U and λ ∈ R. 190 CHAPTER 9. LINEAR TRANSFORMATIONS (i) For all u1, u2 ∈ U we have (S ◦ T )(u1 + u2) = S(T (u1 + u2)) = S(T (u1) + T (u2)) = S(T (u1)) + S(T (u2)) = (S ◦ T )(u1) + (S ◦ T )(u2). (ii) For all λ ∈ R and u ∈ U we have (S ◦ T )(λu) = S(T (λu)) = S(λT (u)) = λS(T (u)) = λ(S ◦ T )(u). Therefore, (S ◦ T ) is a linear transformation from U to W , as required. 9.4 Kernel and image The kernel and the image of a linear transformation T : V → W are important subspaces associated with T . The kernel is a subspace of V , while the image is a subspace of W . Definition 9.7. Suppose that T : V → W is a linear transformation. Then the kernel, is the subset of V defined by ker(T ) = {v ∈ V | T (v) = 0} and the image of T is the subset of W defined by im(T ) = {T (v) | v ∈ V }. Theorem 9.8. Suppose that V and W are vector spaces over F and let T : V → W be a linear transformation. Then ker(T ) is a subspace of V and im(T ) is a subspace of W . Proof: We show that ker(T ) is a subspace of V and leave the second part as an exercise. Since T (0) = 0, the kernel is a non-empty set. Let u, w ∈ ker(T ) and λ ∈ F. Then T (λu + w) = λT (u) + T (w) = λ0 + 0 = 0. Hence λu + w ∈ ker(T ) and therefore ker(T ) is a subspace by the subspace test. 9.5. MATRIX REPRESENTATION 191 Lemma 9.9. Suppose that V and W are vector spaces over F and let T : V → W be a linear transformation. Then (i) T is onto (surjective) if and only if im(T ) = W . (ii) T is one to one (injective) if and only if ker(T ) = {0}. (iii) if {v1, . . . , vn} is a basis for V , then im(T ) = span(T (v1), . . . , T (vn)). Proof: Part (i) is just the definition of a map being onto. Part (ii) is more interesting. Suppose that ker(T ) = {0} and that v, u ∈ V have the property that T (v) = T (u). Then T (v) − T (u) = 0. Since T is a linear transformation, T (v) − T (u) = T (v − u) and so v − u ∈ ker(T ) = {0}. Hence v = u and T is one to one. Suppose that T is one to one. Then 0 is the unique element that maps to 0 and so ker(T ) = {0}. Sometimes ker(T ) is also referred to as the nullspace of T . It can be that ker(T ) = {0}, but this is not always true. For example, ker(O) = V, where O is the zero transformation (see Example 129). The dimension of ker(T ) is called the nullity of the linear transfor- mation T . The subspace im(T ) is also called the range of T or the image space of T . The dimension of the image of T is called the rank of the linear transformation T . Example 132: Find a basis of ker(T ) and im(T ) where T : R 4 → R 3 is the linear transformation given by T ((x1, x2, x3, x4)) = (x1 + x2 + 2x3 + x4, 2x1 + x2 + x3 − x4, 3x1 − x2 − 6x3 − 9x4) for all (x1, x2, x3, x4) ∈ R 4, and hence determine the nullity of T and the rank of T . For the linear transformation used in the previous two exercises, we find that the nullity of T is 2 and the rank of T is 2. Hence, the nullity of T + the rank of T is the dimension of the domain of T , i.e. R 4. This holds more generally: Theorem 9.10. (rank-nullity) Consider two real vector spaces V and W , with dim(V ) = n. Let T : V → W be a linear transformation. Then, rank of T + nullity of T = n. We delay the proof of the rank-nullity theorem until next year. 9.5 Matrix representation Every linear transformation from a real vector space to another real vector space can be described by a matrix. Such a representation, requires specified bases in both vector 192 CHAPTER 9. LINEAR TRANSFORMATIONS spaces. Since the coordinates of a vector with respect to a basis generally differ from the coordinates of the same vector with respect to a different basis, there is no unique matrix representation valid for all bases in a vector space! Definition 9.11. Let V and W be vector spaces over F, and suppose that BV = {v1, v2, . . . , vn} and BW = {w1, w2, . . . , wm} are ordered bases of V and W respectively. Assume that R : V → W is a linear transformation. Then the matrix of R corresponding to the ordered bases BV and BW is the m × n matrix in which the j-th column is given by the transpose of the coordinate vector of R(vj) with respect to BW . To emphasise the dependence of the matrix on the choice of basis, often the notation BV MBW (R), MBV BW (R) or M {R; BV , BW } is used to denote the matrix representing the linear transformation R from V to W corresponding to the bases BV of V and BW of W . Thus if R(v1) = a11w1 + a21w2 + . . . + am1wm, R(v2) = a12w1 + a22w2 + . . . + am2wm, ... ... ... ... R(vn) = a1nw1 + a2nw2 + . . . + amnwm, the matrix of R corresponding to the basis BV in V and BW in W is given by A =       a11 a12 . . . a1n a21 a22 . . . a2n ... ... ... am1 am2 . . . amn       . Hence, we can also write,       R(v1) R(v2) ... R(vn)       =       a11 a21 . . . am1 a12 a22 . . . am2 ... ... ... a1n a2n . . . amn       ·       w1 w2 ... wm       = AT ·       w1 w2 ... wm       . Suppose that v = λ1v1 + λ2v2 + · · · + λnvn so that v has coordinate vector (λ1, λ2, . . . , λn) with respect to BV and R(v) = µ1w1 + µ2w2 + · · · + µmwm 9.5. MATRIX REPRESENTATION 193 has coordinate vector (µ1, µ2, . . . , µm) with respect to BW . Then       µ1 µ2 ... µm       = A ·       λ1 λ2 ... λn       . Example 133: Consider the linear transformation T : R 3 → R 2 given by T ((x1, x2, x3)) = (3x1 − x2 − 4x3, 2x1 + 3x2 − x3) for all (x1, x2, x3) ∈ R3. Find the matrix of T corresponding to (i) the standard ordered bases of R3 and R 2 respectively, (ii) the ordered basis {(1, 1, 0), (1, −1, 0), (1, 1, 1)} of R 3 and the ordered basis {(1, 2), (2, 1)} of R 2. Typically, if we change one of the two bases, the matrix of T will change, but it will always be a m × n matrix. One exception to the matrix changing is the zero transformation, which for whatever basis in V and whatever basis in W is always represented by a m × n zero matrix. Indeed, whatever bases are used, O(vj) = 0w1 + 0w2 + . . . + 0wm. Example 134: Let V be a real vector space with dim(V ) = n and ordered basis B. Find the matrix representing the identity transformation IV : V → V with respect to the basis B in both the domain and co-domain. Example 135: Let B = {(1, 1), (1, 3)} be the ordered basis for the domain of identity transformation IV : R 2 → R 2. The basis for the co-domain is the standard ordered basis of R 2. Determine the matrix representing of the identity transformation with respect to these two bases. Theorem 9.12. Suppose that V and W are finite dimensional vector spaces over F and T : V → W is a linear transformation. Let A be the matrix representing T with respect to a basis BV in V and basis BW in W . Then rank of T = rank of A. Proof: The image of T is spanned by the image of the elements of BV = {v1, . . . , vn}. Thus im(T ) is spanned by the coordinates corresponding to the columns of A. Thus dim T = dim col(A). Since the column rank of A is the rank of A, this proves the claim. 194 CHAPTER 9. LINEAR TRANSFORMATIONS 9.6 Transformation of coordinates Can we use the matrix representation to calculate the image of a vector from a linear transformation? To do this, we need to work with the coordinates of that vector with respect to a given basis. Theorem 9.13. Suppose that V and W are vector spaces over F and R : V → W be a linear transformation. Let BV = {v1, v2, . . . , vn}, and BW = {w1, w2, . . . , wm} be ordered basis of V and W respectively and A be the ma- trix representing R with respect to the bases BV and BW . Assume that v ∈ V , w = R(v) ∈ W and (i) a = (λ1, . . . , λn) are the coordinates of v with respect to BV . (ii) b = (µ1, . . . , µm) are the coordinates of w with respect to BW . Then bT = A · aT . Proof: By the definition of coordinates v = λ1v1 + λ2v2 + . . . + λnvn and R(v) = w = µ1w1 + µ2w2 + . . . + µmwm. (9.6) Now, R(v) = R(λ1v1 + λ2v2 + . . . + λnvn) = λ1R(v1) + λ2R(v2) + . . . + λnR(vn) = λ1(a11w1 + a21w2 + . . . + am1wm) +λ2(a12w1 + a22w2 + . . . + am2wm) ... +λn(a1nw1 + a2nw2 + . . . + amnwm) = (λ1a11 + λ2a12 + . . . + λna1n)w1 +(λ1a21 + λ2a22 + . . . + λna2n)w2 ... +(λ1am1 + λ2am2 + . . . + λnamn)wm. (9.7) 9.6. TRANSFORMATION OF COORDINATES 195 Since R(v) is expressed uniquely as a linear combination of the vectors in the basis BW , it follows from (9.7) that µ1 = λ1a11 + λ2a12 + . . . + λna1n, µ2 = λ1a21 + λ2a22 + . . . + λna2n, ... µm = λ1am1 + λ2am2 + . . . + λnamn, or, in matrix form,       µ1 µ2 ... µm       =       a11 a12 . . . a1n a21 a22 . . . a2n ... ... ... am1 am2 . . . amn       ·       λ1 λ2 ... λn       , which completes the proof, as required. Example 136: Consider the linear transformation T : R 4 → R 3, with matrix represen- tation    1 2 −3 1 1 −3 1 −2 2 1 −3 4    with respect to the ordered basis BV = {(1, 2, −2, −1), (2, 1, −1, 4), (3, 0, −3, 2), (3, 7, −5, 1)}, in R4 and the ordered basis BW = {(1, 2, −3), (1, 3, 2), (2, −1, −5)} in R3. Determine the image T (v) ∈ R 3 of the vector v ∈ R4 with coordinates (5, −1, 4, 2) relative to BV . The result highlighted in the previous example is a general one: Theorem 9.14. Suppose that U, V and W are vector spaces over F and T : U → V and S : V → W are linear transformations. Consider the ordered bases BU , BV and BW of U , V and W respectively. Let dim(U ) = n, dim(V ) = m and dim(W ) = p. Assume that (i) B ∈ Mmn(F) be the matrix representing the linear transformation T with respect to the bases BU and BV , (ii) A ∈ Mpm(F) is the matrix representing the linear transformation S with re- spect to the bases BV and BW . Then C = A · B ∈ Mpn(F) is the matrix representing the composition of T and S, S ◦ T : U → W , with respect to the bases BU and BW . 196 CHAPTER 9. LINEAR TRANSFORMATIONS Proof: We have u = n∑ i=1 λiui ∈ U. (9.8) So that the coordinate vector for u is (λ1, . . . , λn) with respect to BU . Let (µ1, . . . , µm) be the coordinate vector for v = T (u) with respect to BV . Then     µ1 ... µm     = B     λ1 ... λn     . The coordinate vectors for S(w) with respect to BW is then (τ1, . . . , τp) where     τ1 ... τp     = A     µ1 ... µm     . Hence     τ1 ... τp     = A(B     λ1 ... λn    ) = (A · B)     λ1 ... λn     . Hence, the matrix representing S ◦ T with respect to BU in the domain and BW in the co-domain is C = A · B, as required. Example 137: Consider the two ordered bases B and ˜B in R2, with B = {(1, 1), (1, 2)} and ˜B = {(2, 3), (3, 2)}. Consider the identity transformation 1R2 : R2 → R 2. Then, (i) determine the matrix representing A1 of 1R2 with respect to the basis B (in the domain) and ˜B (in the co-domain), (ii) determine the matrix representing A2 of 1R2 with respect to the basis ˜B (in the domain) and B (in the co-domain), (iii) evaluate the matrix products A1 · A2 and A2 · A1. Theorem 9.15. Suppose that V is a vector space over F, with dim(V ) = n. Con- sider two ordered bases B and ˜B for V . Let (i) the n × n matrix A1 be the matrix representing the identity transformation 1V with respect to the bases B in the domain and ˜B in the co-domain, (ii) the n × n matrix A2 be the matrix representing the identity transformation 1V with respect to the bases ˜B in the domain and B in the co-domain. Then, A1 · A2 = A2 · A1 = In. 9.7. TRANSITION MATRICES; CHANGE OF BASIS MATRICES 197 Proof: See practise questions. This implies that both matrix representations A1 and A2 for the identity transformation 1V : V → V are invertible and that A2 = A−1 1 , i.e. A2 is the unique inverse of A1. 9.7 Transition matrices; change of basis matrices Consider two ordered bases B and ˜B in the same vector space V . Any vector can be written uniquely as a linear combination of vectors in each basis, yielding two different sets of coordinates. Like with a linear transformation, the relationship between these two sets of coordinates is fully determined if we know how the vectors in one basis are expressed in terms of the other. Definition 9.16. Let V be a vector space over F with dim(V ) = n and bases B and ˜B given by B = {u1, u2, . . . , un} and ˜B = {v1, v2, . . . , vn}. The matrix of transition from the ordered basis B to the ordered basis ˜B is the n × n matrix P which has j-th column the transpose of the coordinate vector of uj ∈ B written with respect to the basis ˜B. So, if IV (u1) = u1 = p11v1 + p21v2 + . . . + pn1vn, IV (u2) = u2 = p12v1 + p22v2 + . . . + pn2vn, ... IV (un) = un = p1nv1 + p2nv2 + . . . + pnnvn, the transition matrix P is given by P =       p11 p12 . . . p1n p21 p22 . . . p2n ... ... ... pn1 pn2 . . . pnn       . (9.9) The relationship between the vectors of the new basis and the vectors of the old basis can then be written in short form as       u1 u2 ... un       = P T ·       v1 v2 ... vn       . 198 CHAPTER 9. LINEAR TRANSFORMATIONS Example 138: Consider the ordered bases B = {(1, 1), (1, 2)} and ˜B = {(2, 3), (3, 2)} in R2. Determine the matrix of transition from B to ˜B. Comparing this result with the result from the last exercise in the previous section, we can see that the transition matrix of B to ˜B is equal to the matrix representing the identity transformation 1R2 relative to the basis B in the domain and ˜B in the co-domain. Again, this is an example of a more general result. Theorem 9.17. Let V be a vector space over F of dimension n and let B and ˜B be ordered bases for V . Let the n×n matrix P (as in (9.9)) be the matrix representation of the identity transformation 1V with respect to the bases B in the domain and ˜B in the co-domain. Then P is the matrix of transition in V from the ordered basis B to the ordered basis ˜B. Proof: Follows immediately from Definition 9.16 and the definition of the identity transformation. This immediately implies that a transition matrix P from the ordered basis B to the ordered basis ˜B is invertible and that P−1 is the matrix of transition from the ordered basis ˜B to the ordered basis B. Now if (λ1, λ2, . . . , λn) is the coordinate vector w in V with respect to the basis ˜B = {v1, v2, . . . , vn}, we have that w = λ1v1 + λ2v2 + . . . + λnvn. Similarly, let (µ1, µ2, . . . , µn) the coordinate vector of w in V with respect to the basis B = {u1, u2, . . . , un}, or w = µ1u1 + µ2u2 + . . . + µnun. Since 1V (w) = w, the transformation of coordinates formula in Theorem 9.13, we have,       λ1 λ2 ... λn       =       p11 p12 . . . p1n p21 p22 . . . p2n ... ... ... pn1 pn2 . . . pnn       ·       µ1 µ2 ... µn       . This can now be read as stating that the coordinates of a vector with respect to the “new\" basis ˜B can be found by the matrix product of the transition matrix from B to ˜B with the column vector containing the coordinates with respect to the “old\" basis B. 9.7. TRANSITION MATRICES; CHANGE OF BASIS MATRICES 199 Please note that not all textbooks/sources will follow this definition of a transition matrix. Some 3 will define the transition matrix as being P −1. Example 139: Consider the linear transformations T : U → V and S : V → W with U = R 2, V = R4 and W = R 3 with T ((x1, x2)) = (x1 + 2x2, −2x1 + 3x2, 3x1 − 2x2, −x1 − x2), S((x1, x2, x3, x4)) = (2x1 + x2 + x3 − x4, 3x1 − x3 + x4, x1 − 4x2 + 2x3) for all (x1, x2) ∈ R 2 and (x1, x2, x3, x4) ∈ R 4 with respect to standard ordered bases. Consider the following bases for R 2, R 4 and R 3 respectively: BU = {(1, 1), (1, −1)}, BV = {(1, 0, 0, 0), (1, 1, 0, 0), (1, 1, 1, 0), (1, 1, 1, 1)}, BW = {(0, 0, 1), (0, 1, 1), (1, 1, 1)}. Determine the matrix representation of T with respect to the bases BU and BV , the matrix representation of S with respect to the bases BV and BW , and, the matrix representation of S ◦ T with respect to the bases BU and BW . 3So if using textbooks or alternative sources for additional support, please remember this. Appendix A Sets and Notation ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • understand what sets are; • recall the basic properties of sets; • understand and utilize common notation used to describe sets; and • use basic operations to manipulate sets. [5, p.1-22 and p.165-200] contain alternative presentations of material in this chapter that you may find helpful. A.1 Introduction Definition A.1. A set is a collection of uniquely identifiable objects which are known as elements or members. A set which contains elements with a property P is denoted { x : x has property P } where the notation “ : \" means “such that\". In simple cases, we sometimes list the elements of a set, for example, {x : x = a or x = b} can be written {a, b}. Example 140: Consider the sets S = {1, 11, 2000} , R = {♣, ♠, 5} and Q = {pear, plum, banana, pear} . Do all 3 sets contain the same number of elements? i ii APPENDIX A. SETS AND NOTATION Note that the sets S, R and Q in Example 140 have three elements since there is a repeated element (pear) in Q i.e. repetitions are ignored (since they correspond to elements that are already uniquely defined). We usually use {·} (curly brackets) to denote a set, and for small sets we can simply list the elements. Sometimes, if it is clear, we abbreviate using the following notation for larger sets: {1, 3, 5, . . . , 99} or {2, 4, . . . , 1024} . The rule in this second example is not clear. Are the elements even numbers or powers of 2? Without additional information, one cannot know which elements are in this set. Example 141: We use notation below to denote standard sets that appear throughout the course1: N = {1, 2, 3, 4, 5, . . .} (natural numbers), Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .} (integers), Q = {m n : m, n ∈ Z, n > 0 } (rational numbers). Observe that 2 3, 23, −7 8 , −77 8 ∈ Q where “ ∈ \" means “is an element of\". We also have the following special cases: N0 = {0, 1, 2, 3, 4, 5, . . .} , Z + = {x ∈ Z : x > 0} = {1, 2, 3, . . .} = N, Z + 0 = {x ∈ Z : x ≥ 0} = {0, 1, 2, 3, 4, 5, . . .} = N0, Z − = {x ∈ Z : x < 0} = {. . . , −3, −2, −1} , Z − 0 = {x ∈ Z : x ≤ 0} = {. . . , −3, −1, −1, 0} , Q + = {x ∈ Q : x > 0} , Q + 0 = {x ∈ Q : x ≥ 0} , Q− = {x ∈ Q : x < 0} , Q− 0 = {x ∈ Q : x ≤ 0} . In Example 141, we have defined a set by a defining property which is written as A = {x : x satisfies P } . Sometimes ‘ : ’ is replaced by ‘ | ’ so that the set A may be written A = {x | x satisfies P } . 1These notations may differ slightly in reference texts and other courses. A.2. INTERVAL NOTATION iii Example 142: Let A be the set of all even natural numbers, A = {x : x ∈ N and is divisibe by 2} = {2, 4, 6, 8, 10, 12, . . .} . The set A can also be expressed as 2N (the set with elements of the form 2n where n is a natural number). To ‘complete’ the rational numbers (to express everything on the number-line), we require irrational numbers (for instance √2, √3, π, ln 2, e, . . . ). The set containing all rational and irrational numbers is referred to as the set of real numbers2 and denoted by R = the set of all real numbers. R can be thought of as being inside C (the set of complex numbers), which will be discussed in Chapter 7. Some sets are finite, for example A = {1, 2, 3}, and others are infinite, for example R, Q, N etc. For finite sets we use |A| to denote the number of elements in the set or equivalently, the size of the set. |A| is known as the order or cardinality of the set A. So for A = {1, 2, 3}, the cardinality is given by |A| = 3. A.2 Interval Notation Consider two real numbers, a and b, with a ≤ b. Intervals of finite length are represented as follows: [a, b] = {x ∈ R : a ≤ x ≤ b} with a ≤ b Closed Interval [a, a] = {a} Open Interval (a, b) = {x ∈ R : a < x < b} (a, b] = {x ∈ R : a < x ≤ b} Half Open [a, b) = {x ∈ R : a ≤ x < b} 2For a detailed construction of the real numbers, see [14, p.101-110], but note that this is not within the scope of this course. iv APPENDIX A. SETS AND NOTATION Moreover, intervals of infinite length can be represented as follows: [a, ∞) = {x ∈ R : x ≥ a} , (−∞, b) = {x ∈ R : x < b} . ⋆⋆ WARNING ‘∞’ is not a number: it is used here as a form of shorthand ⋆⋆ Example 143: We can express R +, R+ 0 , R − and R − 0 in interval notation: R + = {x ∈ R : x > 0} = (0, ∞), R+ 0 = {x ∈ R : x ≥ 0} = [0, ∞), R − = {x ∈ R : x < 0} = (−∞, 0), R − 0 = {x ∈ R : x ≤ 0} = (−∞, 0]. A.3 Inclusion Among Sets Definition A.2. We write A ⊆ B when every element of A is also an element of B. Equivalently, A ⊆ B if either of the following equivalent statements hold: • for all x, if x ∈ A then x ∈ B; and • ∀x : x ∈ A =⇒ x ∈ B. Note that the notation “ ∀ \" means “for all/for each\" or “for any\", and “ =⇒ \" means “implies\" or “then\". Additionally the “ : \" in Definition A.2 is just a colon, not “such that\". If A ⊆ B, we say that “A is a subset of B\". Example 144: 2N ⊆ N, {1, 2} ⊆ {1, 2, 3} , {1, 2, 3} ⊆ {1, 2, 3} , N ⊂ Z ⊂ Q ⊂ R. For strict inclusion we write A ⊂ B, which means A ⊆ B but A ̸= B. When A ⊂ B we say “A is a strict subset of B.\" A.4. INTERSECTION, UNION AND DIFFERENCE v Definition A.3. Two sets A and B are equal, denoted A = B, if and only if, A ⊆ B and B ⊆ A. As for Definition A.2, Definition A.3 can be equivalently given as: A = B if and only if, for all x : (x ∈ A =⇒ x ∈ B) and (x ∈ B =⇒ x ∈ A), or A = B iff ∀x : x ∈ A ⇐⇒ x ∈ B. Note that in order to prove that two sets are equal, you must prove both inclusions. Moreover, the words “if and only if\" are often shortened to “iff\". The notation “ A ⇐⇒ B \" means “A =⇒ B and B =⇒ A \". Example 145: Which set inclusions hold for A = {1, 2, 3} and B = {1, 2, 3, {3}}? Answer: Since 1, 2, 3 ∈ B and A = {1, 2, 3}, it follows that for all x, if x ∈ A then x ∈ B. Therefore A ⊆ B. However, since {3} ∈ B but {3} ̸∈ A, it follows that A ̸= B and hence A ⊂ B. We can also conclude that B ̸⊆ A. A.4 Intersection, Union and Difference Definition A.4. For sets A and B, we define A ∩ B, the intersection of A and B as, A ∩ B = {x : (x ∈ A) and (x ∈ B)} . Example 146: {1, 2, 3} ∩ {2, 4, 6} = {2} , N ∩ 2N = 2N. Definition A.5. For sets A and B, we define A ∪ B, the union of A and B, as, A ∪ B = {x : (x ∈ A) or (x ∈ B)} . In this case the “or\" is an inclusive 3 “or\" that embraces x ∈ A or x ∈ B, or, both x ∈ A and x ∈ B. 3Note that an “exclusive or\" here would mean (x ∈ A and x ̸∈ B) or (x ∈ B and x ̸∈ A) vi APPENDIX A. SETS AND NOTATION Example 147: {1, 2, 3} ∪ {2, 4, 6} = {1, 2, 3, 4, 6} , N ∪ 2N = N. Definition A.6. For sets A and B, we define A \\ B, the difference of A and B, as, A \\ B = {x : (x ∈ A) and (x /∈ B)} . Example 148: {1, 2, 3} \\ {2, 3, 6} = {1} , N \\ 2N = {1, 3, 5, 7, 9, . . .} . Some authors use alternative notation for the difference of sets A and B, for example, A− B. It is often useful to use number-lines or Venn diagrams to determine the intersection, union or difference of sets. A.5 The Empty Set We can define the empty set in the following way: Definition A.7. The empty set is a set that contains no elements and is denoted by ∅. Equivalently, ∅ = {x : x ̸= x} . It follows that: |∅| = 0, | {∅} | = 1, | {∅, {∅}} | = 2. Also note that for any set A, A ∪ ∅ = A and A ∩ ∅ = ∅. A.6. OPERATIONS WITH SETS vii A.6 Operations With Sets Before considering operations with sets, recall these properties which describe familiar properties of numbers (for example, numbers in R): Remark A.8. Let a, b, c ∈ R. (i) Addition is Commutative. That is a + b = b + a, i.e. order of the addition operation does not matter; (ii) Addition is Associative. That is a + (b + c) = (a + b) + c, i.e. re-bracketing does not effect addition; and (iii) Addition satisfies the Distributive Law. That is a(b + c) = ab + ac, i.e. expanding brackets is allowed. We can look for similar properties in set operations, specifically: A ∩ B = B ∩ A and A ∪ B = B ∪ A, so intersect and union set operations are commutative. However, clearly the difference set operation is not commutative 4, since in general, A ∖ B ̸= B ∖ A. Theorem A.9. For any three sets A, B and C: (i) A ∩ (B ∩ C) = (A ∩ B) ∩ C , (ii) A ∪ (B ∪ C) = (A ∪ B) ∪ C . Proof: To establish (i), we must show that for arbitrary x: x ∈ A ∩ (B ∩ C) implies x ∈ (A ∩ B) ∩ C, and; x ∈ (A ∩ B) ∩ C implies x ∈ A ∩ (B ∩ C). Since, x ∈ (A ∩ B) ∩ C ⇐⇒ x ∈ (A ∩ B) and x ∈ C ⇐⇒ x ∈ A and x ∈ B and x ∈ C ⇐⇒ x ∈ A and x ∈ (B ∩ C) ⇐⇒ x ∈ A ∩ (B ∩ C), 4To make this clear, we should provide a suitable counter-example. viii APPENDIX A. SETS AND NOTATION we conclude that (A ∩ B) ∩ C ⊆ A ∩ (B ∩ C) and A ∩ (B ∩ C) ⊆ (A ∩ B) ∩ C. Hence, (A ∩ B) ∩ C = A ∩ (B ∩ C), as required. To prove (ii), a similar argument can be used, and you should provide the details (see practice questions). Theorem A.9 states that the set operations union and intersection, are associative (or satisfy the associative law). Example 149: Is A ∩ (B ∪ C) = (A ∩ B) ∪ C? Answer: In general, the answer is no. For example let A = ∅, B = Z and C = Q . Then since ∅ ⊂ Z ⊂ Q, A ∩ (B ∪ C) = A ∩ Q = ∅ ∩ Q = ∅ and (A ∩ B) ∪ C = (∅ ∩ Z) ∪ Q = Q. Therefore, A ∩ (B ∪ C) ̸= (A ∩ B) ∪ C. Example 150: Consider the sets A, B and C given by, A = {apple, passion fruit} , B = {apple, pear} , C = {tomato, pear} . Show that A ∩ (B ∪ C) ̸= (A ∩ B) ∪ C. Answer: Since A ∩ (B ∪ C) = {apple, passion fruit} ∩ {apple, pear, tomato} = {apple} and (A ∩ B) ∪ C = {apple} ∪ {tomato, pear} = {apple, tomato, pear} , it follows that A ∩ (B ∪ C) ̸= (A ∩ B) ∪ C. Note that in the proof of Theorem A.9 and Examples 149 and 150, we see an important feature of mathematics: a true statement (a theorem, proposition, lemma etc) requires proof, whereas a false statement requires a counterexample. A.7. THE UNIVERSAL SET ix A.7 The Universal Set The sets we are interested in are often contained in a larger universal set, and hence, we can define the complement of a set A contained within this universal set. Definition A.10. Let U be a set, namely, the universal set. For A ⊆ U , we define the complement of A (with respect to U ) as A′, with A ′ = {x ∈ U : x /∈ A} . The complement of A ⊆ U as in Definition A.10 can be expressed as A ′ = U \\ A. Example 151: If U = R and A = Z then A ′ = {x ∈ R : x /∈ Z } = {x ∈ R : x ̸= . . . , −2, −1, 0, 1, 2, . . .} . The set in Example 151 can also be written using interval notation, specifically: A ′ = · · · ∪ (−2, −1) ∪ (−1, 0) ∪ (0, 1) ∪ (1, 2) ∪ · · · = ⋃ n∈Z(n, n + 1). Example 152: What are the complements of the universal set and the empty set? Answer: U ′ = ∅ and ∅′ = U . A.8 de Morgan’s Laws Theorem A.11. (de Morgan’s Laws) For sets A and B contained in a universal set U : (i) (A ∪ B)′ = A′ ∩ B′, (ii) (A ∩ B)′ = A′ ∪ B′. Proof: To establish (i), we must show that for arbitrary x: x ∈ (A ∪ B)′ implies x ∈ A ′ ∩ B′, and; x ∈ A ′ ∩ B′ implies x ∈ (A ∪ B)′. Since for x ∈ U , x ∈ (A ∪ B) ′ ⇐⇒ x ̸∈ (A ∪ B) x APPENDIX A. SETS AND NOTATION ⇐⇒ x ̸∈ A and x ̸∈ B ⇐⇒ x ∈ A′ and x ∈ B′ ⇐⇒ x ∈ A′ ∩ B′, it follows (as in the proof of Theorem A.9 since all implications above are if and only if) that (A ∪ B)′ = A′ ∩ B′, as required. To prove (ii), a similar argument can be used, and ... again, you should provide it (see practice questions). Corollary A.12. For any sets A, B and C contained in a universal set U : (A ∪ B ∪ C)′ = A′ ∩ B′ ∩ C ′. Proof: Let A1 = A and A2 = B ∪ C. Then, via two applications of Theorem A.11, we have, (A ∪ B ∪ C) ′ = (A ∪ (B ∪ C)) ′ = (A1 ∪ A2)′ = A ′ 1 ∩ A ′ 2 = A ′ 1 ∩ (B ∪ C)′ = A ′ 1 ∩ (B′ ∩ C ′) = A ′ ∩ B′ ∩ C ′, as required. Theorem A.13. Distributive law for intersection and union: For any sets A, B and C: (i) A ∩ (B ∪ C) = (A ∩ B) ∪ (A ∩ C), (ii) A ∪ (B ∩ C) = (A ∪ B) ∩ (A ∪ C). Proof: To establish (i), since x ∈ A ∩ (B ∪ C) ⇐⇒ (x ∈ A) and x ∈ (B ∪ C) ⇐⇒ (x ∈ A) and (x ∈ B or x ∈ C) ⇐⇒ (x ∈ A and x ∈ B) or (x ∈ A and x ∈ C) ⇐⇒ (x ∈ A ∩ B) or (x ∈ A ∩ C) ⇐⇒ x ∈ (A ∩ B) ∪ (A ∩ C), A.9. CARTESIAN PRODUCT xi it follows that A ∩ (B ∪ C) = (A ∩ B) ∪ (A ∩ C), as required. To establish (ii), a similar argument can be employed, and ... once again, you should provide it (see practice questions). Results similar to Corollary A.12 and Theorem A.13 which are for operations on n sets (for n ∈ N with n ≥ 3) can be established by mathematical induction5, which we will consider in Chapter 2. A.9 Cartesian Product Definition A.14. The Cartesian product of sets A and B, denoted A×B, is defined as, A × B = {(a, b) : a ∈ A, b ∈ B}. Example 153: Let A = {1, 2, 3} and B = {3, 5}. Then, A × B = {(1, 3), (1, 5), (2, 3), (2, 5), (3, 3), (3, 5)}. Note that |A| = 3, |B| = 2 and |A × B| = 6. Example 154: Let A = {♡, ♣} and B = {x, y, z}. Observe that A × B = {(♡, x), (♡, y), (♡, z), (♣, x), (♣, y), (♣, z)}, B × A = {(x, ♡), (y, ♡), (z, ♡), (x, ♣), (y, ♣), (z, ♣)}, and hence, A × B ̸= B × A. From Definition A.14 we can define functions from a relational approach6. Definition A.15. Consider the sets X, Y and R ⊆ X × Y . The set R is referred to as a binary relation over X and Y . Suppose that: • for each x ∈ X, there exists (x, y) in R; and • if (x, y1), (x, y2) ∈ R for some x ∈ X and y1, y2 ∈ Y , then y1 = y2. Then R defines a function f : X → Y via the rule f (x) = y for all (x, y) ∈ R. For example, R = X ×Y in Example 153 does not define a function (since (x, y1), (x, y2) ∈ R does not imply that y1 = y2). However, R = {(1, 3), (2, 5), (3, 5)} ⊂ X × Y defines a function f : X → Y given by: f (1) = 3, f (2) = 5 and f (3) = 5. 5We note here that “De Morgan’s Laws\" are named after British Mathematician Augustus De Morgan (1806-1871) who also gave the first rigorous treatment of mathematical induction [9]. 6You will consider relations in more detail in J1AC [7]. xii APPENDIX A. SETS AND NOTATION Appendix B Mathematical Induction ▶ Learning Outcomes ◀ After the completion of the lectures and support sessions associated with this chapter you should be able to: • recall the statement of common forms of mathematical induction and understand how these are equivalent in simple cases; and • produce correctly structured arguments using the principle of mathemat- ical induction. [2, p.109-115] and [5, p.201-207] contain alternative presentations of material in this chapter that you may find helpful. Mathematical Induction is a method used to prove statements which hold for all n ∈ N. We denote such statements by P (n) for each n ∈ N. Example 155: Consider the statement P (n) for n ∈ N given by, the sum of the first n natural numbers is equal to 1 2n(n + 1). Equivalently, the statement P (n) for n ∈ N is given by, n∑ i=1 i = 1 + 2 + · · · + (n − 1) + n = 1 2 n(n + 1). xiii xiv APPENDIX B. MATHEMATICAL INDUCTION Theorem B.1. (Principle of mathematical induction) Let P (n) be a state- ment for each n ∈ N. Additionally, suppose that both of the following statements are satisfied: (i) P (1) is true; and (ii) for each k ∈ N, we have P (k) is true =⇒ P (k + 1) is true. Then P (n) is true for all n ∈ N. Note that we refer to P (n), condition (i) and condition (ii) in Theorem B.1 as the induc- tion hypothesis, the base step and the induction step, respectively. Additionally, note that we will prove Theorem B.1 using proof by contradiction, a widely used method of proof. Proof: Suppose that for some n ∈ N, the statement P (n) is false. Therefore, there exists ¯n ∈ N, for which, the statement P (¯n) is false, and via (i), P (k) is true for all k ∈ N such that 1 ≤ k ≤ ¯n − 1. Since P (¯n − 1) is true, (ii) implies that P (¯n) is true, which contradicts the statement P (¯n) is false. Therefore, the supposition is invalid, and we conclude that P (n) is true for all n ∈ N, as required. Great care is needed to write good (and hence useful) proofs using the PMI (Principle of Mathematical Induction). The following example (with footnotes) provides a good model which you should use as a template in your proofs which use the PMI. Example 156: Show, using the principle of mathematical induction, that for all n ∈ N, n∑ i=1 i = 1 2 n(n + 1). Answer: Let P (n) for n ∈ N be the statement 1 n∑ i=1 i = 1 + 2 + · · · + (n − 1) + n = 1 2 n(n + 1). (B.1) To establish condition (i) in Theorem B.1, observe that when n = 1, the LHS (left hand side) and RHS (right hand side) of (B.1) are given respectively by 1∑ i=1 i = 1 and 1 2 (1)(1 + 1) = 1, (B.2) 1You should first state the induction hypothesis P (n). xv so P (1) is true, (B.3) i.e. the base step of Theorem B.1 is satisfied2. To establish condition (ii) in Theorem B.1 for P (n), suppose that P (k) is true for some k ∈ N, or equivalently via (B.1), assume that k∑ i=1 i = 1 2 k(k + 1). (B.4) For n = k + 1, the LHS of (B.1) is given by k+1∑ i=1 i = k∑ i=1 i + (k + 1) = ( 1 2k(k + 1)) + (k + 1) (via (B.4)) = (k + 1) ( 1 2k + 1) = 1 2 (k + 1)(k + 2). (B.5) Since the RHS of (B.5) is the RHS of (B.1) for n = k + 1, it follows from (B.4)-(B.5) that for k ∈ N, P (k) is true =⇒ P (k + 1) is true, (B.6) i.e. P (n) satisfies the induction step3 in Theorem B.1. Therefore, via (B.3) and (B.6), P (n) given by (B.1) satisfies the conditions of Theorem B.1, and we conclude 4 that P (n) is true for all n ∈ N, as required. Note that the statement P (n) is written explicitly in (B.1). Moreover, observe that the equations are numbered in the proof, and these are referred to in the text of the proof (and subsequent remark). When proving statements yourself, you are advised to include numbering next to all calculations (at first) so you can refer to them in the text that justifies your arguments (unused equation numbers can be removed afterwards). 2Secondly you should show that the base step is satisfied. 3Thirdly you should show that P (n) satisfies the induction step. 4Finally, refer to the principle of mathematical induction (in this case Theorem B.1) to conclude your proof. xvi APPENDIX B. MATHEMATICAL INDUCTION Theorem B.2. (Principle of Mathematical Induction) Let P (n) be a state- ment for each n ∈ Z with n ≥ m, for some m ∈ Z. Additionally, suppose that both of the followings statements are satisfied: (i) P (m) is true; and (ii) for each k ∈ Z with k ≥ m, we have P (k) is true =⇒ P (k + 1) is true. Then, P (n) is true for all n ∈ Z with n ≥ m. Proof: The statement Q(n) defined by Q(n) = P (n + m − 1) ∀n ∈ N, satisfies the conditions of Theorem B.1. From Theorem B.1 we conclude that Q(n) is true for all n ∈ N and therefore, P (n) is true for all n ∈ Z with n ≥ m, as required. Example 157: Show, using the principle of mathematical induction, that 3 n ≥ 10 + 2n ∀n ∈ Z with n ≥ 3. Answer: Let P (n) for n ∈ Z with n ≥ 3, be the statement 3 n ≥ 10 + 2n. (B.7) We now show that P (n) satisfies the conditions of Theorem B.2 with m = 3. To establish that P (n) satisfies condition (i), observe that the LHS and RHS of (B.7) with n = 3 are given respectively by, 3 3 = 27 and 10 + 23 = 18. (B.8) Since 27 > 18 it follows from (B.7) and (B.8) that P (3) is true. (B.9) To establish that P (n) satisfies condition (ii) with m = 3, suppose that for some k ∈ Z with k ≥ 3, that P (k) is true, i.e. 3 k ≥ 10 + 2k. (B.10) Via (B.7), the LHS of P (k + 1) is given by 3 k+1 = 3 × 3 k xvii ≥ 3 × (10 + 2k) (via (B.10)) = 30 + (3 × 2 k) ≥ 10 + 2 × 2 k = 10 + 2k+1 = RHS of P (k + 1). (B.11) Thus, via (B.10) and (B.11), for k ∈ Z with k ≥ 3, we conclude that P (k) is true =⇒ P (k + 1) is true. (B.12) Therefore, via (B.9) and (B.12) respectively, the base step and induction step in Theorem B.2 are satisfied (m = 3) and hence, via Theorem B.2 we conclude that for all n ∈ Z with n ≥ 3, that P (n) given by (B.7) is true, as required. The hardest situations arise where some initial guess has to be made for the base step, for example: Example 158: n! (pronounced n factorial) is defined as n! = n∏ i=1 i = n × (n − 1) × (n − 2) × . . . 3 × 2 × 1. Using Theorem B.2, find m ∈ Z such that n! > 10 n for all n ∈ Z with n ≥ m. For example, when n = 3, 3! = 6 and 10 3 = 1000 i.e. the statement does not hold for n = 3. Note that to apply the principle of mathematical induction, as stated in Theorems B.1 and B.2, the base step and induction step need to be satisfied by the induction hypothesis. If one does not establish conditions in these theorems before drawing conclusions from them, some ‘invalid’ statements might be claimed. To highlight this, see if you can find the mistakes in the following two examples. Example 159: All natural numbers of the form Fn = 2 2n + 1 with n ∈ N0 are prime numbers. Proof: It follows that F0 = 3 which is a prime number. F1 = 5 which is a prime number. F2 = 17 which is a prime number. F3 = 257 which is a prime number. F4 = 65537 which is a prime number. ... One can establish that for k ∈ N0, if Fk is a prime number, then Fk+1 is a prime number. Therefore, the conditions of Theorem B.2 are satisfied (with m = 0), and we conclude that P (n) is true for all n ∈ N0, as required. xviii APPENDIX B. MATHEMATICAL INDUCTION Example 160: Prove that every person in the room has the same hairstyle. Proof: Consider the statement P (n) for n ∈ N given by For any n people in this room, they all have the same hairstyle. Now, for any 1 person in the room, they have the same hairstyle as ... themselves, and hence P(1) is true. Now assume that P (k) is true for some k ≥ 1. Then for any group of k + 1 people, remove 1 person, and hence via the assumption, all remaining k people have the same hairstyle. Additionally, consider the same k + 1 people and remove a different person. Via the assumption, it follows that all remaining k people have the same hairstyle, and hence all k + 1 people have the same hairstyle. Therefore, the conditions of Theorem B.1 are satisfied, and we conclude that P (n) is true for all n ∈ N, as required. Appendix C Conics: Optional Extra Content C.1 General notions C.1.1 Intersection of a cone and a plane Quadratic equations in 2 variables occur frequently in applications of mathematics, and hence, it is useful to classify their solution structure. We proceed with this classification in the following subsections, giving details at the end of the chapter. Definition C.1. A (real) quadratic equation in 2 variables is given by, Ax2 + Bxy + Cy2 + Dx + Ey + F = 0, for A, B, C, D, E, F ∈ R. Consider a cone with its vertex at the origin in 3-dimensional space and axis of symmetry along the z-axis. Its equation is given by x2 + y2 = k2z2, for some k > 0. The general equation of a plane in 3-dimensional space is ax + by + cz + d = 0 for a, b, c, z ∈ R. If we can eliminate z using the equation of the plane (i.e. c ̸= 0 1) we get x2 + y2 = k2 ( −d − ax − by c )2 , 1When c = 0 we can substitute x or y instead of z and derive a similar equation in 2 variables. xix xx APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT which rearranges to a quadratic equation in 2 variables, with A = a2 − c2 k2 , B = 2ab, C = b2 − c2 k2 , D = 2ad, E = 2bd, F = d2. (C.1) In standard form, we note that the quadratic equation in 2 variables describing the ellipse and hyperbola have B, D and E equal to 0, whereas for a parabola, B, C, D and F equal 0. C.1.2 Rotation of the coordinate system In this subsection, we establish that via a change of coordinates (a rotation), we can eliminate the xy term in a quadratic equation in 2 variables. We achieve this by setting the transformed coefficient of ˜x˜y, namely ˜B, equal to 0. This quadratic equation, with no ˜x˜y term, can then be simplified further by a translation (change of origin of the coordinate system). Consider a coordinate system which is rotated by an angle α, which has the following transformation formulae, { ˜x = x cos(α) + y sin(α), ˜y = −x sin(α) + y cos(α), { x = ˜x cos(α) − ˜y sin(α), y = ˜x sin(α) + ˜y cos(α), (C.2) See Figure C.1 for a geometric interpretation of the new coordinate system. i j ˜i ˜j α α •P = xi + yj = ˜x˜i + ˜y˜j • Q •R • S •T O Figure C.1: Sketch of the rotated coordinate system. Here, that ⃗OQ = xi, ⃗OR = yj, ⃗OT = ˜xi and ⃗OS = ˜yj. C.1. GENERAL NOTIONS xxi Note that the coordinate relations in (C.2) can be given as, (˜x ˜y ) = ( cos (α) sin (α) − sin (α) cos (α) ) · ( x y ) , ( x y ) = ( cos (α) − sin (α) sin (α) cos (α) ) · (˜x ˜y ) (C.3) The 2 × 2 matrices in (C.3) are referred to as rotation matrices. Using (C.2), the quadratic equation in Definition C.1 can be transformed into the equation A (˜x cos(α) − ˜y sin(α)) 2 + B (˜x cos(α) − ˜y sin(α)) (˜x sin(α) + ˜y cos(α)) + C (˜x sin(α) + ˜y cos(α)) 2 + D (˜x cos(α) − ˜y sin(α)) + E (˜x sin(α) + ˜y cos(α)) + F = 0, which can be rewritten as (A cos 2(α) + B sin(α) cos(α) + C sin2(α) ) ˜x2 + (−2A sin(α) cos(α) + B cos 2(α) − B sin2(α) + 2C sin(α) cos(α) ) ˜x˜y + (A sin2(α)) − B sin(α) cos(α) + C cos 2(α) ) ˜y2 + (D cos(α) + E sin(α)) ˜x + (−D sin(α) + E cos(α)) ˜y + F = 0. The ˜x˜y term vanishes when B cos(2α) + (C − A) sin(2α) = 0, or when α satisfies one of the equations cot(2α) = A − C B or tan(2α) = B A − C . C.1.3 Translation of the coordinate system If we have the quadratic equation in the form ˜A˜x2 + ˜C ˜y2 + ˜D˜x + ˜E ˜y + ˜F = 0, (C.4) we can eliminate the linear terms in ˜x and ˜y (assuming ˜A, ˜C ̸= 0) by either translating the coordinate system (or by completing the squares in the expression). The equation then becomes, ˜A (˜x + ˜D 2 ˜A )2 + ˜C (˜y + ˜E 2 ˜C )2 + F − ˜D2 4 ˜A − ˜E2 4 ˜C = 0. xxii APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT A translation of the coordinate system to coordinates (ˆx, ˆy), such that the new origin is at ˆO ( − ˜D 2 ˜A , − ˜E 2 ˜C ) , i.e.    ˆx = ˜x + ˜D 2 ˜A ˆy = ˜y + ˜E 2 ˜C , then yields a standard form of the quadratic equation in 2 variables: ˜A(ˆx) 2 + ˜C(ˆy) 2 = −∆, (C.5) with ∆ = F − ˜D2 4 ˜A − ˜E2 4 ˜C . (C.6) From (C.4)-(C.6) and the standard equations for conics which follow, we will classify solutions to quadratic equations in 2 variables (see end of the chapter). C.2 Parabola Definition C.2. A parabola is the set of all points in the plane that are equidistant from a given fixed point and fixed line in the plane, that do not intersect. • The fixed point is called the focus of the parabola. • The fixed line is known as the directrix of the parabola. • The distance from a point P to a line is the distance between P and the point of intersection between the line and the perpendicular to this line through P (this is the same as the distance from the point P to the nearest point on the line). We will now derive the standard equation for a parabola, using a convenient coordinate system. The distance from the point P (x, y) to the focus F (0, p) is given by | ⃗P F | = √ x2 + (y − p)2. (C.7) The distance from P to the directrix is given by the distance from P to the point Q(x, −p) which is the intersection of the directrix with the perpendicular line through P . The distance from P to the directrix is thus given by | ⃗P Q| = √(y + p)2. (C.8) C.2. PARABOLA xxiii The points on the parabola are then described by setting | ⃗P F | = | ⃗P Q| or, via (C.7) and (C.8) √ x2 + (y − p)2 = √ (y + p)2 ⇐⇒ x2 + (y − p) 2 = (y + p) 2 ⇐⇒ x2 + y2 − 2py + p2 = y2 + 2py + p2 ⇐⇒ x2 = 4py. (C.9) The equation in (C.9), is often referred to as the standard equation of a parabola and can be written as y = x2 4p . (C.10) The parabola in (C.10) is obviously symmetric with respect to the y-axis, which is called the axis of the parabola. The point where the parabola crosses its axis, midway between the focus and the directrix, is called the vertex of the parabola. Here, the parabola has its vertex at the origin. Notice that p is the distance from the vertex to the focus and is known as the focal length. The width of the parabola at the focus is equal to 4p. C.2.1 Parabolas with vertex at the origin We have seen the standard parabola. The equation for differently orientated parabolas can be obtained in the same way, starting from the definition. Alternatively, one can use a coordinate transformation to obtain the equation. Recall that the coordinates (˜x, ˜y) in a coordinate system which is rotated anti-clockwise by an angle θ with respect to the standard coordinate system with coordinates (x, y) (see Figure C.1) is linked to the original coordinates by the transformation formulae given in (C.2) and (C.3). We now consider four special cases. Axis equal to the positive x-axis Theorem C.3. The equation of a parabola with focus at the point F with coordinates (p, 0) and directrix x = −p, with p > 0, is given by y2 = 4px. Proof: The graph of the parabola is identical to the graph of the parabola with standard equation in a coordinate system which is rotated by α = −π/2. In this rotated coordinate system, the equation for the parabola is given by ˜y = ˜x2 4p , xxiv APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT while the coordinates are linked by the transformation formula { ˜x = x cos(−π/2) + y sin(−π/2) = −y, ˜y = −x sin(−π/2) + y cos(−π/2) = x. The equation in the original coordinate system is then given by x = (−y) 2 4p , or equivalently, y2 = 4px, (C.11) as required. Figure C.2 depicts the parabola given in equation (C.11) with p = 2. Figure C.2: Parabolas from Theorems C.3-C.5 plotted with p = 2. Axis equal to the negative x-axis Theorem C.4. The equation of a parabola with focus at the point F with coordinates (−p, 0) and directrix x = p, with p > 0, is given by y2 = −4px. C.2. PARABOLA xxv Proof: In this case, the standard equation can be used in a coordinate system which is rotated through an angle α = π/2, leading to the transformation formula { ˜x = x cos(π/2) + y sin(π/2) = y, ˜y = −x sin(π/2) + y cos(π/2) = −x. so that the equation in the original coordinate system is given by −x = y2 4p , or equivalently, y2 = −4px, (C.12) as required. Figure C.2 depicts the parabola given in (C.12) for p = 2. Axis equal to the negative y-axis Theorem C.5. The equation of a parabola with focus at the point F with coordinates (0, −p) and directrix y = p, with p > 0, is given by x2 = −4py. Proof: We find the equation by considering a coordinate system rotated through an angle α = π, so that { ˜x = x cos(π) + y sin(π) = −x, ˜y = −x sin(π) + y cos(π) = −y. This leads to the equation −y = (−x) 2 4p , or equivalently y = − x2 4p, (C.13) as required. Again, the parabola given in (C.13) is plotted in Figure C.2 for p = 2. Axis equal to the bisectrix in the first quadrant The bisectrix of the first quadrant (region where x, y > 0) is the line that divides the first quadrant in the xy-plane into 2 isometric (same shape) regions. xxvi APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT Theorem C.6. The equation of a parabola with vertex at (0, 0) and directrix y = −x − √ 2p, with p > 0 is given by (x − y) 2 = 4p√2(x + y). Proof: The equation can now be deduced by considering a coordinate system which is rotated through an angle α = −π/4. { ˜x = x cos(−π/4) + y sin(−π/4) = √2 2 (x − y), ˜y = −x sin(−π/4) + y cos(−π/4) = √2 2 (x + y). In the (˜x, ˜y) coordinate system, the equation of the parabola is given by 4p˜y = ˜x2, with p the focal distance, i.e. the distance from the origin to the point F, and with vertex at (0, 0). This yields the equation √ 2 2 (x + y) = 1 2 (x − y)2 4p , or equivalently, (x − y) 2 = 4p√2(x + y), (C.14) as required. If we expand the equation in (C.14) we get 1 8p x2 − 1 4pxy + 1 8py2 − √ 2 2 x − √2 2 y = 0. This illustrates the fact that the equation of a parabola in an arbitrary position is not a simple relationship between x and y. Notice that this equation is quadratic in both x and y and does contain a term in xy! The parabola given in (C.14) is depicted in Figure C.3 for p = 0.5. C.2.2 Shifting a parabola Theorem C.7. The equation of a parabola with axis parallel to the positive y-axis (and pointing in the same direction), vertex at the point P with coordinates (q, s) and focal length p, with p > 0, is given by (x − q)2 = 4p(y − s). C.2. PARABOLA xxvii Figure C.3: A parabola with axis y = x. Proof: Consider a parabola with equation x2 = 4py, with vertex at the origin, focal length p and axis (of the parabola) given by the y-axis. The parabola we seek, has the same form, ˜x2 = 4p˜y, (C.15) where the transformation formula is given a translation2 , { ˜x = x − q, ˜y = y − s. (C.16) Note that this transformation merely shifts the origin of the coordinate system without rotating or stretching the axes. Substituting (C.16) into (C.15) gives (x − q)2 = 4p(y − s), (C.17) as required. Observe that we can rewrite (C.17) as y = ax2 − 2aqx + (aq2 + s), (C.18) with a = 1 4p so that the RHS of (C.18) is a quadratic in x. 2The word translation here means a shift of any combination of left/right/up/down. xxviii APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT From (C.18), we can easily find the equation of a parabola with vertex in a given point, if the equation of a similar parabola with vertex at the origin is known. In particular, via Theorem C.5, the parabola with vertex at P (q, s) and axis parallel to the negative y-axis, is characterised by y − s = −a(x − q)2, (C.19) with a = 1 4p . Similarly, via Theorem C.3, a parabola with axis parallel to the positive x-axis (and pointing in the same direction) and with vertex at P (q, s), is described by the equation (y − s) 2 = 4p(x − q). Moreover, via Theorem C.4, a parabola that has the axis parallel to the negative x-axis and vertex at P (q, s) is described by the equation (y − s) 2 = −4p(x − q). C.2.3 The tangent to a parabola Consider the intersection of a line with equation y = mx + q, (C.20) and the parabola with equation 4px = y2. (C.21) The points of intersection are found by solving the nonlinear system of equations { y = mx + q, 4px = y2. When we use y in (C.20) and substitute it for y in (C.21), we find a quadratic equation for the x-coordinates of the intersection points: m 2x2 + (2mq − 4p)x + q2 = 0. The discriminant of this equation3 is given by D = (2mq − 4p)2 − 4m 2q2 = 16p(p − mq), (C.22) so that when D > 0 the line has two points of intersection. When D < 0, the line does not intersect the parabola, and when D = 0, the two points of intersection coincide. In this case, we say that the line is a tangent 4 to the parabola at the specified point. 3This only exists if m ̸= 0. If m = 0 we have a line parallel to the axis of the parabola that intersects/crosses the parabola once at (x1, y1) which is not the tangent line to the parabola at (x1, y1). 4Compare this approach to finding a tangent to a general curve (see [15] or [14]). You should consider why the approach we adopt here works? Additionally, note that one can find the tangent in specific problems via implicit differentiation with more ease, however, to prove properties satisfied by parabola, this idea of a tangent line is helpful. C.2. PARABOLA xxix Let us now assume that D = 0 or equivalently via (C.22) that p = mq, and that the line y = mx + q is the tangent to the parabola at the point P (x1, y1). Since P is on the parabola, 4px1 = y2 1, and y1 − mx1 − q = 0 ⇐⇒ 4py1 − my2 1 − 4pq = 0 ⇐⇒ 4py1 − p q y2 1 − 4pq = 0 ⇐⇒ 4pq2 − 4qpy1 + py2 1 = 0 ⇐⇒ p (2q − y1) 2 = 0 ⇐⇒ q = y1 2 . (C.23) It then follows from (C.23) that (for y1 ̸= 0) m = p q = 2p y1 . The equation of the tangent is thus given by (for y1 ̸= 0) y = 2p y1 x + y1 2 , (C.24) which can be rewritten as y1y = 2px + y2 1 2 , or equivalently 5 y1y = 2p (x + x1) . (C.25) It is now straightforward to write the equation of the normal to the parabola at a point. Since the normal is perpendicular to the tangent, its gradient is given by m = − y1 2p , so that the equation of the normal through a point P (x1, y1) on the parabola is given by y − y1 = − y1 2p (x − x1). Figure C.4 depicts the parabola 3x = y2 and the tangent and normal at the point P (2, 2 √ a) on the parabola. 5Note that this equation holds for any y1 ∈ R, since if y1 = 0, x = 0 is in fact the tangent to the parabola xxx APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT Figure C.4: Tangent and normal to a parabola at P (2, 2 √a). C.2.4 Reflective property Theorem C.8. The tangent and the normal at a point of a parabola are the bisectors of the angles defined by the line P F , with F the focus, and the line through P parallel to the axis of the parabola. This is illustrated in Figure C.5. Note that the bisector of an angle is the line which divides an angle into two equal angles. Proof: To prove this property, we first define the point G as the intersection of the directrix of the parabola and the line parallel to the axis of the parabola through P . The point T is defined as the intersection of the tangent at P and the axis of the parabola. When we can prove that the quadrangle P F T G is a rhombus, then we know that its C.2. PARABOLA xxxi Figure C.5: Reflective property of a parabola. diagonal P T bisects the angle between the sides P F and P G. It then also follows that the normal bisects the adjacent angle. So all we have to do is to prove that P F T G is a rhombus. By construction, we know that ⃗T F is parallel to ⃗P G. When we can show that | ⃗T F | = | ⃗GP |, then we have shown that the quadrangle P F T G is a parallelogram. First choose a coordinate system in which the origin is located at the vertex of the parabola and the x-axis coincides with the axis of the parabola. The equation of the parabola is then given by y2 = 4px, with focus at F (p, 0). The directrix is given by x = −p. We assume that the point P has coordinates P (x1, y1). The point G then has coordinates G(−p, y1) since it is on the directrix and the line P G is parallel to the x-axis. The coordinates of T can be found as the intersection of the axis, y = 0, with the tangent in P , namely in (C.25) y1y = 2p(x + x1), so T (−x1, 0). The distances | ⃗T F | and | ⃗GP | are then given by | ⃗T F | = | p + x1 |, (C.26) xxxii APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT | ⃗GP | = | −p − x1 |=| p + x1 | . (C.27) Identities (C.26) and (C.27) prove that | ⃗T F | = | ⃗GP |, or that the quadrangle P F T G is a parallelogram. It will be a rhombus when in addition | ⃗P G| = | ⃗P F |. This equality follows from the fact that P is a point of the parabola with focus at F (see Definition C.2). Indeed, | ⃗P G| is the distance from P to the directrix and | ⃗P F | is the distance from P to the focal point. Therefore, we have proved that the quadrangle P F T G is a rhombus. It follows that the tangent P T is the bisector of the inner angle in P defined by P F and P G. Since the normal is perpendicular to the tangent, it must also bisect the adjacent angle, which completes the proof, as required. A consequence of Theorem C.8, the reflection property, is that any ray of light which falls onto a parabolic mirror (see Figure C.4) parallel to its axis will be reflected into the focal point. C.3 Ellipse Definition C.9. An ellipse is the set of points in a plane whose distances from two distinct fixed points F1 and F2 in the plane have a constant sum 2a with 2a > | ⃗F1F2|. • The two fixed points are the foci of the ellipse. • The midpoint between the two foci is called the centre of the ellipse. • The line through the foci of an ellipse is the focal axis (or major axis) of the ellipse. The line perpendicular to the focal axis through the centre of the ellipse is the minor axis 6. • The points where the focal axis intersects the ellipse are the vertices of the ellipse. An ellipse can easily be drawn using a piece of string fixed at the two foci. A proper choice of the coordinate system will lead to the standard equation of an ellipse. We will also consider how the standard equation changes in a translated or rotated coordinate system. We will derive the equation for the tangent and normal at a point of the ellipse and discuss the reflective properties of ellipses. 6Sometimes, we refer to the major axis of an ellipse as the distance between the 2 intersection points of the ellipse and its ‘major axis’. Similarly, we sometimes refer to the minor axis of an ellipse as the distance between the 2 intersection points of the ellipse and the ‘ minor axis’. C.3. ELLIPSE xxxiii We will now derive the standard equation of an ellipse. Since | ⃗F1F2| = 2c and 2a = | ⃗P F1| + | ⃗P F2|, it follows that a > c. A point P (x, y) is on the ellipse (defined by F1, F2 and a) when the sum of the distances | ⃗P F1| and | ⃗P F2| is 2a, or equivalently, | ⃗P F1| + | ⃗P F2| = 2a ⇐⇒ √ (x − c)2 + y2 + √((x + c)2 + y2 = 2a (C.28) ⇐⇒ (x − c) 2 + y2 + 2√ ((x − c)2 + y2) ((x + c)2 + y2) + (x + c)2 + y2 = 4a2. (C.29) Note that (C.29) is equivalent to (C.28) since both sides of equation (C.28) are positive. Now, equation (C.29) can be rewritten as, 2 (x2 + c 2 + y2) + 2√((x − c)2 + y2) ((x + c)2 + y2) = 4a 2 ⇐⇒ √((x − c)2 + y2) ((x + c)2 + y2) = 2a 2 − ( x2 + c2 + y2) =⇒ ((x − c) 2 + y2) ((x + c) 2 + y2) = (C.30) 4a4 − 4a 2 (x2 + y2 + c2) + (x2 + c2 + y2)2 ⇐⇒ (x − c) 2(x + c) 2 + ((x − c)2 + (x + c) 2) y2 + y4 = 4a4 − 4a2 (x2 + y2 + c2) + x4 + y4 + c4 + 2x2y2 + 2x2c2 + 2c2y2 ⇐⇒ x4 − 2x2c2 + c4 + 2x2y2 + 2c2y2 + y4 = 4a4 − 4a2 (x2 + y2 + c2) + x4 + y4 + c4 + 2x2y2 + 2x2c2 + 2c2y2 ⇐⇒ 4a4 − 4a2 (x2 + y2 + c2) + 4x2c2 = 0 ⇐⇒ (a2 − c2) x2 + a2y2 = a2 ( a2 − c2) . (C.31) Since b2 = a2 − c2 > 0, (C.32) we can express equation (C.31) as b2x2 + a2y2 = a2b2, xxxiv APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT or equivalently, x2 a2 + y2 b2 = 1. (C.33) Notice that we have only shown that all points, for which the sum of their distance to the two foci is constant, satisfy equation (C.33) (see the step in (C.30)). Before we can state that equation (C.33) is the standard equation of an ellipse, we need to show that all points satisfying this equation indeed are on the ellipse. Therefore, assume that the point Q(x1, y1) satisfies (C.33), i.e. x2 1 a2 + y2 1 b2 = 1, (C.34) then | ⃗QF1| = √(x1 − c)2 + y2 1 = √ x2 1 − 2cx1 + c2 + b2 − b2x2 1 a2 (via (C.34)) = √a2 − b2 a2 x2 1 − 2cx1 + c2 + b2 = √ c2 a2 x2 1 − 2cx1 + a2 (via (C.32)) = √( cx1 a − a)2. (C.35) Since a > c and −a ≤ x1 ≤ a (since Q(x1, y1) satisfies (C.33)), it follows that −a < cx1 a < a, (C.36) and therefore, | ⃗QF1| = a − cx1 a . (C.37) Similarly, the distance from Q to the second focus is given by | ⃗QF2| = √ c2x2 1 a2 + 2cx1 + a2 = √( cx1 a + a)2. (C.38) Thus, via (C.36), | ⃗QF2| = a + cx1 a . (C.39) Finally, via (C.37) and (C.39) | ⃗QF1| + | ⃗QF2| = a − cx1 a + a + cx1 a = 2a, C.3. ELLIPSE xxxv which shows that any point, at which the coordinates satisfy (C.33) must be on the ellipse, as required. We therefore call equation (C.33) the standard equation of the ellipse. This ellipse inter- sects the y-axis at the points B1(0, b) and B2(0, −b), as for x = 0, we have that y2 b2 = 1 ⇐⇒ y = ±b. The number c is the centre-to-focus distance of the ellipse. C.3.1 Ellipse with a vertical major axis Theorem C.10. The equation of an ellipse with foci at F1(0, c) and F2(0, −c), where c > 0, is given by x2 b2 + y2 a2 = 1, with a and b as in Definition 6.5. Proof: To obtain the standard equation of an ellipse with its foci on the y-axis, we write the standard equation in Definition 6.5 in a coordinate system which is rotated by α = π/2: ˜x2 a2 + ˜y2 b2 = 1, with { ˜x = x cos(π/2) + y sin(π/2) = y, ˜y = −x sin(π/2) + y cos(π/2) = −x. This yields the standard equation in the original coordinates, x2 b2 + y2 a2 = 1. Now the vertices have coordinates (0, a) and (0, −a). The endpoints of the minor axis have coordinates (−b, 0) and (b, 0). The foci are located at F1(0, c) and F2(0, −c) with c2 = a2 − b2, as required. Example 161: The ellipse with equation x2 + y2 4 = 1, is plotted as in Figure C.6. One can recognise the orientation of the major axis by identifying the smallest coefficient in the equation: when the coefficient of x2 is smallest, the major axis is horizontal. When the coefficient of y2 is smallest, the major axis is vertical. Of course both of these statements only apply if there is no xy term present in the equation defining the ellipse. xxxvi APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT Figure C.6: The ellipse with equation x2 + y2 4 = 1. C.3.2 Ellipse with centre at P (q, s) Theorem C.11. The equation of the ellipse with centre at P (q, s), with centre-to- focus distance c > 0 and with major axis parallel to the x-axis, is given by (x − q) 2 a2 + (y − s) 2 b2 = 1, with a and b as in Definition 6.5. Proof: To obtain the equation of an ellipse with centre at a given point P (q, s), we C.3. ELLIPSE xxxvii consider the standard equation in a shifted coordinate system with origin at P (q, s): ˜x2 a2 + ˜y2 b2 = 1, with { ˜x = x − q, ˜y = y − s. Substituting the expressions for ˜x and ˜y yields the equation for the ellipse in the original coordinates, (x − q) 2 a2 + (y − s) 2 b2 = 1, as required. Example 162: Figure C.7 depicts the ellipse with equation (x − 1.3) 2 3 + (y − 2.2) 2 1.8 = 1. (C.40) This ellipse has centre at P (1.3, 2.2), and the major axis is horizontal. Figure C.7: The ellipse with centre at P (1.3, 2.2). We can rewrite the equation of an ellipse with centre at P (q, s) as b2x2 − 2b2qx + a2y2 − 2a2sy + (b2q2 + a2s2 − a2b2) = 0, xxxviii APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT which is of the form Ax2 + Cy2 + Dx + Ey + F = 0 (C.41) (the letter B is associated with the xy-term, so here B = 0). An equation of the form in (C.41) can be reduced to the standard equation for an ellipse by completing the squares in x and y. The ellipse defined by equation (C.40) and plotted in Figure C.7, can equivalently be defined by the equation, 1.8x2 − 4.68x + 3y2 − 13.2y + 12.162 = 0. C.3.3 Ellipse with a tilted major axis The equation of an ellipse with major axis neither horizontal or vertical can be obtained by writing the standard equation in a rotated coordinate system. Consider the coordinate system rotated by an angle α. The standard equation in this system is ˜x2 a2 + ˜y2 b2 = 1, with { ˜x = x cos(α) + y sin(α), ˜y = −x sin(α) + y cos(α). The equation for the ellipse in the original coordinate system is (x cos(α) + y sin(α)) 2 a2 + (−x sin(α) + y cos(α)) 2 b2 = 1, which can be rewritten as (cos 2(α) a2 + sin2(α) b2 ) x2 + ( sin2(α) a2 + cos 2(α) b2 ) y2 + 2 cos(α) sin(α) ( 1 a2 − 1 b2 ) xy = 1. (C.42) The equation defining the rotated ellipse typically gains a term in xy (see (C.42)) when the major axis is not parallel to the x-axis or the y-axis. Example 163: The ellipse with major axis rotated anti-clockwise (from standard form) by α = π/6 radians, with focal length c = 2, and length of the major axis equal to 6 (or a = 3), can be written as 1 = (cos 2(π/6) 9 + sin2(π/6) 5 ) x2 + ( sin2(π/6) 9 + cos 2(π/6) 5 ) y2 + 2 cos(π/6) sin(π/6) ( 1 9 − 1 5 ) xy, C.3. ELLIPSE xxxix = (3/4 9 + 1/4 5 ) x2 + ( 1/4 9 + 3/4 5 ) y2 + 2   √(3) 2   ( 1 2 ) (− 4 45 ) xy, = 2x2 15 + 8 45y2 − 2√ 3xy 45 . This ellipse is depicted in Figure C.8. Figure C.8: The ellipse with tilted major axis. C.3.4 The tangent to an ellipse Again, we will study the intersection of a line with the ellipse. Consider the line with equation y = mx + q, (C.43) and the ellipse x2 a2 + y2 b2 = 1. (C.44) The x-coordinate of an intersection point between the line in (C.43) and the ellipse in (C.44) can be found by substituting y in (C.43) into (C.44): x2 a2 + (mx + q) 2 b2 = 1 ⇔ (b2 + a2m 2) x2 + 2mqa 2x + a2q2 − a2b2 = 0. (C.45) xl APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT The discriminant of the quadratic equation7 in (C.45) (in x) is given by D = 4m 2q2a4 − 4 (a 2q2 − a2b2) (b2 + m2a2) = −4a2b2 (q2 − b2 − a2m 2) . Again, D > 0 implies that there are two possible points of intersection, and D < 0 implies that there are no points of intersection. To obtain a tangent to the ellipse (at a point of intersection) we must have D = 0, i.e. q2 = b2 + a2m2. (C.46) We want to write the gradient of the tangent m as a function of known quantities. Assume the point of intersection is P (x1, y1) with y1 ̸= 0. Then via (C.43), y1 − mx1 = q. (C.47) Squaring both sides of (C.47) and substituting q2 in (C.46), yields a2m 2 + b2 − m2x2 1 + 2mx1y1 − y2 1 = 0 ⇐⇒ (a2 − x2 1) m 2 + 2x1y1m + b2 − y2 1 = 0. (C.48) The quadratic equation in (C.48) (in m) has only one solution since its discriminant is given by 4x2 1y2 1 − 4 (b2 − y2 1) (a2 − x2 1) = 4b2x2 1 + 4a2y2 1 − 4a2b2 = 4a2b2 (x2 1 a2 + y2 1 b2 − 1 ) = 0 (via (C.44)), since P (x1, y1) lies on the ellipse. Therefore the gradient of the tangent at P (x1, y1) is given by m = −2x1y1 2 (a2 − x2 1) = −x1y1b2 a2y2 1 = − b2x1 a2y1 . (C.49) The equation of the tangent to the ellipse at (x1, y1), via (C.49) and (C.43) is y − y1 = − b2x1 a2y1 (x − x1) ⇐⇒ y1 b2 (y − y1) = −x1 a2 (x − x1) 7Note that the discriminant always exists since b2 + a 2m2 > 0. C.3. ELLIPSE xli ⇐⇒ y1y b2 + x1x a2 = y2 1 b2 + x2 1 a2 ⇐⇒ x1x a2 + y1y b2 = 1 (via (C.44)). (C.50) Therefore, the tangent line to the ellipse in (C.44) at P (x1, y1) on the ellipse is given by (C.50), where in fact, y1 = 0 is now allowed i.e. if (x1, y1) = (±a, 0) then the tangent to the ellipse is x = ±a. Example 164: Consider the point P (3/2, √3) on the ellipse with equation x2 9 + y2 4 = 1. The tangent to the ellipse at P is then given by (3/2)x 9 + √3y 4 = 1, via (C.50), or equivalently, 2x + 3√3y − 12 = 0. This is illustrated in Figure C.9. Figure C.9: The tangent and normal to the ellipse x2 9 + y2 4 = 1 at P (3/2, √3). xlii APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT The normal to the ellipse in (C.44) at P (x1, y1) can now be determined easily from (C.50). The gradient of the normal to the ellipse at a point P (x1, y1) is given by m = a2y1 b2x1 , for x1 ̸= 0, so that the equation of the normal to the ellipse at P (with x1 ̸= 0) is given by y − y1 = a2y1 b2x1 (x − x1). (C.51) Example 165: For the ellipse in Example 164, the normal at P (3/2, √3) is given by y − √3 = 9 √3 4(3/2)(x − 3/2), via (C.51), or equivalently, 6 √ 3x − 4y − 5 √ 3 = 0. This is also illustrated in Figure C.9. C.3.5 Reflection property Theorem C.12. The tangent and the normal to an ellipse at a point P are the bisectors of the angles formed by the lines that connect P with the foci of the ellipse. This property is illustrated in Figure C.10. Proof: To prove this property, we construct the bisector of the lines F1P and F2P and show the bisector must be normal or tangent (depends which bisector we consider) to the ellipse at P . First, consider the point Q on the line F1P (outside the ellipse) such that | ⃗P Q| = | ⃗P F2|. (C.52) Each point E on the bisector of the angle ̂F2P Q defined by the lines F2P and P Q must then be equally far from F2 as from Q: | ⃗EF2| = | ⃗EQ|. (C.53) When E ̸= P it follows that | ⃗EF2| + | ⃗EF1| = | ⃗EQ| + | ⃗EF1| (via (C.53) > | ⃗F1Q| (via triangle inequality) = | ⃗F1P | + | ⃗P Q| C.4. HYPERBOLA xliii Figure C.10: The reflection property of an ellipse. = | ⃗F1P | + | ⃗P F2| = 2a. (C.54) Therefore, via (C.54), E cannot be on the ellipse. So we have shown that each point on the bisector of the angle ̂F2P Q not equal to P cannot be on the ellipse, or equivalently, that the bisector of the angle ̂F2P Q has only the point P in common with the ellipse. This means that the bisector of the angle ̂F2P Q is the tangent to the ellipse at P . It then also follows that the normal to the ellipse at P is the bisector of the angle ̂F1P F2, as required. A consequence of the reflective property property is that a ray of light which passes through a focus of an elliptical mirror is reflected into the other focus. C.4 Hyperbola Definition C.13. A hyperbola is the set of points in a plane whose distances from two distinct fixed points F1 and F2 in the plane have a constant difference 2a with 0 < 2a < | ⃗F1F2|. • The two fixed points are the foci of the hyperbola. xliv APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT • The midpoint between the two foci is called the centre of the hyperbola. • The line through the foci of a hyperbola is the focal axis (or major axis). The line perpendicular to the focal axis through the centre is the minor axis. • The points where the hyperbola crosses the focal axis are the vertices. If we are given foci at F1 and F2 with constant a > 0, then these define a hyperbola. A point P on this hyperbola (for instance) satisfies | ⃗P F1|−| ⃗P F2| = 2a or | ⃗P F2|−| ⃗P F1| = 2a. We will derive the standard equation of a hyperbola and study how it changes under translation or rotation of the coordinate system. We will derive an equation for the tangent and normal at a point on the hyperbola and briefly discuss its reflective properties. Since 2a < | ⃗F1F2| = 2c, it follows that a < c and b2 = c2 − a2 > 0. A point P (x, y) is on the hyperbola when | ⃗P F1| − | ⃗P F2| = ±2a (C.55) ⇐⇒ √(x + c)2 + y2 − √ (x − c)2 + y2 = ±2a ⇐⇒ (x + c) 2 + y2 − 2 √(x + c)2 + y2√ (x − c)2 + y2 + (x − c) 2 + y2 = 4a2 ⇐⇒ 2x2 + 2y2 + 2c2 − 2 √ (x2 + c2 + y2 − 2cx) (x2 + c2 + y2 + 2cx) = 4a2. ⇐⇒ x2 + y2 + c2 − 2a2 = √(x2 + y2 + c2) 2 − 4c2x2. (C.56) Equation (C.56) implies, but is not necessarily equivalent to, (x2 + y2 + c2 − 2a2)2 = (x2 + y2 + c2)2 − 4c2x2 ⇐⇒ (x2 + c2 + y2)2 − 4a2 (x2 + c2 + y2) + 4a4 = (x2 + y2 + c2)2 − 4c2x2 ⇐⇒ (c2 − a2) x2 + a2 (a2 − c2) − a2y2 = 0 (C.57) ⇐⇒ b2x2 − a2y2 = a2b2. (C.58) C.4. HYPERBOLA xlv Since b2 > 0, it follows that (C.58) is equivalent to x2 a2 − y2 b2 = 1, (C.59) Again, we have lost the equivalence of (C.59) and (C.55) (from (C.56) to (C.58)) since we were not sure x2 + y2 + c2 − 2a2 ≥ 0. Therefore, now assume a point Q(x1, y1) satisfies the equation (C.59) with b2 = c2−a2 > 0. This implies that x2 1 ≥ a2. (C.60) In addition, a positive number is always larger then a negative one, so y2 1 ≥ −b2. (C.61) Therefore, x2 1 + y2 1 + c2 − 2a2 ≥ a2 − b2 + c2 − 2a2 (via (C.60) and (C.61)) ≥ a2 − b2 + b2 − a2 (since c2 = a2 + b2) = 0. (C.62) So when a point satisfies (C.59) with b2 = c2 − a2 > 0, we know from (C.62) that √(x2 + y2 + c2 − 2a2) 2 = x2 + y2 + c2 − 2a 2, and therefore (C.58) implies (C.56) and hence, (since all other relations are iff) (C.56) implies (C.55), as required. We can therefore refer to (C.59) as the standard equation of a hyperbola. C.4.1 Alternative formulae Focal axis is y-axis Theorem C.14. The equation of a hyperbola with foci at F1(0, c) and F2(0, −c), where c > 0, is given by y2 a2 − x2 b2 = 1, with a and b as in Theorem 6.11. xlvi APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT Proof: We can find the equation of a hyperbola with foci on the y-axis and centre at the origin by writing the standard equation for a hyperbola in a coordinate system rotated through an angle α = π/2 and then use the transformation formula, similar to the approach used for the parabola and ellipse. Set { ˜x = cos (π/2)x + sin (π/2)y = y, ˜y = − sin (π/2)x + cos (π/2)y = −x. and substituting the new coordinates into ˜x2 a2 − ˜y2 b2 = 1 gives y2 a2 − x2 b2 = 1, in the original coordinates, as required. The hyperbola with equation y2 4 − x2 5 = 1, is depicted in Figure C.11. Figure C.11: Hyperbola with vertical focal axis. C.4. HYPERBOLA xlvii Hyperbola with tilted focal axis Assume the focal axis of a hyperbola is tilted by an angle α from the positive x-axis in the anti-clockwise direction. Here, we write the standard equation in a coordinate system which is rotated through an angle α in the clockwise direction. Using the transformation formula for a rotation of the coordinate system, we obtain the equation (x cos(α) + y sin(α)) 2 a2 − (−x sin(α) + y cos(α)) 2 b2 = 1, or equivalently, ( cos 2(α) a2 − sin2(α) b2 ) x2 + ( sin2(α) a2 − cos 2(α) b2 ) y2 + 2 sin(α) cos(α) ( 1 a2 + 1 b2 ) xy = 1. (C.63) Observe that a xy term appears in (C.63), which is not present in the equations of hyperbola in Theorem 6.11 and C.14. Example 166: Figure C.12 depicts a hyperbola with a = 2 and b = √5 and with the focal axis at an angle α = π/3 to the positive x-axis. The equation for the hyperbola is given by − 7 80x2 + 11 80y2 + 9 √3 40 xy = 1. Note that there is a non-zero xy term in the equation above. Hyperbola with centre at P (q, s) Theorem C.15. The equation of the hyperbola with centre at P (q, s), with centre- to-focus distance c > 0 and major axis parallel to the x-axis, is given by (x − q)2 a2 − (y − s)2 b2 = 1, with a and b as in Definition 6.11. Proof: We consider the standard equation in a translated coordinate system with origin at P (q, s). The transformation formula (see for example the proof of Theorem C.11) then leads to the equation (x − q)2 a2 − (y − s)2 b2 = 1, xlviii APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT Figure C.12: Hyperbola with tilted focal axis. as required. Example 167: The hyperbola with equation (x − 1.3) 2 4 − (y − 3.1) 2 5 = 1, (C.64) is depicted in Figure C.13. Equation (C.64) can be expanded to yield 0.25x2 − 0.65x − 0.2y2 + 1.24y = 2.4995. C.4.2 The tangent to a hyperbola As in the case of the ellipse, the points of intersection of a hyperbola in standard form and the line y = mx + q, (C.65) are determined by substituting (C.65) into the equation in Definition 6.11, i.e. x2 a2 − (mx + q) 2 b2 = 1 C.4. HYPERBOLA xlix Figure C.13: Hyperbola with centre in P (1.3, 3.1). ⇔ (b2 − m2a2) x2 − 2mqa 2x − (a2q2 + a2b2) = 0. The discriminant of this quadratic equation8 (in x) is given by D = 4a2b2 (b2 − m2a2 + q2) , (C.66) where D > 0 corresponds to two point of intersection, D < 0 indicates that there are no points of intersection, and, D = 0 implies the line is tangent to the hyperbola. We can eliminate q from (C.66) by specifying that the point of intersection Q(x1, y1) between the line and the hyperbola is unique, i.e. q = y1 − mx1. The condition D = 0 can then be re-written by substitution of q into (C.66) (x2 1 − a2) m 2 − 2x1y1m + b2 + y2 1 = 0. (C.67) Since the point (x1, y1) is on the hyperbola, it follows that the quadratic equation in (C.67) in m, has zero discriminant, and hence, the gradient of the tangent to the hyperbola at (x1, y1) (with x1 ̸= ±a) is equal to m = x1y1 x2 1 − a2 = x1b2 y1a2 . (C.68) The tangent line determined by q, (C.65) and (C.68) can then, after some manipulations, be expressed as, x1x a2 − y1y b2 = 1. (C.69) 8The discriminant exists iff b 2 − m 2a 2 ̸= 0 i.e. provided that m ̸= ±b/a. If m = ±b/a, then the corresponding lines intersect/cross the hyperbola once at (x1, y1) and are parallel to a corresponding asymptote of the hyperbola. Neither of these lines are the tangent line to the hyperbola at (x1, y1). l APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT Note that (C.69) holds for all points on the hyperbola. Similarly, the equation for the normal to the hyperbola at (x1, y1) can be expressed as y − y1 = −a2y1 b2x1 (x − x1) . (C.70) Example 168: The hyperbola given by x2 4 − y2 9 = 1, has tangent at P (2√ 2, 3) given by 3 √2x − 2y − 6 = 0, via (C.69). In addition, the normal to the hyperbola at P (2 √2, 3) is given by y = −√2 3 x + 13 3 , via (C.70). The tangent and normal to the hyperbola at P are depicted in Figure C.14. Figure C.14: Tangent and normal at a hyperbola. C.4. HYPERBOLA li C.4.3 Reflection property Theorem C.16. The tangent and the normal at a point P to a hyperbola are the bisectors of the angles formed by the lines that connect P with the two focal points. This property is also illustrated in Figure C.14. Proof: To prove this property, we construct the bisector of the lines F1P and F2P and show the bisector must be normal or tangent (depends which bisector we consider) to the hyperbola at P . We consider the branch of the hyperbola defined by | ⃗P F1| − | ⃗P F2| = 2a with the argument for the other branch following the same argument. Now, consider the point Q on the line F1P such that | ⃗P Q| = | ⃗P F2|. (C.71) Each point E on the bisector of the angle ̂QP F2 defined by the lines QP and P F2 must then be equally far from F2 as from Q: | ⃗EF2| = | ⃗EQ|. (C.72) When E ̸= P it follows that || ⃗EF1| − | ⃗EF2|| = || ⃗EF1| − | ⃗EQ|| (via (C.72) < | ⃗F1Q| (via triangle inequality) = | ⃗F1P | − | ⃗P Q| = | ⃗F1P | − | ⃗P F2| (via (C.71)) = 2a. (C.73) Therefore, via (C.73), | ⃗EF1| − | ⃗EF2| ∈ (−2a, 2a) and hence, E is not on the hyperbola. Now, consider the continuous function G : R 2 → R given by G(P ) = || ⃗P F1| − | ⃗P F2|| ∀P ∈ R 2. It follows that: G = 2a on the hyperbola; G < 2a in the connected region of R 2, denoted by R, which contains the centre of the hyperbola and is bounded by the 2 arcs of the hyperbola; and G > 2a in the 2 disjoint connected regions, denoted by S1 and S2, which are bounded by the arcs of the hyperbola, and that contain the foci F1 and F2 respectively. To clarify ¯R ∪ ¯S1 ∪ ¯S2 = R2. The bisector of ̂QP F2 is contained in R ∪ P . Moreover, the lines that intersect the hyperbola at P that are parallel to the asymptotes, necessarily intersect S29. Therefore, the bisector of ̂QP F2 is the tangent to the hyperbola at P . The argument for the other branch of the hyperbola follows similarly. 9As an exercise, you should demonstrate this for a hyperbola in standard form. lii APPENDIX C. CONICS: OPTIONAL EXTRA CONTENT C.5 Classification of quadratic equations in 2 vari- ables Recall from Sections C.1.2-C.1.3, that given a quadratic equation in 2 variables, Ax2 + Bxy + Cy2 + Dx + Ey + F = 0, (C.74) we can choose a rotated coordinate system ˜x, ˜y such that (C.74) is equivalent to ˜A˜x2 + ˜C ˜y2 + ˜D˜x + ˜E ˜y + ˜F = 0. (C.75) If ˜A and ˜C are non-zero, we can translate the coordinate system (complete the square) to ˆx, ˆy so that (C.75) is equivalent to ˜A(ˆx) 2 + ˜C ˆy2 = −∆. (C.76) From (C.74)-(C.76) we can infer the following classification of quadratic equations in 2 variables: 1. ˜A = ˜C ̸= 0: circle, point or no graph depending on the sign of ∆. 2. ˜A = ˜C = 0: straight line, the whole plane, or no graph depending on ˜D, ˜E and ˜F . 3. ˜A ˜C > 0 and ˜A ̸= ˜C: ellipse, point or no graph depending on sign of ∆. 4. ˜A ˜C < 0: hyperbola ∆ ̸= 0 or pair of intersecting lines (∆ = 0). 5. ˜A = 0, ˜C ̸= 0 and ˜D ̸= 0: parabola (slightly different translation is required). 6. ˜C = 0, ˜A ̸= 0 and ˜E ̸= 0: parabola (slightly different translation is required). 7. ˜A = ˜D = 0: two parallel lines, two coinciding parallel lines (i.e. one line), or no graph depending on the sign of ˜E2 − 4 ˜C ˜F . 8. ˜C = ˜E = 0: two parallel lines, two coinciding parallel lines (i.e. one line), or no graph depending on the sign of ˜D2 − 4 ˜A ˜F . One can show that the classification can also be achieved using the discriminant B2 − 4AC, which uses the coefficients in the equation (C.1). Ignoring the degenerate cases, the classification becomes: 1. B2 − 4AC = 0: parabola. 2. B2 − 4AC > 0: hyperbola. 3. B2 − 4AC < 0: ellipse. Bibliography [1] 2011. URL http://www.cliffsnotes.com/assets/256137.png. [Accessed 2017-2- 14]. [2] R. A. Adams and C. Essex. Calculus: a Complete course. Pearson, 8th edition, Toronto, 2013. [3] S. C. Althoen and R. Mclauglin. Gauss-Jordan reduction: A brief history. The American mathematical monthly, 94(2):130–142, 1987. [4] R. A. Beezer. A first course in linear algebra, version 3.5, 2015. URL http:// linear.pugetsound.edu./. [Accessed 2017-17-12]. [5] J. E. Fields. A gentle introduction to the art of mathematics, Version 3.1. GNU, 2013. URL http://giam.southernct.edu/GIAM/GIAM.pdf. [6] The Quality Assurance Agency for Higher Education. Aca- demic credit in higher education in england - an introduction, 2009. URL http://www.qaa.ac.uk/en/Publications/Documents/ Academic-credit-in-higher-education-in-England---an-introduction.pdf. [Accessed 2017-03-12]. [7] S. Goodwin. Algebra and combinatorics (lecture notes), 2020. [8] D. Pixton M. Beck, G. Marchesi and L. Sabalka. A first course in complex analy- sis, 2012. URL http://math.sfsu.edu/beck/papers/complexorth.pdf. [Accessed 2019-01-19]. [9] J. J. O’Connor and E. F. Robertson. Augustus de morgan, 1996. URL http:// www-history.mcs.st-andrews.ac.uk/Biographies/De_Morgan.html. [Accessed 2017-10-12]. [10] J. J. O’Connor and E. F. Robertson. René descartes, 1996. URL http: //www-groups.dcs.st-and.ac.uk/history/Biographies/Descartes.html. [Ac- cessed 2018-01-22]. [11] J. J. O’Connor and E. F. Robertson. Leonhard euler, 1996. URL http:// www-history.mcs.st-and.ac.uk/Biographies/Euler.html. [Accessed 2018-04- 14]. liii liv BIBLIOGRAPHY [12] The Norwegian Academy of Science and Letters. Niels henrik abel. URL http: //www.abelprize.no/c53672/seksjon/vis.html?tid=53910. [Accessed 2018-04- 14]. [13] H. A. Priestley. Introduction to Complex Analysis. Oxford University Press, 2nd edition, GB, 2003. [14] T. Tao. Analysis I. Hindustan, 2nd edition, New Delhi, 2009. [15] Y. Wang and J. Huang. Real analysis and the calculus (lecture notes), 2019. These lecture notes include contributions from Dr H. C. Wilkie, Prof. J. R. Blake, Dr J. Kyle, Mr B. J. Philp, Dr D. F. M. Hermans, Dr J. C. Meyer and Prof. C. W. Parker. The original mathematical typesetting was undertaken using LATEX by Mr B. Taylor.","libVersion":"0.3.2","langs":""}