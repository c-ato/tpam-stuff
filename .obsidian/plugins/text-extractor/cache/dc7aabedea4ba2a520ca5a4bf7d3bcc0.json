{"path":"Year 1/Prob/Lecture/Lecture8.pdf","text":"Introduction to Probability Lecture 8 Today Poisson example Sums of random variables Covariance Function of random variables (discrete) Attendance: 61475838 Introduction to Probability Summary Bernoulli 𝑃 𝑥 𝑝 = 𝑝𝑥 1 − 𝑝 1−𝑥 𝑥 = 0,1 Binomial 𝑃 𝑘 𝑁, 𝑝 = 𝑁 𝑘 𝑝𝑘 1 − 𝑝 𝑁−𝑘 𝑘 = 0,1 … 𝑁 Poisson 𝑃 𝑘 𝜆 ≡ 𝜆𝑘 𝑘! 𝑒−𝜆 𝑘 = 0,1 … Introduction to Probability Multivariate Distributions Multivariate Distributions Introduction to Probability One variable 𝑃 𝑥 𝑥 ≡ ෍ 𝑥 𝑥 𝑃 𝑥 More than one variable 𝑃 𝑥, 𝑦 𝑥 ≡ ෍ 𝑥 ෍ 𝑦 𝑥 𝑃 𝑥, 𝑦 = ෍ 𝑥 𝑥 𝑃(𝑥) Example What is ⟨𝑥⟩ for the following? 𝑃 𝑥 = 0, 𝑦 = 0 = 0.1 𝑃 𝑥 = 1, 𝑦 = 0 = 0.1 𝑃 𝑥 = 0, 𝑦 = 1 = 0.4 𝑃 𝑥 = 1, 𝑦 = 1 = 0.4 Introduction to Probability 𝑥 ≡ ෍ 𝑥 ෍ 𝑦 𝑥 𝑃 𝑥, 𝑦 = ෍ 𝑥 𝑥 𝑃(𝑥) So 𝑥 = 0 × 𝑃 0,0 + 0 × 𝑃 0,1 + 1 × 𝑃 1,0 + 1 × 𝑃(1,1) = 1 × 0.1 + 1 × 0.4 = 0.5 Or 𝑃 𝑥 = 0 = 𝑃 0,0 + 𝑃 0,1 = 0.5 𝑃 𝑥 = 1 = 𝑃 1,0 + 𝑃 1,1 = 0.5 → 𝑥 = 0 × 0.5 + 1 × 0.5 = 0.5 Sums of Random Variables Notation We say 𝑥 was drawn according P(𝑥) by writing 𝑥 ∼ 𝑃(𝑥) This tells us the distribution that 𝑥 follows. Example: binomial, Poisson. Introduction to Probability Sum of Random Variables Introduction to Probability Consider 𝑁 variables all drawn according to 𝑃(𝑥) 𝑥1 ∼ 𝑃 𝑥 ; 𝑥2 ∼ 𝑃 𝑥 … Define 𝑡 = 𝑥1 + 𝑥2 + ⋯ 𝑥𝑁 What is 𝑃(𝑡)? Generally this is too difficult to calculate. Can we calculate ⟨𝑡⟩ or var(𝑡)? Note: ҧ𝑥 = 1 𝑁 𝑥1 + 𝑥2 + ⋯ 𝑥𝑁 = 𝑡 𝑁 Expectation Value 𝑡 = ⟨𝑥1 + 𝑥2 + ⋯ 𝑥𝑁⟩ Introduction to Probability We already have linearity of expectation, so 𝑥1 + 𝑥2 + ⋯ 𝑥𝑁 = 𝑥1 + 𝑥2 + ⋯ 𝑥𝑁 Or we can do it explicitly. Consider 𝑁 = 2 𝑥1 + 𝑥2 = ෍ 𝑥1,𝑥2 𝑥1 + 𝑥2 𝑃(𝑥1, 𝑥2) = ෍ 𝑥1,𝑥2 𝑥1 𝑃(𝑥1, 𝑥2) + ෍ 𝑥1,𝑥2 𝑥2 𝑃(𝑥1, 𝑥2) = 𝑥1 + ⟨𝑥2⟩ In general ෍ 𝑛 𝑥𝑛 = ෍ 𝑛 ⟨𝑥𝑛⟩ Variance Let’s use 𝑁 = 2 Introduction to Probability 𝑡 = 𝑥1 + 𝑥2; var 𝑡 = 𝑡2 − 𝑡 2; 𝑡 = 𝑥1 + ⟨𝑥2⟩ 𝑣𝑎𝑟 𝑡 = 𝑥1 + 𝑥2 2 − 𝑥1 + 𝑥2 2 = 𝑥1 2 + 𝑥2 2 + 2 𝑥1𝑥2 − 𝑥1 2 − 𝑥2 2 − 2 𝑥1 𝑥2 Re-ordering = 𝑥1 2 − 𝑥1 2 + 𝑥2 2 − 𝑥2 2 + 2 𝑥1𝑥2 − 2 𝑥1 𝑥2 = var 𝑥1 + var 𝑥2 + 2cov(𝑥1, 𝑥2) Covariance Introduction to Probability var 𝑥 ≡ ෍ 𝑥 𝑥 − 𝑥 2 𝑃 𝑥 = 𝑥2 − 𝑥 2 cov 𝑥, 𝑦 ≡ ෍ 𝑥𝑦 𝑥 − 𝑥 𝑦 − 𝑦 𝑃 𝑥, 𝑦 = 𝑥 − 𝑥 𝑦 − 𝑦 = 𝑥𝑦 − 2 𝑥 𝑦 + 𝑥 𝑦 𝑥𝑦 − 𝑥 𝑦 Note var 𝑥 = cov(𝑥, 𝑥) Covariance (2) Introduction to Probability Covariance measures linear association corr 𝑥, 𝑦 = cov 𝑥, 𝑦 std 𝑥 std(𝑦) Example Calculate the covariance between 𝑥 and 𝑦 for the following distribution 𝑃 𝑥 = 0, 𝑦 = 0 = 0.2 𝑃 𝑥 = 0, 𝑦 = 1 = 0.2 𝑃 𝑥 = 1, 𝑦 = 0 = 0.2 𝑃 𝑥 = 1, 𝑦 = 1 = 0.4 Introduction to Probability 𝑥 = ෍ 𝑥𝑦 𝑥 𝑃 𝑥, 𝑦 = 0.6 𝑦 = ෍ 𝑥𝑦 𝑦 𝑃 𝑥, 𝑦 = 0.6 𝑥𝑦 = ෍ 𝑥𝑦 𝑥𝑦 𝑃 𝑥, 𝑦 = 0.4 cov 𝑥, 𝑦 = 𝑥𝑦 − ⟨𝑥⟩⟨𝑦⟩ = 0.4 − 0.62 = 0.04 Covariance and Independence If 𝑥 and 𝑦 are independent then 𝑃 𝑥, 𝑦 = 𝑃(𝑥)𝑃(𝑦) Introduction to Probability cov 𝑥, 𝑦 = 𝑥𝑦 − ⟨𝑥⟩⟨𝑦⟩ 𝑥𝑦 = ෍ 𝑥𝑦 𝑥𝑦 𝑃 𝑥, 𝑦 = ෍ 𝑥𝑦 𝑥𝑦 𝑃 𝑥 𝑃(𝑦) = ෍ 𝑥 𝑥 𝑃 𝑥 ෍ 𝑦 𝑦 𝑃 𝑦 = ⟨𝑥⟩⟨𝑦⟩ So cov 𝑥, 𝑦 → 0 Variance of Sum If the variables are independent, then the following rule holds: Otherwise we must include the covariance. var(𝑥1 + 𝑥2 + ⋯ 𝑥𝑁) = ෍ 𝑛 var 𝑥𝑛 + 2 ෍ 𝑚>𝑛 cov(𝑥𝑛, 𝑥𝑚) Introduction to Probability The variance of the sum is the sum of the variances Example What is the expectation value and variance of the sum of 𝑁 independent Bernoulli variables, each with parameter 𝑝. Introduction to Probability For a Bernoulli 𝑥 = 𝑝; var 𝑥 = 𝑝(1 − 𝑝) Then 𝑘 = 𝑥1 + 𝑥2 + ⋯ 𝑥𝑁 → 𝑘 = 𝑥1 + 𝑥2 + ⋯ 𝑥𝑁 = 𝑁𝑝 → var(𝑘) = 𝑁𝑝(1 − 𝑝) Change of Variables Change of Variables (Discrete) Imagine we have some 𝑃𝑥 𝑥 . The sample space of 𝑥 is Ω𝑥. We make a function of 𝑥 like y = 𝑓 𝑥 What is 𝑃𝑦(𝑦)? Or Ω𝑦? Introduction to Probability Types of function Introduction to Probability Paired (bijection) Unpaired Same 𝑥 goes onto 𝑦Unique 𝑥 → Unique 𝑦 𝑦 = 𝑓(𝑥) 𝑦 = 𝑓(𝑥) Example Consider a fair six-sided die 𝑃𝑥 𝑥 = 1 6 𝑥 = 1,2,3,4,5,6 1. If 𝑦 = 𝑥 − 2, what is Ω𝑦 and 𝑃𝑦 𝑦 2. If z = |𝑥 − 2|, what is Ω𝑧 and 𝑃𝑧 𝑧 Introduction to Probability Ω𝑦 = {−1,0,1,2,3,4} So 𝑃𝑦 𝑦 = 1 6 for 𝑦 ∈ Ω𝑦 Ω𝑧 = {0,1,2,3,4} So 𝑃𝑧 𝑧 = 1 6 𝑧 = 0,2,3,4 ; 𝑃𝑧 𝑧 = 2 6 𝑧 = 1 𝑥 1 2 3 4 5 6 𝑦 -1 0 1 2 3 4 z 1 0 1 2 3 4 General Formula The general formula for 𝑃𝑦 𝑦 if we have 𝑦 = 𝑓(𝑥) 𝑃𝑦 𝑦 = ෍ 𝑥:𝑦=𝑓(𝑥) 𝑃𝑥(𝑥) And Ω𝑦 is the unique set of values that come 𝑦 = 𝑓(𝑥) for all 𝑥 in Ω𝑥. Introduction to Probability Summary Expectation value of sum 𝑥1 + 𝑥2 + ⋯ 𝑥𝑁 = 𝑥1 + 𝑥2 + ⋯ ⟨𝑥𝑁⟩ Variance of sum var 𝑥1 + 𝑥2 + ⋯ 𝑥𝑁 = ෍ 𝑛 var 𝑥𝑛 + 2 ෍ 𝑛>𝑚 cov 𝑥𝑛, 𝑥𝑚 Variance of sum (independent) var 𝑥1 + 𝑥2 + ⋯ 𝑥𝑁 = ෍ 𝑛 var 𝑥𝑛 Change of Variables 𝑥 ∼ 𝑃 𝑥 , 𝑦 = 𝑓 𝑥 → 𝑃 𝑦 = ෍ 𝑓 𝑥 =𝑦 𝑃(𝑥) Introduction to Probability Examples Class Example Calculate the correlation between 𝑥 and 𝑦 for the following distribution 𝑃 𝑥 = 0, 𝑦 = 0 = 0.2 𝑃 𝑥 = 0, 𝑦 = 1 = 0.2 𝑃 𝑥 = 1, 𝑦 = 0 = 0.2 𝑃 𝑥 = 1, 𝑦 = 1 = 0.4 corr 𝑥, 𝑦 ≡ cov 𝑥, 𝑦 std 𝑥 std(𝑦) Introduction to Probability From earlier: 𝑥 = 0.4; 𝑦 = 0.6; cov 𝑥, 𝑦 = 0.04 var 𝑥 = 0.4 − 0.42 = 0.24 var 𝑦 = 0.6 − 0.62 = 0.24 → corr 𝑥, 𝑦 = 0.04 0.24 = 1 7 Class Example Consider a fair six-sided die 𝑃𝑥 𝑥 = 1 6 𝑥 = 1,2,3,4,5,6 If 𝑦 = |𝑥 − 3|, what is Ω𝑦 and 𝑃𝑦 𝑦 and ⟨𝑦⟩? Introduction to Probability Ω𝑦 = {0,1,2,3} So 𝑃𝑦 𝑦 = 1 6 𝑦 = 0,3 1 3 𝑦 = 1,2 𝑦 = 1 6 × 0 + 1 3 × 1 + 1 3 × 2 + 1 6 × 3 = 3 2 𝑥 1 2 3 4 5 6 𝑦 2 1 0 1 2 3","libVersion":"0.3.2","langs":""}